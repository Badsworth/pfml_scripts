# Terraform configuration for any alarm that's stored in New Relic.

# ----------------------------------------------------------------------------------------------------------------------
# High-level administrative objects: (empty) alert policies, notification channels, etc.

resource "newrelic_alert_policy" "api_alerts" {
  name                = "PFML API Alerts (${upper(var.environment_name)})"
  account_id          = local.newrelic_account_id
  incident_preference = "PER_CONDITION" # a new alarm will sound for every distinct alert condition violated
}

resource "newrelic_alert_policy" "low_priority_api_alerts" {
  name                = "PFML Low Priority API Alerts (${upper(var.environment_name)})"
  account_id          = local.newrelic_account_id
  incident_preference = "PER_CONDITION" # a new alarm will sound for every distinct alert condition violated
}

# This key was generated by Kevin Yeh on 10-21-2020 and should be replaced if he leaves.
# This was manually stored in SSM and is not managed through Terraform.
##
data "aws_ssm_parameter" "pagerduty_api_key" {
  name = "/admin/common/pagerduty-api-key"
}

locals {
  low_priority_channel_key  = var.low_priority_nr_integration_key
  high_priority_channel_key = var.high_priority_nr_integration_key

  alert_channel = {
    "test"        = local.low_priority_channel_key,
    "performance" = local.low_priority_channel_key,
    "training"    = local.low_priority_channel_key,
    "stage"       = local.low_priority_channel_key,
    "uat"         = local.low_priority_channel_key,
    "breakfix"    = local.low_priority_channel_key,
    "cps-preview" = local.low_priority_channel_key,
    "prod"        = local.high_priority_channel_key,
  }
}
resource "newrelic_alert_channel" "newrelic_api_notifications" {
  name = "PFML API ${var.environment_name == "prod" ? "High" : "Low"} priority alerting channel"
  type = "pagerduty"

  config {
    service_key = local.alert_channel[var.environment_name]
  }
}

resource "newrelic_alert_policy_channel" "pfml_alerts" {
  policy_id = newrelic_alert_policy.api_alerts.id
  channel_ids = [
    newrelic_alert_channel.newrelic_api_notifications.id
  ]
}

resource "newrelic_alert_channel" "newrelic_api_prod_low_priority_notifications" {
  count = var.environment_name == "prod" ? 1 : 0
  name  = "PFML API Low priority alerting channel"
  type  = "pagerduty"

  config {
    service_key = local.low_priority_channel_key
  }
}

resource "newrelic_alert_policy_channel" "pfml_prod_low_priority_alerts" {
  count     = var.environment_name == "prod" ? 1 : 0
  policy_id = newrelic_alert_policy.low_priority_api_alerts.id
  channel_ids = [
    newrelic_alert_channel.newrelic_api_prod_low_priority_notifications[0].id
  ]
}

# ----------------------------------------------------------------------------------------------------------------------
# Alerts relating to the API's generic performance metrics

resource "newrelic_nrql_alert_condition" "api_error_rate" {
  # WARN: error rate above 5% in any five-minute period, with at least 10 unique users.
  # CRIT: error rate above 10% in two five-minute periods, with at least 10 unique users in each period.
  #
  # These should be tuned down once we can better distinguish
  # certain types of errors.
  name               = "API error rate too high"
  policy_id          = newrelic_alert_policy.api_alerts.id
  type               = "static"
  value_function     = "single_value"
  enabled            = true
  aggregation_window = 300 # 5-minute window

  nrql {
    # Calculate error percentage. Clamping is applied to ensure that any data points with less than 10 unique users
    # is removed from the calculation and interpreted as 0%. This is a workaround to prevent periods of low activity
    # (e.g. late night) from triggering the alarm and waking people up.
    #
    # Ignore the following triaged errors until they are resolved:
    # - mark_document_as_received 422 (PSD-842)
    # - mark_document_as_received 500 (PSD-1456)
    # - get_customer_info 403 (PSD-1122)
    # - document_upload 502 (PSD-1595)
    # - document_upload 403 (PSD-1800)
    # - download_document_as_leave_admin 502 (PSD-1738)
    # - FINEOSFatalUnavailable (Should be ignored entirely in https://github.com/EOLWD/pfml/pull/3516)
    #
    # This keeps the error rate signal clean so we can catch new frequent issues.
    #
    # Also ignore the following transactions:
    # - push_db (This is a before_request method that New Relic breaks out as a separate transaction)
    # - 503 errors (captured in a separate alarm with a higher threshold)
    #
    # Note that 504 errors are not mentioned here because 504s are not reflected in Transaction/TransactionError.
    # 504s come from the API Gateway, which sit in front of the API servers.
    #
    query             = <<-NRQL
      SELECT filter(
        count(error.message),
        WHERE NOT error.message LIKE '(mark_document_as_recieved) expected 200, but got 422%'
        AND NOT error.message LIKE '(mark_document_as_recieved) FINEOSFatalResponseError: 500%'
        AND NOT error.message LIKE '(get_customer_info) expected 200, but got 403%'
        AND NOT error.message LIKE '(upload_documents) FINEOSFatalResponseError: 502%'
        AND NOT error.message LIKE '(upload_documents) expected 200, but got 403%'
        AND NOT error.message LIKE '(download_document_as_leave_admin) FINEOSFatalResponseError: 502%'
        AND NOT error.class = 'massgov.pfml.fineos.exception:FINEOSFatalUnavailable'
      ) * 100 * clamp_max(floor(uniqueCount(current_user.user_id) / 10), 1) / uniqueCount(traceId)
      FROM Transaction, TransactionError
      WHERE appName='PFML-API-${upper(var.environment_name)}'
        AND (name IS NULL or name NOT LIKE '%push_db')
        AND numeric(response.status) != 503
        AND (transactionName LIKE 'WebTransaction%' or transactionType = 'Web')
    NRQL
    evaluation_offset = 1 # offset by one window
  }

  violation_time_limit_seconds = 86400 # 24 hours

  warning {
    threshold_duration    = 300 # five minutes
    threshold             = 5   # units: percentage
    operator              = "above"
    threshold_occurrences = "at_least_once"
  }

  critical {
    threshold_duration    = 600 # ten minutes (2 windows)
    threshold             = 10  # units: percentage
    operator              = "above"
    threshold_occurrences = "all"
  }
}

resource "newrelic_nrql_alert_condition" "api_network_error_rate" {
  # WARN: error rate above 10% in any five-minute period
  # CRIT: error rate above 20% in two five-minute periods
  #
  name               = "API high rate of 503 network errors"
  policy_id          = newrelic_alert_policy.api_alerts.id
  type               = "static"
  value_function     = "single_value"
  enabled            = true
  aggregation_window = 300 # 5-minute window

  nrql {
    query             = <<-NRQL
      SELECT filter(
          count(*),
          WHERE numeric(response.status) = 503
        ) * 100 * clamp_max(floor(uniqueCount(current_user.user_id) / 10), 1) / uniqueCount(traceId)
      FROM Transaction
      WHERE appName='PFML-API-${upper(var.environment_name)}'
        AND name NOT LIKE '%push_db'
        AND transactionType = 'web'
    NRQL
    evaluation_offset = 1 # offset by one window
  }

  violation_time_limit_seconds = 86400 # 24 hours

  warning {
    threshold_duration    = 300 # five minutes
    threshold             = 10  # units: percentage
    operator              = "above"
    threshold_occurrences = "at_least_once"
  }

  critical {
    threshold_duration    = 600 # ten minutes (2 windows)
    threshold             = 20  # units: percentage
    operator              = "above"
    threshold_occurrences = "all"
  }
}

resource "newrelic_alert_condition" "api_response_time" {
  # WARN: Average response time above 750ms for at least ten minutes
  # CRIT: Average response time above 2Â½sec for at least ten minutes
  policy_id       = newrelic_alert_policy.api_alerts.id
  name            = "API response time too high"
  type            = "apm_app_metric"
  entities        = [data.newrelic_entity.pfml-api.application_id]
  metric          = "response_time_web"
  condition_scope = "application"

  term {
    priority      = "warning"
    time_function = "all" # e.g. "for at least..."
    duration      = 10    # units: minutes
    operator      = "above"
    threshold     = 0.75 # units: seconds
  }

  term {
    priority      = "critical"
    time_function = "all" # e.g. "for at least..."
    duration      = 10    # units: minutes
    operator      = "above"
    threshold     = 2.5 # units: seconds
  }
}

resource "newrelic_nrql_alert_condition" "get_claims_response_time" {
  # WARN: 95th percentile response time for GET /claims queries is > 1 second for any 15 minute period
  # CRIT: 95th percentile response time for GET /claims queries is > 2 seconds for any 15-minute period
  policy_id                    = newrelic_alert_policy.low_priority_api_alerts.id
  name                         = "GET Claims response time too high (${upper(var.environment_name)})"
  aggregation_window           = 900 # units: seconds
  value_function               = "single_value"
  violation_time_limit_seconds = 86400 # 24 hours

  nrql {
    query             = "SELECT percentile(duration, 95) FROM Transaction WHERE appName = 'PFML-API-${upper(var.environment_name)}' AND request.uri LIKE '/v1/claims' AND request.method = 'GET'"
    evaluation_offset = 1
  }

  warning {
    threshold_occurrences = "ALL"
    threshold_duration    = 900 # units: seconds
    operator              = "above"
    threshold             = 1 # units: seconds
  }

  critical {
    threshold_occurrences = "ALL"
    threshold_duration    = 900 # units: seconds
    operator              = "above"
    threshold             = 2 # units: seconds
  }
}

# ----------------------------------------------------------------------------------------------------------------------
# Alerts relating to the API's RDS database

resource "newrelic_nrql_alert_condition" "rds_high_cpu_utilization" {
  # WARN: CPU Utilization above 75% for at least 5 minutes
  # CRIT: CPU Utilization above 90% for at least 5 minutes
  policy_id      = newrelic_alert_policy.api_alerts.id
  name           = "RDS CPU Utilization too high"
  type           = "static"
  value_function = "single_value"
  enabled        = true

  nrql {
    query             = "SELECT percentile(`provider.cpuUtilization.total`, 99) from DatastoreSample where provider = 'RdsDbInstance' and displayName = data.aws_db_instance.default.db_instance_identifier"
    evaluation_offset = 3
  }

  violation_time_limit_seconds = 86400 # 24 hours

  warning {
    threshold_duration    = 300
    threshold             = 75
    operator              = "above"
    threshold_occurrences = "ALL"
  }

  critical {
    threshold_duration    = 300
    threshold             = 90
    operator              = "above"
    threshold_occurrences = "ALL"
  }
}

resource "newrelic_nrql_alert_condition" "rds_low_storage_space" {
  # WARN: RDS storage space is below 30% for at least 5 minutes
  # CRIT: RDS storage space is below 20% for at least 5 minutes
  policy_id      = newrelic_alert_policy.api_alerts.id
  name           = "RDS instance has <= 20% free storage space"
  type           = "static"
  value_function = "single_value"
  enabled        = true

  nrql {
    query             = "SELECT average(provider.freeStorageSpaceBytes.Maximum) / average(provider.allocatedStorageBytes ) * 100 FROM DatastoreSample where provider = 'RdsDbInstance' and displayName = data.aws_db_instance.default.db_instance_identifier"
    evaluation_offset = 3
  }

  violation_time_limit_seconds = 86400 # 24 hours

  warning {
    threshold_duration    = 300
    threshold             = 30
    operator              = "below"
    threshold_occurrences = "ALL"
  }

  critical {
    threshold_duration    = 300
    threshold             = 20
    operator              = "below"
    threshold_occurrences = "ALL"
  }
}

# ----------------------------------------------------------------------------------------------------------------------
# Alerts relating to abnormal traffic against the /notifications endpoint, where FINEOS POSTs new claims

resource "newrelic_nrql_alert_condition" "notifications_endpoint_infinite_email_spam" {
  # CRIT: â¥ 12 transactions to this endpoint in 15 minutes, for the same absence case ID and recipient type
  # Traffic surges have happened in the past for the same absence case ID, but different recipient types

  description    = <<-TXT
    There's too much traffic on the notifications endpoint for a specific absence case ID & specific type of recipient.
    This usually means FINEOS is stuck in an infinite loop and is sending huge quantities of emails to a real human.
    This can also mean E2E testing traffic against nonprod is producing a false positive (see INFRA-637).
    This alarm SHOULD never go off in prod, now that FINEOS has released their 6/26/2021 service pack.
  TXT
  name           = "(${upper(var.environment_name)}) Notifications endpoint spam alert"
  policy_id      = newrelic_alert_policy.api_alerts.id
  type           = "static"
  value_function = "single_value"
  enabled        = true

  aggregation_window           = 900    # 15 minutes
  violation_time_limit_seconds = 172800 # 72 hours; longest surge yet recorded lasted about four days

  nrql {
    query             = <<-NRQL
      SELECT count(*) FROM Transaction
      WHERE appName = 'PFML-API-${upper(var.environment_name)}' AND request.uri = '/v1/notifications'
      FACET notification.absence_case_id, notification.recipient_type
    NRQL
    evaluation_offset = 1
  }

  critical {
    threshold             = 11  # to emulate a 'greater than or equal to 12' threshold
    threshold_duration    = 900 # 15 minutes
    operator              = "above"
    threshold_occurrences = "all"
  }
}

# ----------------------------------------------------------------------------------------------------------------------
# Alarms relating to problems in the payments pipeline

module "payments_errors_from_fineos" {
  count  = (var.environment_name == "prod") ? 1 : 0
  source = "../newrelic_single_error_alarm"

  enabled   = true
  name      = "Errors encountered by the payments-fineos-process task"
  policy_id = (var.environment_name == "prod") ? newrelic_alert_policy.low_priority_api_alerts.id : newrelic_alert_policy.api_alerts.id

  nrql = <<-NRQL
    SELECT count(*) FROM Log
    WHERE aws.logGroup = 'service/pfml-api-prod/ecs-tasks'
      AND aws.logStream LIKE 'prod/payments-fineos-process/%'
      AND levelname = 'ERROR'
  NRQL
}

module "payments_errors_from_comptroller" {
  count  = (var.environment_name == "prod") ? 1 : 0
  source = "../newrelic_single_error_alarm"

  enabled   = true
  name      = "Errors encountered by the payments-ctr-process task"
  policy_id = (var.environment_name == "prod") ? newrelic_alert_policy.low_priority_api_alerts.id : newrelic_alert_policy.api_alerts.id

  nrql = <<-NRQL
    SELECT count(*) FROM Log
    WHERE aws.logGroup = 'service/pfml-api-prod/ecs-tasks'
      AND aws.logStream LIKE 'prod/payments-ctr-process/%'
      AND levelname = 'ERROR'
    NRQL
}

# ----------------------------------------------------------------------------------------------------------------------
# Alarms relating to problems in the PUB delegated payments pipeline

module "pub_delegated_payments_errors" {
  count  = (var.environment_name == "prod") ? 1 : 0
  source = "../newrelic_single_error_alarm"

  enabled   = true
  name      = "Errors encountered by a PUB delegated payments ECS task"
  policy_id = newrelic_alert_policy.low_priority_api_alerts.id

  nrql = <<-NRQL
    SELECT count(*) FROM Log
    WHERE aws.logGroup = 'service/pfml-api-${var.environment_name}/ecs-tasks'
      AND aws.logStream LIKE '${var.environment_name}/pub-payments%'
      AND levelname = 'ERROR'
  NRQL
}

module "pub_delegated_payments_ecs_task_failures" {
  source    = "../newrelic_single_error_alarm"
  policy_id = (var.environment_name == "prod") ? newrelic_alert_policy.api_alerts.id : newrelic_alert_policy.low_priority_api_alerts.id

  enabled = true
  name    = "PUB delegated payments ECS task failed"
  nrql    = <<-NRQL
    SELECT count(*) FROM Log
    WHERE aws.logGroup = 'service/pfml-api-${var.environment_name}/ecs-tasks'
      AND aws.logStream LIKE '${var.environment_name}/pub-payments%'
      AND message LIKE 'Traceback%'
  NRQL
}

# ------------------------------------------------------------------------------------------------------------------------------

resource "newrelic_nrql_alert_condition" "unsuccessful_register_leave_admin_job" {
  # WARN: Register Leave Admin job has only run 2 times in the past hour
  # CRIT: Register Leave Admin job has not run in the past hour
  name           = "Leave admin registration in FINEOS is failing in ${upper(var.environment_name)}"
  policy_id      = newrelic_alert_policy.api_alerts.id
  type           = "static"
  value_function = "single_value"
  fill_option    = "last_value"
  enabled        = true

  nrql {
    query             = <<-NRQL
    SELECT count(*) AS 'Successful Job completion' FROM Log WHERE message = 'Completed FINEOS Leave Admin Creation Script'
    AND aws.logGroup = 'service/pfml-api-${var.environment_name}/ecs-tasks'
    NRQL
    evaluation_offset = 3 # recommended offset from the Terraform docs for this resource
  }

  violation_time_limit_seconds = 86400 # one day

  warning {
    threshold_duration    = 1800 # thirty minutes
    threshold             = 1    # register leave admin job ran only twice in an hour
    operator              = "below"
    threshold_occurrences = "all"
  }

  critical {
    threshold_duration    = 3600 # sixty minutes
    threshold             = 1    # register leave admin job didn't run at all in an hour
    operator              = "below"
    threshold_occurrences = "all"
  }
}

# ------------------------------------------------------------------------------------------------------------------------------

resource "newrelic_nrql_alert_condition" "unprocessed_leave_admin_records" {
  # WARN: Register Leave Admin job has left unprocessed records twice
  # CRIT: Register Leave Admin job has left unprocessed records four times
  name           = "Leave admin registration in FINEOS did not process all records in ${upper(var.environment_name)}"
  policy_id      = newrelic_alert_policy.api_alerts.id
  type           = "static"
  value_function = "single_value"
  fill_option    = "last_value"
  enabled        = true

  nrql {
    query             = <<-NRQL
    SELECT COUNT(*) as 'Job runs with unprocessed records' FROM Log WHERE message LIKE '%Leave admin records left unprocessed%' AND aws.logGroup = 'service/pfml-api-${var.environment_name}/ecs-tasks' AND numeric(`Left unprocessed`) > 0
    NRQL
    evaluation_offset = 3 # recommended offset from the Terraform docs for this resource
  }

  violation_time_limit_seconds = 86400 # one day

  warning {
    threshold_duration    = 4500 # seventy-five minutes, job runs every 15 minutes, should account for ~4-5 runs
    threshold             = 1    # more than 1 of the past 4-5 runs has left unprocessed records
    operator              = "above"
    threshold_occurrences = "at_least_once"
  }

  critical {
    threshold_duration    = 4500 # seventy-five minutes, job runs every 15 minutes, should account for ~4-5 runs
    threshold             = 3    # 4 of the past 4-5 runs have left unprocessed records
    operator              = "above"
    threshold_occurrences = "at_least_once"
  }
}

# ------------------------------------------------------------------------------------------------------------------------------

module "newrelic_alert_info_request-errors" {
  source    = "../newrelic_baseline_error_rate"
  policy_id = newrelic_alert_policy.api_alerts.id

  name  = "Info request error rate too high"
  query = <<-NRQL
    SELECT percentage(
            COUNT(*), WHERE status_code != '200' 
          ) FROM Log
          WHERE method in ('GET', 'PATCH') AND path LIKE '/v1/employers/claims/%/review' and aws.logGroup like '%${var.environment_name}' and status_code IS NOT NULL
    NRQL
}

# ----------------------------------------------------------------------------------------------------------------------
# Alarms related to unexpected startup issues

module "unexpected_import_error" {
  source = "../newrelic_single_error_alarm"

  enabled   = true
  name      = "Unexpected import error"
  policy_id = (var.environment_name == "prod") ? newrelic_alert_policy.low_priority_api_alerts.id : newrelic_alert_policy.api_alerts.id

  nrql = <<-NRQL
    SELECT count(*) FROM Log
    WHERE message LIKE '%Unable to import module%'
      AND aws.logGroup LIKE '%${var.environment_name}%'
  NRQL
}
