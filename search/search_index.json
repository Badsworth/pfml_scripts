{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Massachusetts PFML (Portal and API) Applications to support claimants for the Paid Family and Medical Leave program.","title":"Massachusetts PFML (Portal and API)"},{"location":"#massachusetts-pfml-portal-and-api","text":"Applications to support claimants for the Paid Family and Medical Leave program.","title":"Massachusetts PFML (Portal and API)"},{"location":"contributing/","text":"Contributing Delivery Workflow Please see the Delivery Workflow Confluence page for guidance on how to pick up tickets, when to deploy, which environment to deploy to, etc. Local Development For practical instructions on getting set up locally, see the repo README . Below are guidelines for working once you're ready. Commit in your own branch, and include your name in the branch name, e.g. lorenyu/pfml-123-new-feature . Try to keep commits small and focused on one thing. If you find yourself using \u201cand\u201d and \u201calso\u201d in your title or description, that may be a sign to break things up into smaller chunks. You can selectively git add files or use the interactive option: git add -p . Note that after a PR is merged, your commit message history will be squashed and rewritten. Your local commit history will persist on the PR's page unless you force-push a squashed version of the branch. Informative commit messages can help when context-switching between branches, or for PR clarity in larger PRs. See Seven Rules for an external reference on commit message best practices. To keep your branch updated with others' changes, you can: Rebase your changes onto main with git rebase main , or Merge main onto your own branch: git merge main . The favored pathway to keep your branch updated is to use git merge main. It's still possible to rebase and force-push your branch using git rebase main , but this pathway should be avoided if your branch is being worked on by more than one person at a time, due to the risk of causing unnecessary conflicts. Code Owners Many parts of the code have owners, defined in the CODEOWNERS file . Code owners help to improve the quality of the code by: - Reviewing changes (see below) - Encouraging consistency and maintainability - Refactoring the code as it grows Normally each module or file has two or more owners that share these tasks. Code Reviews Code reviews are intended to help all of us grow as engineers and improve the quality of what we ship. These guidelines are meant to reinforce those two goals. For authors or requesters: Include the JIRA ticket number in the title if it exists. For example: PFML-123: Implement API endpoint Include a GitHub label if relevant. These labels can be a helpful wayfinding and context building tool, especially for new engineers onboarding into the codebase. The architecture label should be used for changes to abstractions or interfaces that usually affect multiple existing files. The pattern label should be used for a new structure or interface that will be reused later. The operations label should be used for changes to engineering operations, such as dev set-up or deployment. If your PR is a work-in-progress, or if you are looking for specific feedback on things, create a Draft Pull Request and state what you are looking for in the description. Your PR should be small enough that a reviewer can reasonably respond within 1 business day. For larger changes, break them down into a series of PRs. If refactors are included in your changes, try to split them out as recommended below. As a PR writer, you should consider your description and comments as documentation; current and future team members will refer to it to understand your design decisions. Include relevant context and business requirements, and add preemptive comments (in code or PR) for sections of code that may be confusing or worth debate. When you're ready to request a review, consider including folks who are less familiar with that part of the codebase. This allows others to see what you're working on, along with your development/communication style and ways of working. Once you've received feedback, acknowledge each comment with a response or code change. For reviewers: Aim to respond to code reviews within 1 business day. Remember to highlight things that you like and appreciate while reading through the changes, and to make any other feedback clearly actionable by indicating if it is optional preference, an important consideration, or an error. Don't be afraid to comment with a question, or to ask for clarification, or provide a suggestion, whenever you don\u2019t understand what is going on at first glance \u2014 or if you think an approach or decision can be improved. Code reviews give us a chance to learn from one another, and to reflect, iterate on, and document why certain decisions are made. Once you're ready to approve or request changes, err on the side of trust. Send a vote of approval if the PR looks ready except for small minor changes, and trust that the recipient will address your comments before merging by replying via comment or code to any asks. Use \"request changes\" sparingly, unless there's a blocking issue or major refactors that should be done. Auto assignment Code review assignment can be setup to occur automatically when a Pull Request is created, through a combination of the CODEOWNERS file and GitHub teams . Teams exist for the API and Portal engineers: API engineers: @pfml-api Portal engineers: @pfml-portal The Portal engineering team is configured so that PR reviews are automatically assigned to a member of the team via a round robin algorithm, with the goal of equally distributing PR reviews across the team. Since the Portal engineering team includes those working on either the Claimant Portal or Employer Portal, PR reviews are distributed between both groups. Pull Request Titles Please title Pull Requests in GitHub as \"API-nnn: summary\", \"CP-nnn: summary\", etc. For tickets that have subtasks, use the parent ticket's id in the title, and include the subtask in the PR description. When making a fix with no ticket, please use \"API: summary\" or \"Portal: summary\". This makes it easier to search for PRs and also links from JIRA tickets to GitHub PRs automatically. Merging a PR How to merge When merging a PR, use the PR description as a starting point. Copy it into the merge commit body and clean it up as desired. Example: PFML-540: Implement GET claims endpoint (#145) https://jira.cms.gov/browse/PFML-540 Implements /claims endpoint which is a GET that takes a valid user ID and returns their first claim from the database. Demo: Unit tests and adhoc testing via swagger UI. When to merge A PR should only be merged when all its automated checks have returned green and when the PR has received at least one approval. Although GitHub does not block the merge of a PR that fails its checks, any failed checks should be treated as a blocking issue and resolved before the PR is merged. Merges & the API database A particular source of danger when merging a PR is the risk that the merge will introduce conflicts to the PFML API's database schema. This scenario is not uncommon, especially for branches that introduce new database migrations and that have not yet been updated with the latest changes to main. The API's test suite cannot detect these conflicts, so they have historically been discovered only after PR merge, when deployment of the API begins to fail. To mitigate this risk, any engineer who introduces new database migrations to the API should pay special attention to the \"Pre-Merge Conflict Check\", an automated check that should appear on any pull request that makes changes to api/massgov/pfml/db/migrations/versions . This special 'pseudo-check' will diff your branch against main, and fail if it detects that your branch is missing any database migrations that are already present on main. To mitigate the risk that updates to main will, occasionally, introduce new database conflicts to unmerged branches, this pseudo-check will automatically re-run itself whenever a new database migration is pushed to main, and additionally whenever new commits are pushed to your PR's feature branch. For these reasons, please make sure you check the status of any database conflicts before merging an approved PR. To avoid these conflicts, any PR that alters the database must be updated with the latest changes to main before being merged.","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#delivery-workflow","text":"Please see the Delivery Workflow Confluence page for guidance on how to pick up tickets, when to deploy, which environment to deploy to, etc.","title":"Delivery Workflow"},{"location":"contributing/#local-development","text":"For practical instructions on getting set up locally, see the repo README . Below are guidelines for working once you're ready. Commit in your own branch, and include your name in the branch name, e.g. lorenyu/pfml-123-new-feature . Try to keep commits small and focused on one thing. If you find yourself using \u201cand\u201d and \u201calso\u201d in your title or description, that may be a sign to break things up into smaller chunks. You can selectively git add files or use the interactive option: git add -p . Note that after a PR is merged, your commit message history will be squashed and rewritten. Your local commit history will persist on the PR's page unless you force-push a squashed version of the branch. Informative commit messages can help when context-switching between branches, or for PR clarity in larger PRs. See Seven Rules for an external reference on commit message best practices. To keep your branch updated with others' changes, you can: Rebase your changes onto main with git rebase main , or Merge main onto your own branch: git merge main . The favored pathway to keep your branch updated is to use git merge main. It's still possible to rebase and force-push your branch using git rebase main , but this pathway should be avoided if your branch is being worked on by more than one person at a time, due to the risk of causing unnecessary conflicts.","title":"Local Development"},{"location":"contributing/#code-owners","text":"Many parts of the code have owners, defined in the CODEOWNERS file . Code owners help to improve the quality of the code by: - Reviewing changes (see below) - Encouraging consistency and maintainability - Refactoring the code as it grows Normally each module or file has two or more owners that share these tasks.","title":"Code Owners"},{"location":"contributing/#code-reviews","text":"Code reviews are intended to help all of us grow as engineers and improve the quality of what we ship. These guidelines are meant to reinforce those two goals.","title":"Code Reviews"},{"location":"contributing/#for-authors-or-requesters","text":"Include the JIRA ticket number in the title if it exists. For example: PFML-123: Implement API endpoint Include a GitHub label if relevant. These labels can be a helpful wayfinding and context building tool, especially for new engineers onboarding into the codebase. The architecture label should be used for changes to abstractions or interfaces that usually affect multiple existing files. The pattern label should be used for a new structure or interface that will be reused later. The operations label should be used for changes to engineering operations, such as dev set-up or deployment. If your PR is a work-in-progress, or if you are looking for specific feedback on things, create a Draft Pull Request and state what you are looking for in the description. Your PR should be small enough that a reviewer can reasonably respond within 1 business day. For larger changes, break them down into a series of PRs. If refactors are included in your changes, try to split them out as recommended below. As a PR writer, you should consider your description and comments as documentation; current and future team members will refer to it to understand your design decisions. Include relevant context and business requirements, and add preemptive comments (in code or PR) for sections of code that may be confusing or worth debate. When you're ready to request a review, consider including folks who are less familiar with that part of the codebase. This allows others to see what you're working on, along with your development/communication style and ways of working. Once you've received feedback, acknowledge each comment with a response or code change.","title":"For authors or requesters:"},{"location":"contributing/#for-reviewers","text":"Aim to respond to code reviews within 1 business day. Remember to highlight things that you like and appreciate while reading through the changes, and to make any other feedback clearly actionable by indicating if it is optional preference, an important consideration, or an error. Don't be afraid to comment with a question, or to ask for clarification, or provide a suggestion, whenever you don\u2019t understand what is going on at first glance \u2014 or if you think an approach or decision can be improved. Code reviews give us a chance to learn from one another, and to reflect, iterate on, and document why certain decisions are made. Once you're ready to approve or request changes, err on the side of trust. Send a vote of approval if the PR looks ready except for small minor changes, and trust that the recipient will address your comments before merging by replying via comment or code to any asks. Use \"request changes\" sparingly, unless there's a blocking issue or major refactors that should be done.","title":"For reviewers:"},{"location":"contributing/#auto-assignment","text":"Code review assignment can be setup to occur automatically when a Pull Request is created, through a combination of the CODEOWNERS file and GitHub teams . Teams exist for the API and Portal engineers: API engineers: @pfml-api Portal engineers: @pfml-portal The Portal engineering team is configured so that PR reviews are automatically assigned to a member of the team via a round robin algorithm, with the goal of equally distributing PR reviews across the team. Since the Portal engineering team includes those working on either the Claimant Portal or Employer Portal, PR reviews are distributed between both groups.","title":"Auto assignment"},{"location":"contributing/#pull-request-titles","text":"Please title Pull Requests in GitHub as \"API-nnn: summary\", \"CP-nnn: summary\", etc. For tickets that have subtasks, use the parent ticket's id in the title, and include the subtask in the PR description. When making a fix with no ticket, please use \"API: summary\" or \"Portal: summary\". This makes it easier to search for PRs and also links from JIRA tickets to GitHub PRs automatically.","title":"Pull Request Titles"},{"location":"contributing/#merging-a-pr","text":"","title":"Merging a PR"},{"location":"contributing/#how-to-merge","text":"When merging a PR, use the PR description as a starting point. Copy it into the merge commit body and clean it up as desired. Example: PFML-540: Implement GET claims endpoint (#145) https://jira.cms.gov/browse/PFML-540 Implements /claims endpoint which is a GET that takes a valid user ID and returns their first claim from the database. Demo: Unit tests and adhoc testing via swagger UI.","title":"How to merge"},{"location":"contributing/#when-to-merge","text":"A PR should only be merged when all its automated checks have returned green and when the PR has received at least one approval. Although GitHub does not block the merge of a PR that fails its checks, any failed checks should be treated as a blocking issue and resolved before the PR is merged.","title":"When to merge"},{"location":"contributing/#merges-the-api-database","text":"A particular source of danger when merging a PR is the risk that the merge will introduce conflicts to the PFML API's database schema. This scenario is not uncommon, especially for branches that introduce new database migrations and that have not yet been updated with the latest changes to main. The API's test suite cannot detect these conflicts, so they have historically been discovered only after PR merge, when deployment of the API begins to fail. To mitigate this risk, any engineer who introduces new database migrations to the API should pay special attention to the \"Pre-Merge Conflict Check\", an automated check that should appear on any pull request that makes changes to api/massgov/pfml/db/migrations/versions . This special 'pseudo-check' will diff your branch against main, and fail if it detects that your branch is missing any database migrations that are already present on main. To mitigate the risk that updates to main will, occasionally, introduce new database conflicts to unmerged branches, this pseudo-check will automatically re-run itself whenever a new database migration is pushed to main, and additionally whenever new commits are pushed to your PR's feature branch. For these reasons, please make sure you check the status of any database conflicts before merging an approved PR. To avoid these conflicts, any PR that alters the database must be updated with the latest changes to main before being merged.","title":"Merges &amp; the API database"},{"location":"creating-environments/","text":"Setting up a new environment The easiest way to set up resources in a new environment is using the templates in /bin/bootstrap-env/ . The S3 bucket for holding tfstate files must be created first in infra/pfml-aws/s3.tf Then, individual terraform components ( env-shared , api , or portal ) may be set up. pfml$ bin/bootstrap-env/bootstrap-env.sh <new-env> <component> For example: pfml$ bin/bootstrap-env/bootstrap-env.sh test portal Note that the env-shared component must be created and applied before the API. The portal has no direct dependencies. The command above creates a new directory: infra/<component>/environments/<new-env> From the new directory, run: terraform init Depending on the component you're setting up, you may need to do a few more things outside of Terraform: env-shared / api: Confirm or configure the variables in environments/<new-env>/main.tf . Create the environment: Portal: Note: When creating a new environment for the Portal, you'll need to set cloudfront_origin_path since nothing will be deployed yet: terraform plan -var = 'cloudfront_origin_path=' terraform apply -var = 'cloudfront_origin_path=' Api: When applying changes, provide an initial application version to deploy. Since this requires a version that is built and pushed to ECR, it's easiest to use the latest version on main. $ git rev-parse main 82043ae1e04621251fb9e7896176b38c884f027e $ terraform apply -var = 'service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e' Other components: terraform plan If everything from the plan looks right, then: terraform apply Follow additional steps specific to each component: Additional steps for new Portal environments Add the new environment to the CI build matrix so it can be validated when it's changed. Setting up Custom Domains If you need a custom mass.gov domain for your environment, please follow these steps: Request an ACM cert in the AWS Console, with Email Verification. Example: Domain Name SANs paidleave-api-performance.mass.gov paidleave-api-training.mass.gov, paidleave-training.mass.gov, paidleave-performance.mass.gov Send an email to Vijay with the domain name/SANs, so he can request approval from Sarah Bourne / Chris Velluto / Bill Cole. Once EOTSS has approved the certificate, update the cert_domains map in the following files: infra/portal/template/acm.tf infra/env-shared/template/acm.tf and flip enable_pretty_domains to true in infra/env-shared/environments/$ENV/main.tf. After merging and applying the changes, ask Vijay to create a ServiceNow request for CNAME entries for the Portal and Gateway. These should all be cloudfront URLs. Check the AWS Console under API Gateway > Custom Domain Names for the API Gateway Cloudfront URLs. App CNAME URL API perf paidleave-api-performance.mass.gov https://abcd123.cloudfront.net API training paidleave-api-training.mass.gov https://zaww123.cloudfront.net Portal perf paidleave-performance.mass.gov https://vfcs123.cloudfront.net Portal training paidleave-training.mass.gov https://qwer123.cloudfront.net After they create the CNAME entries, the custom domains should direct to the appropriate applications. Configuring Cognito Our Terraform scripts enable Advanced Security, however at the time of writing, Terraform didn't support more granular configuration of the Advanced Security settings , so there are some manual steps needed: Log into the AWS Console and navigate to the Cognito User Pool for this environment. Click \"Advanced Security\" in the sidebar Configure the adaptive authentication behavior . In production and stage environments we block high-risk login attempts. This can also be configured for other environments if desired. High-risk login attempts aren't blocked in environments used for testing (test, perf, and training) so that automated test scripts don't get blocked. On the same page in AWS customize the email notification messages by copy-and-pasting the HTML email templates from infra/portal/templates/emails .","title":"Setting up a new environment"},{"location":"creating-environments/#setting-up-a-new-environment","text":"The easiest way to set up resources in a new environment is using the templates in /bin/bootstrap-env/ . The S3 bucket for holding tfstate files must be created first in infra/pfml-aws/s3.tf Then, individual terraform components ( env-shared , api , or portal ) may be set up. pfml$ bin/bootstrap-env/bootstrap-env.sh <new-env> <component> For example: pfml$ bin/bootstrap-env/bootstrap-env.sh test portal Note that the env-shared component must be created and applied before the API. The portal has no direct dependencies. The command above creates a new directory: infra/<component>/environments/<new-env> From the new directory, run: terraform init Depending on the component you're setting up, you may need to do a few more things outside of Terraform: env-shared / api: Confirm or configure the variables in environments/<new-env>/main.tf . Create the environment: Portal: Note: When creating a new environment for the Portal, you'll need to set cloudfront_origin_path since nothing will be deployed yet: terraform plan -var = 'cloudfront_origin_path=' terraform apply -var = 'cloudfront_origin_path=' Api: When applying changes, provide an initial application version to deploy. Since this requires a version that is built and pushed to ECR, it's easiest to use the latest version on main. $ git rev-parse main 82043ae1e04621251fb9e7896176b38c884f027e $ terraform apply -var = 'service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e' Other components: terraform plan If everything from the plan looks right, then: terraform apply Follow additional steps specific to each component: Additional steps for new Portal environments Add the new environment to the CI build matrix so it can be validated when it's changed.","title":"Setting up a new environment"},{"location":"creating-environments/#setting-up-custom-domains","text":"If you need a custom mass.gov domain for your environment, please follow these steps: Request an ACM cert in the AWS Console, with Email Verification. Example: Domain Name SANs paidleave-api-performance.mass.gov paidleave-api-training.mass.gov, paidleave-training.mass.gov, paidleave-performance.mass.gov Send an email to Vijay with the domain name/SANs, so he can request approval from Sarah Bourne / Chris Velluto / Bill Cole. Once EOTSS has approved the certificate, update the cert_domains map in the following files: infra/portal/template/acm.tf infra/env-shared/template/acm.tf and flip enable_pretty_domains to true in infra/env-shared/environments/$ENV/main.tf. After merging and applying the changes, ask Vijay to create a ServiceNow request for CNAME entries for the Portal and Gateway. These should all be cloudfront URLs. Check the AWS Console under API Gateway > Custom Domain Names for the API Gateway Cloudfront URLs. App CNAME URL API perf paidleave-api-performance.mass.gov https://abcd123.cloudfront.net API training paidleave-api-training.mass.gov https://zaww123.cloudfront.net Portal perf paidleave-performance.mass.gov https://vfcs123.cloudfront.net Portal training paidleave-training.mass.gov https://qwer123.cloudfront.net After they create the CNAME entries, the custom domains should direct to the appropriate applications.","title":"Setting up Custom Domains"},{"location":"creating-environments/#configuring-cognito","text":"Our Terraform scripts enable Advanced Security, however at the time of writing, Terraform didn't support more granular configuration of the Advanced Security settings , so there are some manual steps needed: Log into the AWS Console and navigate to the Cognito User Pool for this environment. Click \"Advanced Security\" in the sidebar Configure the adaptive authentication behavior . In production and stage environments we block high-risk login attempts. This can also be configured for other environments if desired. High-risk login attempts aren't blocked in environments used for testing (test, perf, and training) so that automated test scripts don't get blocked. On the same page in AWS customize the email notification messages by copy-and-pasting the HTML email templates from infra/portal/templates/emails .","title":"Configuring Cognito"},{"location":"deployment/","text":"Deployment Deployments to all environments are managed through GitHub Actions. The main branch is automatically deployed to test as pull requests are merged into it. Deployments to other environmments are triggered manually. Deploying to an environment Visit the API or Portal Github Actions homepage: API Deploy Portal Deploy Click the \"Run workflow\" button, and fill in the inputs: Provide the deployment_env from the list of environments. Provide a git branch or tag to deploy. This can be: the name of a git tag, e.g. api/v1.7.0-rc1 , or the name of a git branch, like release/api/v1.7.0 or kev/my-feature-branch . Click the green \"Run workflow\" button. Test Environment Coordination If you are testing a feature branch on the test environment, please go through the following additional steps: During Testing Communicate to #mass-pfml-deploys-shared. \"\u26a0\ufe0f I'll be using the API/Portal test environment soon, please let me know if you have any concerns.\" After running the workflow, click the \"Disable Workflow\" button to prevent auto-deploys from overriding your deployment. After Testing Click the \"Enable Workflow\" button. Re-deploy main to test. Notify the Slack channel. Branch-to-environment mapping At a quick glance, you can view the commit history for any environment based on the branch. Name of API deploy branch Name of Portal deploy branch Corresponding env main main test deploy/api/stage deploy/portal/stage stage deploy/api/prod deploy/portal/prod prod deploy/api/performance deploy/portal/performance performance deploy/api/training deploy/portal/training training deploy/api/uat deploy/portal/uat uat","title":"Deployment"},{"location":"deployment/#deployment","text":"Deployments to all environments are managed through GitHub Actions. The main branch is automatically deployed to test as pull requests are merged into it. Deployments to other environmments are triggered manually.","title":"Deployment"},{"location":"deployment/#deploying-to-an-environment","text":"Visit the API or Portal Github Actions homepage: API Deploy Portal Deploy Click the \"Run workflow\" button, and fill in the inputs: Provide the deployment_env from the list of environments. Provide a git branch or tag to deploy. This can be: the name of a git tag, e.g. api/v1.7.0-rc1 , or the name of a git branch, like release/api/v1.7.0 or kev/my-feature-branch . Click the green \"Run workflow\" button.","title":"Deploying to an environment"},{"location":"deployment/#test-environment-coordination","text":"If you are testing a feature branch on the test environment, please go through the following additional steps:","title":"Test Environment Coordination"},{"location":"deployment/#during-testing","text":"Communicate to #mass-pfml-deploys-shared. \"\u26a0\ufe0f I'll be using the API/Portal test environment soon, please let me know if you have any concerns.\" After running the workflow, click the \"Disable Workflow\" button to prevent auto-deploys from overriding your deployment.","title":"During Testing"},{"location":"deployment/#after-testing","text":"Click the \"Enable Workflow\" button. Re-deploy main to test. Notify the Slack channel.","title":"After Testing"},{"location":"deployment/#branch-to-environment-mapping","text":"At a quick glance, you can view the commit history for any environment based on the branch. Name of API deploy branch Name of Portal deploy branch Corresponding env main main test deploy/api/stage deploy/portal/stage stage deploy/api/prod deploy/portal/prod prod deploy/api/performance deploy/portal/performance performance deploy/api/training deploy/portal/training training deploy/api/uat deploy/portal/uat uat","title":"Branch-to-environment mapping"},{"location":"tools/","text":"Introduction The root of the monorepo includes tools for code style formatting and linting. Both linting and formatting are enabled in a pre-commit hook set up with husky and lint-staged . Linting This project uses ESLint for JavaScript code. npm run lint Code Formatting This project uses Prettier for opinionated code formatting. All .js and .jsx files are automatically re-written using prettier formatting as part of the pre-commit hook.","title":"Tools"},{"location":"tools/#introduction","text":"The root of the monorepo includes tools for code style formatting and linting. Both linting and formatting are enabled in a pre-commit hook set up with husky and lint-staged .","title":"Introduction"},{"location":"tools/#linting","text":"This project uses ESLint for JavaScript code. npm run lint","title":"Linting"},{"location":"tools/#code-formatting","text":"This project uses Prettier for opinionated code formatting. All .js and .jsx files are automatically re-written using prettier formatting as part of the pre-commit hook.","title":"Code Formatting"},{"location":"api/db-roles-and-users/","text":"Database Users Access to the database is broken into two parts: Roles, which are the permission sets for data access Users, what authenticates with the database and are granted Roles Roles Database roles are managed through the migrations (search for CREATE ROLE or GRANT / REVOKE in the migrations). Current roles: app , general purpose role with wide access, automatically granted all permissions on any table created in the application schema (currently public ) Users Database users are declared in /api/massgov/pfml/db/admin.py in the user_configs variable, where the list of roles (as created in migrations) they should have. For AWS environments, an IAM policy needs to be created and connected to the systems that should be able to connect as that user. There is a helper script, /api/bin/create-db-user-config.py , that will print out the configuration required for adding a new user. Local environment To create the database users locally, run: make db-create-users This will create the all users with the password specified by the environment variable DB_PASSWORD (since the actual password doesn't matter in local development). AWS environments CI will automatically run the db-admin-create-db-users task to create users after migrations on every deploy. So should generally not require manually action. But if needed, to run the task manually: ../../bin/run-ecs-task/run-task.sh <env> db-admin-create-db-users <first name>.<last name> AWS database users use IAM authentication instead of regular passwords. High level implications of IAM auth: RDS IAM authentication requires connecting to DB over SSL DB users needing to authenticate with IAM must be granted the rds_iam role IAM policy attached to service IAM roles/users allowing connection to DB as given user Output of the RDS SDK generate_db_auth_token() function used as password when connecting as user. These tokens are only valid for 15 minutes, so this function should be called to get a valid token any time a connection is to be made.","title":"Database Users"},{"location":"api/db-roles-and-users/#database-users","text":"Access to the database is broken into two parts: Roles, which are the permission sets for data access Users, what authenticates with the database and are granted Roles","title":"Database Users"},{"location":"api/db-roles-and-users/#roles","text":"Database roles are managed through the migrations (search for CREATE ROLE or GRANT / REVOKE in the migrations). Current roles: app , general purpose role with wide access, automatically granted all permissions on any table created in the application schema (currently public )","title":"Roles"},{"location":"api/db-roles-and-users/#users","text":"Database users are declared in /api/massgov/pfml/db/admin.py in the user_configs variable, where the list of roles (as created in migrations) they should have. For AWS environments, an IAM policy needs to be created and connected to the systems that should be able to connect as that user. There is a helper script, /api/bin/create-db-user-config.py , that will print out the configuration required for adding a new user.","title":"Users"},{"location":"api/db-roles-and-users/#local-environment","text":"To create the database users locally, run: make db-create-users This will create the all users with the password specified by the environment variable DB_PASSWORD (since the actual password doesn't matter in local development).","title":"Local environment"},{"location":"api/db-roles-and-users/#aws-environments","text":"CI will automatically run the db-admin-create-db-users task to create users after migrations on every deploy. So should generally not require manually action. But if needed, to run the task manually: ../../bin/run-ecs-task/run-task.sh <env> db-admin-create-db-users <first name>.<last name> AWS database users use IAM authentication instead of regular passwords. High level implications of IAM auth: RDS IAM authentication requires connecting to DB over SSL DB users needing to authenticate with IAM must be granted the rds_iam role IAM policy attached to service IAM roles/users allowing connection to DB as given user Output of the RDS SDK generate_db_auth_token() function used as password when connecting as user. These tokens are only valid for 15 minutes, so this function should be called to get a valid token any time a connection is to be made.","title":"AWS environments"},{"location":"api/environment-variables/","text":"Environment Variables We configure the application by using environment variables . Local Development During local development, we specify environment variables through docker-compose.yml . mass-pfml-api : ... environment : - ENVIRONMENT=local - DB_HOST=mass-pfml-db - DB_NAME=pfml - DB_USERNAME=pfml - DB_PASSWORD=secret123 When updating these variables, you'll need to run make build in order to rebuild your container and pick up the new values. Overriding AWS credentials To use your AWS credentials locally: Run the login-aws script Override the container's AWS credentials path in docker-compose.override.yml (there's a commented line showing an example) Set the AWS_PROFILE in docker-compose.yml to the AWS profile you want to use Rerun make build Deployed Environments In deployed environments, variables are pulled in through AWS Elastic Container Service (ECS) as listed in the container definition . Non-sensitive values are encoded into the definition when Terraform generates it: \"environment\" : [ { \"name\" : \"ENVIRONMENT\" , \"value\" : \"${environment_name}\" }, { \"name\" : \"DB_HOST\" , \"value\" : \"${db_host}\" }, { \"name\" : \"DB_NAME\" , \"value\" : \"${db_name}\" }, { \"name\" : \"DB_USERNAME\" , \"value\" : \"${db_username}\" }, { ... } ] ...and sensitive values are pulled in from AWS SSM Parameter Store when the container starts: \"secrets\" : [ { \"name\" : \"DB_PASSWORD\" , \"valueFrom\" : \"/service/${app_name}/${environment_name}/db-password\" }, { \"name\" : \"NEW_RELIC_LICENSE_KEY\" , \"valueFrom\" : \"/service/${app_name}/common/newrelic-license-key\" }, { ... } ] To view or update non-sensitive values in the container definition file, set them in the container_definitions resource in service.tf . To recap for non-sensitive values: If it's a variable that should be configured explicitly for each environment: Add new variable to the API template variables.tf Set the new variables in each environment configuration Add and set variable in service.tf , either referring to variables from step 1 or to other Terraform resources. Use variable set in service.tf to set environment variable in container_definitions.json To view or update sensitive values: Go to the key in the AWS Systems Manager/Parameter Store console . Create or update the sensitive string with the default KMS key, matching the valueFrom field that you specify in the container definition above. In both cases, the application will need to be redeployed before any changes to the environment variables are picked up.","title":"Environment Variables"},{"location":"api/environment-variables/#environment-variables","text":"We configure the application by using environment variables .","title":"Environment Variables"},{"location":"api/environment-variables/#local-development","text":"During local development, we specify environment variables through docker-compose.yml . mass-pfml-api : ... environment : - ENVIRONMENT=local - DB_HOST=mass-pfml-db - DB_NAME=pfml - DB_USERNAME=pfml - DB_PASSWORD=secret123 When updating these variables, you'll need to run make build in order to rebuild your container and pick up the new values.","title":"Local Development"},{"location":"api/environment-variables/#overriding-aws-credentials","text":"To use your AWS credentials locally: Run the login-aws script Override the container's AWS credentials path in docker-compose.override.yml (there's a commented line showing an example) Set the AWS_PROFILE in docker-compose.yml to the AWS profile you want to use Rerun make build","title":"Overriding AWS credentials"},{"location":"api/environment-variables/#deployed-environments","text":"In deployed environments, variables are pulled in through AWS Elastic Container Service (ECS) as listed in the container definition . Non-sensitive values are encoded into the definition when Terraform generates it: \"environment\" : [ { \"name\" : \"ENVIRONMENT\" , \"value\" : \"${environment_name}\" }, { \"name\" : \"DB_HOST\" , \"value\" : \"${db_host}\" }, { \"name\" : \"DB_NAME\" , \"value\" : \"${db_name}\" }, { \"name\" : \"DB_USERNAME\" , \"value\" : \"${db_username}\" }, { ... } ] ...and sensitive values are pulled in from AWS SSM Parameter Store when the container starts: \"secrets\" : [ { \"name\" : \"DB_PASSWORD\" , \"valueFrom\" : \"/service/${app_name}/${environment_name}/db-password\" }, { \"name\" : \"NEW_RELIC_LICENSE_KEY\" , \"valueFrom\" : \"/service/${app_name}/common/newrelic-license-key\" }, { ... } ] To view or update non-sensitive values in the container definition file, set them in the container_definitions resource in service.tf . To recap for non-sensitive values: If it's a variable that should be configured explicitly for each environment: Add new variable to the API template variables.tf Set the new variables in each environment configuration Add and set variable in service.tf , either referring to variables from step 1 or to other Terraform resources. Use variable set in service.tf to set environment variable in container_definitions.json To view or update sensitive values: Go to the key in the AWS Systems Manager/Parameter Store console . Create or update the sensitive string with the default KMS key, matching the valueFrom field that you specify in the container definition above. In both cases, the application will need to be redeployed before any changes to the environment variables are picked up.","title":"Deployed Environments"},{"location":"api/fields-and-validations/","text":"During development of the Portal, there are common tasks an API or Portal engineer may need to perform within the API codebase. Adding fields Add the field to the DB model: db/models/applications.py Generate a new migration file See below for how to allow this field in API requests and responses. Accept fields in requests Application fields should be nullable / Optional to support the Portal sending partial request bodies in its multi-page flows. Add the field to the openapi.yaml spec. Add the field to the requests.py Pydantic model Add test coverage to assert the new field is persisted in the DB. Include fields in responses Add the field to the openapi.yaml spec Add the field to the responses.py Pydantic model Add test coverage to assert the new field is included in responses. Validation rules Validation Rules vs. Eligibility Rules We should distinguish validation rules from eligibility rules. It\u2019s one thing to automatically deny someone eligibility based on some eligibility criteria (like child birth date being within 12 months) and then allowing that person to appeal (if they have some valid extenuating circumstances), and it\u2019s an entirely different thing to prevent them from applying altogether claiming that the application isn\u2019t even a valid application. Validation should only serve to help prevent incorrect data (typos, etc) not prevent ineligible applications . We should be careful not to accidentally deny a claimant\u2019s right to an appeal or otherwise make the system less flexible than it needs to be as is so often the case with government systems. Adding validation rules Validation rules currently are enforced at three different layers: OpenAPI generally enforces a subset of rules on individual fields Pydantic models enforce some business rules on individual fields Custom code enforces presence of required fields or rules requiring context of multiple fields OpenAPI OpenAPI is the first layer a request flows through, and enforces a subset of validation rules: type format pattern enum For all endpoints, these validations always result in a 400 status code with errors when a rule is not fulfilled. These validations are located in the openapi.yml spec file. Why we avoid OpenAPI's required property To display a user friendly internationalized error message to Portal users, the error must include the field that is missing and a consistent type . For example: { \"errors\" : [ { \"field\" : \"password\" , \"type\" : \"required\" } ] } OpenAPI pitfalls: The main deal breaker is when using the OpenAPI's required property, the error response doesn't return the name of the specific field that is missing . It just returns the full array of fields that are required. This prevents Portal from displaying a useful error message. Some endpoints are split across multiple pages in the Portal so it will be expected that not all required fields are present at once in a request. Pydantic field validator Individual fields on a Pydantic model can have custom validation logic to enforce business rules or reality checks (e.g a birthdate must be in the past). These validators can raise a ValidationException to cause the API endpoint to respond with a 400 response with errors . Required fields and cross-field validations Our current approach is to use custom code is enforce the presence of fields, or more complex validation logic dependent on the values of multiple fields. The current convention for this set of validations is to: Create a *_rules.py module (e.g user_rules.py ) for the endpoints' validation logic Add get_*_issues method for validating an endpoint Call get_*_issues with the API request and check if there are any issues returned How Application validation works A get_application_issues function exists for reporting potential \"warnings\" on an application. These are primarily rules related to required or conditionally required fields, but may also relate to rules that span multiple fields. For GET and PATCH requests to the /applications/:application_id endpoint, these validations result in data still saving to our database and a 200 status code with warnings rather than errors , since we expect requests to not always have a complete application, since a user will be filling out the application through a multi-page experience. For POST requests to /applications/:application_id/complete_application and /applications/:application_id/submit_application, these validations result in a 400 status code with errors when a rule is not fulfilled. These validations are located in application_rules.py .","title":"Fields and validations"},{"location":"api/fields-and-validations/#adding-fields","text":"Add the field to the DB model: db/models/applications.py Generate a new migration file See below for how to allow this field in API requests and responses.","title":"Adding fields"},{"location":"api/fields-and-validations/#accept-fields-in-requests","text":"Application fields should be nullable / Optional to support the Portal sending partial request bodies in its multi-page flows. Add the field to the openapi.yaml spec. Add the field to the requests.py Pydantic model Add test coverage to assert the new field is persisted in the DB.","title":"Accept fields in requests"},{"location":"api/fields-and-validations/#include-fields-in-responses","text":"Add the field to the openapi.yaml spec Add the field to the responses.py Pydantic model Add test coverage to assert the new field is included in responses.","title":"Include fields in responses"},{"location":"api/fields-and-validations/#validation-rules","text":"","title":"Validation rules"},{"location":"api/fields-and-validations/#validation-rules-vs-eligibility-rules","text":"We should distinguish validation rules from eligibility rules. It\u2019s one thing to automatically deny someone eligibility based on some eligibility criteria (like child birth date being within 12 months) and then allowing that person to appeal (if they have some valid extenuating circumstances), and it\u2019s an entirely different thing to prevent them from applying altogether claiming that the application isn\u2019t even a valid application. Validation should only serve to help prevent incorrect data (typos, etc) not prevent ineligible applications . We should be careful not to accidentally deny a claimant\u2019s right to an appeal or otherwise make the system less flexible than it needs to be as is so often the case with government systems.","title":"Validation Rules vs. Eligibility Rules"},{"location":"api/fields-and-validations/#adding-validation-rules","text":"Validation rules currently are enforced at three different layers: OpenAPI generally enforces a subset of rules on individual fields Pydantic models enforce some business rules on individual fields Custom code enforces presence of required fields or rules requiring context of multiple fields","title":"Adding validation rules"},{"location":"api/fields-and-validations/#openapi","text":"OpenAPI is the first layer a request flows through, and enforces a subset of validation rules: type format pattern enum For all endpoints, these validations always result in a 400 status code with errors when a rule is not fulfilled. These validations are located in the openapi.yml spec file. Why we avoid OpenAPI's required property To display a user friendly internationalized error message to Portal users, the error must include the field that is missing and a consistent type . For example: { \"errors\" : [ { \"field\" : \"password\" , \"type\" : \"required\" } ] } OpenAPI pitfalls: The main deal breaker is when using the OpenAPI's required property, the error response doesn't return the name of the specific field that is missing . It just returns the full array of fields that are required. This prevents Portal from displaying a useful error message. Some endpoints are split across multiple pages in the Portal so it will be expected that not all required fields are present at once in a request.","title":"OpenAPI"},{"location":"api/fields-and-validations/#pydantic-field-validator","text":"Individual fields on a Pydantic model can have custom validation logic to enforce business rules or reality checks (e.g a birthdate must be in the past). These validators can raise a ValidationException to cause the API endpoint to respond with a 400 response with errors .","title":"Pydantic field validator"},{"location":"api/fields-and-validations/#required-fields-and-cross-field-validations","text":"Our current approach is to use custom code is enforce the presence of fields, or more complex validation logic dependent on the values of multiple fields. The current convention for this set of validations is to: Create a *_rules.py module (e.g user_rules.py ) for the endpoints' validation logic Add get_*_issues method for validating an endpoint Call get_*_issues with the API request and check if there are any issues returned","title":"Required fields and cross-field validations"},{"location":"api/fields-and-validations/#how-application-validation-works","text":"A get_application_issues function exists for reporting potential \"warnings\" on an application. These are primarily rules related to required or conditionally required fields, but may also relate to rules that span multiple fields. For GET and PATCH requests to the /applications/:application_id endpoint, these validations result in data still saving to our database and a 200 status code with warnings rather than errors , since we expect requests to not always have a complete application, since a user will be filling out the application through a multi-page experience. For POST requests to /applications/:application_id/complete_application and /applications/:application_id/submit_application, these validations result in a 400 status code with errors when a rule is not fulfilled. These validations are located in application_rules.py .","title":"How Application validation works"},{"location":"api/monitoring/","text":"API Monitoring We use New Relic (for performance monitoring, synthetics, and general queries) and AWS CloudWatch (for infrastructure-level concerns) to monitor the PFML API. We get CloudWatch metrics for free just by deploying the API on AWS Fargate, but the New Relic metrics are sourced from a Python agent that is initialized during server startup. AWS CloudWatch Alarms index SNS topics index Available CloudWatch metrics The API's AWS monitoring is defined in Terraform under infra/api/template/alarms.tf . When an alarm triggers, it will push an event to one or more SNS topics, also defined in alarms.tf . If configured to do so, SNS events can also be generated when an alarm stops alarming and returns to a normal state. To subscribe to AWS alerts, you can do one of two things. You can use the AWS console to create a temporary subscription to one or more SNS topics, but these subscriptions will only last until someone re-applies the canonical Terraform config. You can define a new aws_sns_topic_subscription object in alarms.tf . New Relic Main index Note: Includes applications not owned by Nava. Filter by pfml to see only the PFML API. Most configuration is sourced from api/newrelic.ini except for the license key, which is sourced from AWS SSM and provided at runtime as an environment variable. At this time, New Relic API monitoring is limited only to data collection. Alarms have not yet been configured here.","title":"API Monitoring"},{"location":"api/monitoring/#api-monitoring","text":"We use New Relic (for performance monitoring, synthetics, and general queries) and AWS CloudWatch (for infrastructure-level concerns) to monitor the PFML API. We get CloudWatch metrics for free just by deploying the API on AWS Fargate, but the New Relic metrics are sourced from a Python agent that is initialized during server startup.","title":"API Monitoring"},{"location":"api/monitoring/#aws-cloudwatch","text":"Alarms index SNS topics index Available CloudWatch metrics The API's AWS monitoring is defined in Terraform under infra/api/template/alarms.tf . When an alarm triggers, it will push an event to one or more SNS topics, also defined in alarms.tf . If configured to do so, SNS events can also be generated when an alarm stops alarming and returns to a normal state. To subscribe to AWS alerts, you can do one of two things. You can use the AWS console to create a temporary subscription to one or more SNS topics, but these subscriptions will only last until someone re-applies the canonical Terraform config. You can define a new aws_sns_topic_subscription object in alarms.tf .","title":"AWS CloudWatch"},{"location":"api/monitoring/#new-relic","text":"Main index Note: Includes applications not owned by Nava. Filter by pfml to see only the PFML API. Most configuration is sourced from api/newrelic.ini except for the license key, which is sourced from AWS SSM and provided at runtime as an environment variable. At this time, New Relic API monitoring is limited only to data collection. Alarms have not yet been configured here.","title":"New Relic"},{"location":"api/ses/","text":"Configuring ECS Tasks to send emails In certain cases, it may make sense to send emails from a running application. The instructions below list out the required steps for getting your ECS task configured with the appropriate permissions. Allow your task IAM Role to send emails By default, your ECS task does not have any AWS permissions. IAM roles define what they are allowed to do, with which resources (like an SES email!) There are two IAM roles for a task: the task IAM role and the execution IAM role. The task IAM role defines the permissions used during application runtime. The execution IAM role defines the permissions needed before the task starts. Since you'll be sending emails during an application's runtime, the task IAM role policy will need to be configured with permissions to send emails. This might look like this: statement { sid = \"AllowSESSendEmail\" effect = \"Allow\" actions = [ \"ses:SendEmail\" , \"ses:SendRawEmail\" ] condition { test = \"ForAllValues:StringLike\" variable = \"ses:Recipients\" values = [ # Any email addresses you expect to communicate with. ] } resources = [ \"*\" ] } Add your IAM role name to the SES allowlist Additionally, we maintain a strict list of resources that are allowed to send emails with each of our email addresses in ses.tf . You'll want to update the condition in data.aws_iam_policy_document.restrict_ses_senders to include your IAM roles in this pattern: arn:aws:sts:: ${ data . aws_caller_identity . current . account_id } :assumed-role/MY_IAM_ROLE_PATTERN /*\" Note that this is the name of the IAM role itself, not the policy. e.g. use the name in this block: resource \"aws_iam_role\" \"payments_fineos_process_task_role\" { name = \"${local.app_name}-${var.environment_name}-ecs-tasks-payments-fineos-process\" ... }","title":"Ses"},{"location":"api/ses/#configuring-ecs-tasks-to-send-emails","text":"In certain cases, it may make sense to send emails from a running application. The instructions below list out the required steps for getting your ECS task configured with the appropriate permissions.","title":"Configuring ECS Tasks to send emails"},{"location":"api/ses/#allow-your-task-iam-role-to-send-emails","text":"By default, your ECS task does not have any AWS permissions. IAM roles define what they are allowed to do, with which resources (like an SES email!) There are two IAM roles for a task: the task IAM role and the execution IAM role. The task IAM role defines the permissions used during application runtime. The execution IAM role defines the permissions needed before the task starts. Since you'll be sending emails during an application's runtime, the task IAM role policy will need to be configured with permissions to send emails. This might look like this: statement { sid = \"AllowSESSendEmail\" effect = \"Allow\" actions = [ \"ses:SendEmail\" , \"ses:SendRawEmail\" ] condition { test = \"ForAllValues:StringLike\" variable = \"ses:Recipients\" values = [ # Any email addresses you expect to communicate with. ] } resources = [ \"*\" ] }","title":"Allow your task IAM Role to send emails"},{"location":"api/ses/#add-your-iam-role-name-to-the-ses-allowlist","text":"Additionally, we maintain a strict list of resources that are allowed to send emails with each of our email addresses in ses.tf . You'll want to update the condition in data.aws_iam_policy_document.restrict_ses_senders to include your IAM roles in this pattern: arn:aws:sts:: ${ data . aws_caller_identity . current . account_id } :assumed-role/MY_IAM_ROLE_PATTERN /*\" Note that this is the name of the IAM role itself, not the policy. e.g. use the name in this block: resource \"aws_iam_role\" \"payments_fineos_process_task_role\" { name = \"${local.app_name}-${var.environment_name}-ecs-tasks-payments-fineos-process\" ... }","title":"Add your IAM role name to the SES allowlist"},{"location":"api/writing-tests/","text":"Writing Tests pytest is our test runner, which is simple but powerful. If you are new to pytest, reading up on how fixtures work in particular might be helpful as it's one area that is a bit different than is common with other runners (and languages). Naming pytest automatically discovers tests by following a number of conventions (what it calls \"collection\"). For this project specifically: All tests live under tests/ Under tests/ , the organization mirrors the source code structure, but without the massgov/pfml/ part, so for example: The tests for massgov/pfml/api/ live under tests/api/ massgov/pfml/util/aws/ under test/util/aws/ . Create __init__.py files for each directory. This helps avoid name conflicts when pytest is resolving tests . Test files should begin with the test_ prefix, followed by the module the tests cover, for example, a file foo.py will have tests in a file test_foo.py . Tests for massgov/pfml/api/applications.py live at tests/api/test_applications.py Test cases should begin with the test_ prefix, followed by the function it's testing and some description of what about the function it is testing. In tests/api/test_users.py , the test_users_patch_404 function is a test (because it begins with test_ ), that covers the users_patch function's behavior around 404 responses. Tests can be grouped in classes starting with Test , methods that start with test_ will be picked up as tests, for example TestFeature::test_scenario . There are occasions where tests may not line up exactly with a single source file, function, or otherwise may need to deviate from this exact structure, but this is the setup in general. conftest files conftest.py files are automatically loaded by pytest, making their contents available to tests without needing to be imported. They are an easy place to put shared test fixtures as well as define other pytest configuration (define hooks, load plugins, define new/override assert behavior, etc.). They should never be imported directly. The main tests/conftest.py holds widely useful fixtures included for all tests. Scoped conftest.py files can be created that apply only to the tests below them in the directory hierarchy, for example, the tests/db/conftest.py file is only loaded for tests under tests/db/ . More info: https://docs.pytest.org/en/latest/fixture.html#conftest-py-sharing-fixture-functions Helpers If there is useful functionality that needs to be shared between tests, but is only applicable to testing and is not a fixture, create modules under tests/helpers/ . They can be imported into tests from the path tests.helpers , for example, from tests.helpers.foo import helper_func . Using Factories To facilitate easier setup of test data, most database models have factories via factory_boy in massgov/pfml/db/models/factories.py . There are a few different ways of using the factories , termed \"strategies\": build, create, and stub. Most notably for this project: The build strategy via FooFactory.build() populates a model class with the generated data, but does not attempt to write it to the database The create strategy via FooFactory.create() writes a generated model to the database (can think of it like FooFactory.build() then db_session.add() and db_session.commit() ) The build strategy is useful if the code under test just needs the data on the model and doesn't actually perform any database interactions. In order to use the create strategy, pull in the initialize_factories_session fixture. Regardless of the strategy, can override the values for attributes on the generated models by passing them into the factory call, for example: FooFactory . build ( foo_id = 5 , name = \"Bar\" ) would set foo_id=5 and name=\"Bar\" on the generated model, while all other attributes would use what's configured on the factory class. For creating a collection of generated models, there are batch methods, build_batch() and create_batch() , which will create multiple instances. For example: FooFactory . build_batch ( size = 5 ) will return 5 Foo instances with different data. Attributes set in the batch call will be shared among all the instances, so: FooFactory . build_batch ( size = 2 , parent_widget = widget ) would create 2 Foo instances with widget set as their parent_widget . Integration test marker An integration marker is configured in pytest for the project. Any test that requires a real database connection or any concrete \"resource\" outside of the code itself should be tagged with the integration marker. It indicates an \"integration\" test, as opposed to a \"unit\" test, in a somewhat loose sense. A few common situations are easy cases, if a test is covering API behavior via fixtures like app or client or testing state in the database with test_db_session , the test should be marked with integration . Accessing real files is a bit of a gray area. If testing code that needs file-like objects, should generally prefer using in-memory constructs like StringIO or BytesIO to avoid ever touching the filesystem. But currently if a test needs to load test fixture files or use tmp_path to work with a real file for some purpose, those do not need to be tagged integration . Decorate any individual test with @pytest.mark.integration . If all (or almost all) tests in a given test file are integration tests, they can be tagged all at once with a declaration like the following at the top of the file (after imports): # every test in here requires real resources pytestmark = pytest . mark . integration If a test file has a large mix of integration and unit tests that don't make sense to separate, integration tests can be bundled into a test class which can then be tagged, for example: @pytest . mark . integration class TestIntegrations : (but tagging each individual function with @pytest.mark.integration is also acceptable)","title":"Writing Tests"},{"location":"api/writing-tests/#writing-tests","text":"pytest is our test runner, which is simple but powerful. If you are new to pytest, reading up on how fixtures work in particular might be helpful as it's one area that is a bit different than is common with other runners (and languages).","title":"Writing Tests"},{"location":"api/writing-tests/#naming","text":"pytest automatically discovers tests by following a number of conventions (what it calls \"collection\"). For this project specifically: All tests live under tests/ Under tests/ , the organization mirrors the source code structure, but without the massgov/pfml/ part, so for example: The tests for massgov/pfml/api/ live under tests/api/ massgov/pfml/util/aws/ under test/util/aws/ . Create __init__.py files for each directory. This helps avoid name conflicts when pytest is resolving tests . Test files should begin with the test_ prefix, followed by the module the tests cover, for example, a file foo.py will have tests in a file test_foo.py . Tests for massgov/pfml/api/applications.py live at tests/api/test_applications.py Test cases should begin with the test_ prefix, followed by the function it's testing and some description of what about the function it is testing. In tests/api/test_users.py , the test_users_patch_404 function is a test (because it begins with test_ ), that covers the users_patch function's behavior around 404 responses. Tests can be grouped in classes starting with Test , methods that start with test_ will be picked up as tests, for example TestFeature::test_scenario . There are occasions where tests may not line up exactly with a single source file, function, or otherwise may need to deviate from this exact structure, but this is the setup in general.","title":"Naming"},{"location":"api/writing-tests/#conftest-files","text":"conftest.py files are automatically loaded by pytest, making their contents available to tests without needing to be imported. They are an easy place to put shared test fixtures as well as define other pytest configuration (define hooks, load plugins, define new/override assert behavior, etc.). They should never be imported directly. The main tests/conftest.py holds widely useful fixtures included for all tests. Scoped conftest.py files can be created that apply only to the tests below them in the directory hierarchy, for example, the tests/db/conftest.py file is only loaded for tests under tests/db/ . More info: https://docs.pytest.org/en/latest/fixture.html#conftest-py-sharing-fixture-functions","title":"conftest files"},{"location":"api/writing-tests/#helpers","text":"If there is useful functionality that needs to be shared between tests, but is only applicable to testing and is not a fixture, create modules under tests/helpers/ . They can be imported into tests from the path tests.helpers , for example, from tests.helpers.foo import helper_func .","title":"Helpers"},{"location":"api/writing-tests/#using-factories","text":"To facilitate easier setup of test data, most database models have factories via factory_boy in massgov/pfml/db/models/factories.py . There are a few different ways of using the factories , termed \"strategies\": build, create, and stub. Most notably for this project: The build strategy via FooFactory.build() populates a model class with the generated data, but does not attempt to write it to the database The create strategy via FooFactory.create() writes a generated model to the database (can think of it like FooFactory.build() then db_session.add() and db_session.commit() ) The build strategy is useful if the code under test just needs the data on the model and doesn't actually perform any database interactions. In order to use the create strategy, pull in the initialize_factories_session fixture. Regardless of the strategy, can override the values for attributes on the generated models by passing them into the factory call, for example: FooFactory . build ( foo_id = 5 , name = \"Bar\" ) would set foo_id=5 and name=\"Bar\" on the generated model, while all other attributes would use what's configured on the factory class. For creating a collection of generated models, there are batch methods, build_batch() and create_batch() , which will create multiple instances. For example: FooFactory . build_batch ( size = 5 ) will return 5 Foo instances with different data. Attributes set in the batch call will be shared among all the instances, so: FooFactory . build_batch ( size = 2 , parent_widget = widget ) would create 2 Foo instances with widget set as their parent_widget .","title":"Using Factories"},{"location":"api/writing-tests/#integration-test-marker","text":"An integration marker is configured in pytest for the project. Any test that requires a real database connection or any concrete \"resource\" outside of the code itself should be tagged with the integration marker. It indicates an \"integration\" test, as opposed to a \"unit\" test, in a somewhat loose sense. A few common situations are easy cases, if a test is covering API behavior via fixtures like app or client or testing state in the database with test_db_session , the test should be marked with integration . Accessing real files is a bit of a gray area. If testing code that needs file-like objects, should generally prefer using in-memory constructs like StringIO or BytesIO to avoid ever touching the filesystem. But currently if a test needs to load test fixture files or use tmp_path to work with a real file for some purpose, those do not need to be tagged integration . Decorate any individual test with @pytest.mark.integration . If all (or almost all) tests in a given test file are integration tests, they can be tagged all at once with a declaration like the following at the top of the file (after imports): # every test in here requires real resources pytestmark = pytest . mark . integration If a test file has a large mix of integration and unit tests that don't make sense to separate, integration tests can be bundled into a test class which can then be tagged, for example: @pytest . mark . integration class TestIntegrations : (but tagging each individual function with @pytest.mark.integration is also acceptable)","title":"Integration test marker"},{"location":"portal/browser-support/","text":"Browser support Portal aims to support all modern browsers (Edge, Firefox, Chrome, Safari, Opera, et al). Portal also aims to support Internet Explorer 11. Portal does not support Internet Explorer version 10 or below. Read more about this in Confluence . Read more about Next.js browser support here . Things to be aware of A browserslist file informs our CSS build process which browsers we support. Learn more . Next.js automatically injects polyfills required for IE11 compatibility of our source code, however can't do this for any NPM dependencies that lack IE 11 compatibility. As a result, we have some manual polyfills in src/polyfills.js For all users of Internet Explorer, we display a banner at the top of the site to communicate their browser is not fully supported. See UnsupportedBrowserBanner.js Manual testing in Internet Explorer To test the Portal locally using Internet Explorer 11 in BrowserStack or Sauce Labs, you will need to temporarily disable Amplify's cookie storage config setting in _app.js by commenting out the cookieStorage object. To test in IE 10 or below, you will need to build and export the static site: $ npm build $ npm start These requirements are unique to local development.","title":"Browser support"},{"location":"portal/browser-support/#browser-support","text":"Portal aims to support all modern browsers (Edge, Firefox, Chrome, Safari, Opera, et al). Portal also aims to support Internet Explorer 11. Portal does not support Internet Explorer version 10 or below. Read more about this in Confluence . Read more about Next.js browser support here .","title":"Browser support"},{"location":"portal/browser-support/#things-to-be-aware-of","text":"A browserslist file informs our CSS build process which browsers we support. Learn more . Next.js automatically injects polyfills required for IE11 compatibility of our source code, however can't do this for any NPM dependencies that lack IE 11 compatibility. As a result, we have some manual polyfills in src/polyfills.js For all users of Internet Explorer, we display a banner at the top of the site to communicate their browser is not fully supported. See UnsupportedBrowserBanner.js","title":"Things to be aware of"},{"location":"portal/browser-support/#manual-testing-in-internet-explorer","text":"To test the Portal locally using Internet Explorer 11 in BrowserStack or Sauce Labs, you will need to temporarily disable Amplify's cookie storage config setting in _app.js by commenting out the cookieStorage object. To test in IE 10 or below, you will need to build and export the static site: $ npm build $ npm start These requirements are unique to local development.","title":"Manual testing in Internet Explorer"},{"location":"portal/creating-environments/","text":"Setting up a new environment This docs covers the steps you need to take to configure the Portal web app to support a new environment. Before reading this, make sure you've already ran through the steps to setup the infrastructure for a new environment . Add environment variables Create a new config file for the environment in portal/config , copying the properties from another environment. Update the variables for the environment. Primarily, you'll want to update the environment name and the Cognito IDs, which should have been output after you ran terraform apply . Also look at Monitoring and Web Analytics READMEs for configuring New Relic and Google Tag Manager. Add an entry for the new environment to portal/config/index.js , copying the properties from another environment. Make sure the associated API environment's origin URL is listed in the list of allowed_origins in new-relic.js . Add a build script Each environment should have its own build script in portal/package.json so that the site can be built using the correct environment variables. Refer to other build scripts, like build:test for an example. Update GitHub Actions Each environment will have its own GitHub branch that will deploy when changes are pushed. Add your branch and environment configs to portal-deploy.yml and portal-infra-deploy.yml by adding a branch to the workflow trigger: on: push: branches: - main - deploy/<<environment_name>> and environment variables: env: # ... # map branch name to environment name refs/heads/main: test refs/heads/deploy/<<environment_name>>: <<environment_name>> Also update the deploy doc with details about deploying to the new environment.","title":"Setting up a new environment"},{"location":"portal/creating-environments/#setting-up-a-new-environment","text":"This docs covers the steps you need to take to configure the Portal web app to support a new environment. Before reading this, make sure you've already ran through the steps to setup the infrastructure for a new environment .","title":"Setting up a new environment"},{"location":"portal/creating-environments/#add-environment-variables","text":"Create a new config file for the environment in portal/config , copying the properties from another environment. Update the variables for the environment. Primarily, you'll want to update the environment name and the Cognito IDs, which should have been output after you ran terraform apply . Also look at Monitoring and Web Analytics READMEs for configuring New Relic and Google Tag Manager. Add an entry for the new environment to portal/config/index.js , copying the properties from another environment. Make sure the associated API environment's origin URL is listed in the list of allowed_origins in new-relic.js .","title":"Add environment variables"},{"location":"portal/creating-environments/#add-a-build-script","text":"Each environment should have its own build script in portal/package.json so that the site can be built using the correct environment variables. Refer to other build scripts, like build:test for an example.","title":"Add a build script"},{"location":"portal/creating-environments/#update-github-actions","text":"Each environment will have its own GitHub branch that will deploy when changes are pushed. Add your branch and environment configs to portal-deploy.yml and portal-infra-deploy.yml by adding a branch to the workflow trigger: on: push: branches: - main - deploy/<<environment_name>> and environment variables: env: # ... # map branch name to environment name refs/heads/main: test refs/heads/deploy/<<environment_name>>: <<environment_name>> Also update the deploy doc with details about deploying to the new environment.","title":"Update GitHub Actions"},{"location":"portal/development/","text":"Portal Development This page covers development practices for working on the Mass PFML Portal. Please document liberally so that others may benefit from your learning. TODO comments Our linter enforces that TODO comments include a reference to a Jira ticket that tracks the work to implement the TODO. The expected format of a TODO is: TODO (CP-123): Message ...where CP-123 is the Jira ticket number. Why? We want to avoid losing track of work that we haven't completed, and unintentionally ship an application that doesn't work end-to-end. Making sure that we have tickets in our backlog is one major way we can avoid losing track of this work. In addition, having ticket references within TODO comments provides an engineer a way to learn additional context, and potentially learn that the work has already been completed, but the TODO got left behind in the code by accident. Creating a page All files in the portal/src/pages directory are automatically available as routes based on their name, e.g. about.js is routed to /about . Files named index.js are routed to the root of the directory. See more at the Next.js docs on routing and pages . For Employer-specific pages, files will be nested in an /employers subdirectory. Each time you add a new page, add a new route to src/routes.js . Add content strings for the page to src/locales/app/en-US.js . Add a test file for the page (and for any new components) to tests Question Pages We use a state machine to control routing between question pages in a flow. The library we use behind the scenes for this is XState . Add a new state node for the route to src/flows/claimaint.js , and add a CONTINUE transition. Read more on XState configs here . Within a meta object on the state node, add a step and fields properties. The fields value should contain the field paths for every field that may be displayed on the question page. This is important because this array is what's used for determining what validation errors should display on the question page, and also is used for identifying when a step on the checklist is In Progress or Completed. If the page should display validation issues for specific rules, add an array of applicableRules to the state's meta object. If routing to or from the page is conditional, you'll need to define guards that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/claimant.test.js If routing is conditional, add items with appropriate data to the testData array. Auth Pages Routing Similar to Question Pages, pages used in the Auth flow are also in the /portal/src/pages subdirectory, but routing is controlled using the state machine. Add a new state for the route to src/flows/auth.js . Include a CONTINUE transition. If routing to or from the page is conditional, you'll need to define guard s that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/auth.test.js If routing is conditional, add items with appropriate data to the testData array. next.config.js The next.config.js file is a Node.js module that can be used to configure build and export behavior, such as Webpack settings.","title":"Portal Development"},{"location":"portal/development/#portal-development","text":"This page covers development practices for working on the Mass PFML Portal. Please document liberally so that others may benefit from your learning.","title":"Portal Development"},{"location":"portal/development/#todo-comments","text":"Our linter enforces that TODO comments include a reference to a Jira ticket that tracks the work to implement the TODO. The expected format of a TODO is: TODO (CP-123): Message ...where CP-123 is the Jira ticket number. Why? We want to avoid losing track of work that we haven't completed, and unintentionally ship an application that doesn't work end-to-end. Making sure that we have tickets in our backlog is one major way we can avoid losing track of this work. In addition, having ticket references within TODO comments provides an engineer a way to learn additional context, and potentially learn that the work has already been completed, but the TODO got left behind in the code by accident.","title":"TODO comments"},{"location":"portal/development/#creating-a-page","text":"All files in the portal/src/pages directory are automatically available as routes based on their name, e.g. about.js is routed to /about . Files named index.js are routed to the root of the directory. See more at the Next.js docs on routing and pages . For Employer-specific pages, files will be nested in an /employers subdirectory. Each time you add a new page, add a new route to src/routes.js . Add content strings for the page to src/locales/app/en-US.js . Add a test file for the page (and for any new components) to tests","title":"Creating a page"},{"location":"portal/development/#question-pages","text":"We use a state machine to control routing between question pages in a flow. The library we use behind the scenes for this is XState . Add a new state node for the route to src/flows/claimaint.js , and add a CONTINUE transition. Read more on XState configs here . Within a meta object on the state node, add a step and fields properties. The fields value should contain the field paths for every field that may be displayed on the question page. This is important because this array is what's used for determining what validation errors should display on the question page, and also is used for identifying when a step on the checklist is In Progress or Completed. If the page should display validation issues for specific rules, add an array of applicableRules to the state's meta object. If routing to or from the page is conditional, you'll need to define guards that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/claimant.test.js If routing is conditional, add items with appropriate data to the testData array.","title":"Question Pages"},{"location":"portal/development/#auth-pages-routing","text":"Similar to Question Pages, pages used in the Auth flow are also in the /portal/src/pages subdirectory, but routing is controlled using the state machine. Add a new state for the route to src/flows/auth.js . Include a CONTINUE transition. If routing to or from the page is conditional, you'll need to define guard s that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/auth.test.js If routing is conditional, add items with appropriate data to the testData array.","title":"Auth Pages Routing"},{"location":"portal/development/#nextconfigjs","text":"The next.config.js file is a Node.js module that can be used to configure build and export behavior, such as Webpack settings.","title":"next.config.js"},{"location":"portal/environment-variables/","text":"Environment variables Environment variables include feature flags , maintenance pages , and URLs and keys for external resources such as Cognito and the API. Configuring environment variables Default environment variables live in portal/config/default.js . Each environment has a corresponding configuration file in portal/config , for example production.js The default environment variables are merged with the environment-specific config file in portal/config/index.js . A default environment variable can be overridden in the environment-specific config file. Portal environment variables should never include a secret! Since the Portal is only served on the client-side, these environment variables will be publicly accessible. Each time you add a new environment variable, ensure that you add it to each environment's config file, so that an environment isn't missing anything. If the variable value is shared across many environments, consider adding it as a default environment variable in portal/config/default.js . Referencing an environment variable Within our codebase, environment variables are referenced from process.env . For example: Amplify . config ( process . env . myCustomKey ); How it works We use environment specific NPM scripts in portal/package.json to bundle builds with the correct configuration. For example build:stage . The target environment is set as the BUILD_ENV . For example: BUILD_ENV=stage npm run build When the build script is ran, the contents of the configuration file corresponding to BUILD_ENV are assigned to the Next.js env config option in portal/next.config.js . Next.js replaces process.env references with their values at build time. NODE_ENV The NODE_ENV environment variable is automatically set by Next.js during development and builds. For our test scripts, we manually set this in the test's NPM scripts. This variable determines whether our JS bundle includes the production build of React or the dev build . When our NPM scripts call next dev , NODE_ENV is automatically set to development and our JS bundle includes the React development build. When our NPM scripts call next build , NODE_ENV is automatically set to production and our JS bundle includes the optimized React production build. The NODE_ENV variable is also exposed to our code for use, allowing us to conditionally enable behavior for an environment, like only logging warnings when in development . Related Portal Configuration Management","title":"Environment variables"},{"location":"portal/environment-variables/#environment-variables","text":"Environment variables include feature flags , maintenance pages , and URLs and keys for external resources such as Cognito and the API.","title":"Environment variables"},{"location":"portal/environment-variables/#configuring-environment-variables","text":"Default environment variables live in portal/config/default.js . Each environment has a corresponding configuration file in portal/config , for example production.js The default environment variables are merged with the environment-specific config file in portal/config/index.js . A default environment variable can be overridden in the environment-specific config file. Portal environment variables should never include a secret! Since the Portal is only served on the client-side, these environment variables will be publicly accessible. Each time you add a new environment variable, ensure that you add it to each environment's config file, so that an environment isn't missing anything. If the variable value is shared across many environments, consider adding it as a default environment variable in portal/config/default.js .","title":"Configuring environment variables"},{"location":"portal/environment-variables/#referencing-an-environment-variable","text":"Within our codebase, environment variables are referenced from process.env . For example: Amplify . config ( process . env . myCustomKey );","title":"Referencing an environment variable"},{"location":"portal/environment-variables/#how-it-works","text":"We use environment specific NPM scripts in portal/package.json to bundle builds with the correct configuration. For example build:stage . The target environment is set as the BUILD_ENV . For example: BUILD_ENV=stage npm run build When the build script is ran, the contents of the configuration file corresponding to BUILD_ENV are assigned to the Next.js env config option in portal/next.config.js . Next.js replaces process.env references with their values at build time.","title":"How it works"},{"location":"portal/environment-variables/#node_env","text":"The NODE_ENV environment variable is automatically set by Next.js during development and builds. For our test scripts, we manually set this in the test's NPM scripts. This variable determines whether our JS bundle includes the production build of React or the dev build . When our NPM scripts call next dev , NODE_ENV is automatically set to development and our JS bundle includes the React development build. When our NPM scripts call next build , NODE_ENV is automatically set to production and our JS bundle includes the optimized React production build. The NODE_ENV variable is also exposed to our code for use, allowing us to conditionally enable behavior for an environment, like only logging warnings when in development .","title":"NODE_ENV"},{"location":"portal/environment-variables/#related","text":"Portal Configuration Management","title":"Related"},{"location":"portal/error-handling/","text":"Error handling All API requests flow through a \"logic hook\", which sends the request through the BaseAPI module. The BaseAPI module is then responsible for processing the response. Below is a visual representing the flow of a request and how an API error response flows back through the app and is rendered to the user: BaseAPI The normal API error response includes an errors property in its body representing the request's issues. When errors are present, the BaseAPI throws a ValidationError that holds all API errors that were in the response body. The BaseAPI may also throw other error types, such as NotFoundError or ForbiddenError , depending on the response's status code. Logic hook The logic hooks, like useBenefitsApplicationsLogic , are responsible for catching any errors thrown by the API module and sending the error into appErrorsLogic . A typical pattern for this looks like: async updateClaim ( patchData ) { try { // Send the API request await myApi . update ( patchData ) } catch ( error ) { // Handle any API errors appErrorsLogic . catchError ( error ) } } App Errors Logic hook The useAppErrorsLogic hook is where errors get sent to for processing and storing. When appErrorsLogic.catchError receives a ValidationError holding the API response's errors, it parses each API error and creates an AppErrorInfo instance with a message generated from the API issue's type , rule , and field properties. See the getMessageFromApiIssue method for how i18n keys are generated. If you're unsure what i18n key will be generated for the issue, you can trigger the error in local development and view the console to see what key it says is missing: If a matching i18n key isn't found for the API issue, the app falls back to the original error message sent from the API. We should always have an internationalized error message, and this fallback behavior should not be relied upon. <ErrorsSummary> The ErrorsSummary component is rendered by our app container ( _app.js ) above the page component. When the appErrorsLogic module has errors present, the ErrorsSummary component renders each error's message . Inline errors Each page has access to appErrorsLogic via the appLogic prop. The errors can be read from appLogic.appErrors . If the errors are associated with a specific field on the page, you can render them inline by setting the field component's errorMsg prop. The easiest way to do this is by using the useFunctionalInputProps to set the common props for your fields, one of which is errorMsg . A common pattern for forms in our app looks like this: const { formState, updateFields } = useFormState({ first_name: claim.first_name, }); const getFunctionalInputProps = useFunctionalInputProps({ appErrors: props.appLogic.appErrors, formState, updateFields, }); return ( <form> <InputText {...getFunctionalInputProps(\"first_name\")} label={t(\"pages.claimsName.firstNameLabel\")} /> </form> ); Alternatively, if you're not using useFunctionalInputProps , you can use appErrors.fieldErrorMessage(fieldName) to return the error message for a specific field: <InputText errorMsg={props.appErrors.fieldErrorMessage(\"first_name\")} name=\"first_name\" ...","title":"Error handling"},{"location":"portal/error-handling/#error-handling","text":"All API requests flow through a \"logic hook\", which sends the request through the BaseAPI module. The BaseAPI module is then responsible for processing the response. Below is a visual representing the flow of a request and how an API error response flows back through the app and is rendered to the user:","title":"Error handling"},{"location":"portal/error-handling/#baseapi","text":"The normal API error response includes an errors property in its body representing the request's issues. When errors are present, the BaseAPI throws a ValidationError that holds all API errors that were in the response body. The BaseAPI may also throw other error types, such as NotFoundError or ForbiddenError , depending on the response's status code.","title":"BaseAPI"},{"location":"portal/error-handling/#logic-hook","text":"The logic hooks, like useBenefitsApplicationsLogic , are responsible for catching any errors thrown by the API module and sending the error into appErrorsLogic . A typical pattern for this looks like: async updateClaim ( patchData ) { try { // Send the API request await myApi . update ( patchData ) } catch ( error ) { // Handle any API errors appErrorsLogic . catchError ( error ) } }","title":"Logic hook"},{"location":"portal/error-handling/#app-errors-logic-hook","text":"The useAppErrorsLogic hook is where errors get sent to for processing and storing. When appErrorsLogic.catchError receives a ValidationError holding the API response's errors, it parses each API error and creates an AppErrorInfo instance with a message generated from the API issue's type , rule , and field properties. See the getMessageFromApiIssue method for how i18n keys are generated. If you're unsure what i18n key will be generated for the issue, you can trigger the error in local development and view the console to see what key it says is missing: If a matching i18n key isn't found for the API issue, the app falls back to the original error message sent from the API. We should always have an internationalized error message, and this fallback behavior should not be relied upon.","title":"App Errors Logic hook"},{"location":"portal/error-handling/#errorssummary","text":"The ErrorsSummary component is rendered by our app container ( _app.js ) above the page component. When the appErrorsLogic module has errors present, the ErrorsSummary component renders each error's message .","title":"&lt;ErrorsSummary&gt;"},{"location":"portal/error-handling/#inline-errors","text":"Each page has access to appErrorsLogic via the appLogic prop. The errors can be read from appLogic.appErrors . If the errors are associated with a specific field on the page, you can render them inline by setting the field component's errorMsg prop. The easiest way to do this is by using the useFunctionalInputProps to set the common props for your fields, one of which is errorMsg . A common pattern for forms in our app looks like this: const { formState, updateFields } = useFormState({ first_name: claim.first_name, }); const getFunctionalInputProps = useFunctionalInputProps({ appErrors: props.appLogic.appErrors, formState, updateFields, }); return ( <form> <InputText {...getFunctionalInputProps(\"first_name\")} label={t(\"pages.claimsName.firstNameLabel\")} /> </form> ); Alternatively, if you're not using useFunctionalInputProps , you can use appErrors.fieldErrorMessage(fieldName) to return the error message for a specific field: <InputText errorMsg={props.appErrors.fieldErrorMessage(\"first_name\")} name=\"first_name\" ...","title":"Inline errors"},{"location":"portal/feature-flags/","text":"Feature flags Original tech spec for feature flags is available on Confluence Defining feature flags Feature flags are defined in portal/config/featureFlags.js . These flags are then set as environment variables in portal/next.config.js at initial build time (so they don't live reload when running locally). If you define a new feature flag during local development, you will need to restart the dev server in order for the flag to become available . Feature flags should be named so that the absence of a default value is interpreted as false \u2013 this way if someone forgets to define a feature flag, it doesn\u2019t unintentionally enable the feature for everyone. Checking feature flags in the code The services/featureFlags.js file includes methods for checking the value of a feature flag, as well as a method for overriding environment-level feature flags through the URL's query string. The isFeatureEnabled method can be used to check if a feature flag is enabled: if ( isFeatureEnabled ( \"showExample\" )) { return \"This is an example\" ; } Overriding a feature flag in the browser Cookies are used for overriding an environment's default feature flag. To do so, add a query param of _ff to the site's URL and use JSON notation for its value. A feature flag's value can be true , false , or reset . Setting a flag to reset will restore it to the environment's default. For example: To enable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:true To disable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:false To reset a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:reset You can also manage multiple flags by separating their key/value pairs with a ; . For example: {Site URL}?_ff=unrestrictedClaimFlow:true;anotherFlag:true Preventing / Allowing the site to be rendered While the site is under development, we don't want it to be visible to the press or the public . As a low-tech solution, we use a feature flag prefixed with pfml to determine whether the site should be rendered. To render the site, enable the pfmlTerriyay flag by visiting: {Site URL}?_ff=pfmlTerriyay:true \u26a0\ufe0f The exact flag name may change if we need to force the site to be hidden for people who previously enabled the flag through through browser. If the above flag doesn't work, you should check config/featureFlags.js to verify whether it's still the correct flag name we're using.","title":"Feature flags"},{"location":"portal/feature-flags/#feature-flags","text":"Original tech spec for feature flags is available on Confluence","title":"Feature flags"},{"location":"portal/feature-flags/#defining-feature-flags","text":"Feature flags are defined in portal/config/featureFlags.js . These flags are then set as environment variables in portal/next.config.js at initial build time (so they don't live reload when running locally). If you define a new feature flag during local development, you will need to restart the dev server in order for the flag to become available . Feature flags should be named so that the absence of a default value is interpreted as false \u2013 this way if someone forgets to define a feature flag, it doesn\u2019t unintentionally enable the feature for everyone.","title":"Defining feature flags"},{"location":"portal/feature-flags/#checking-feature-flags-in-the-code","text":"The services/featureFlags.js file includes methods for checking the value of a feature flag, as well as a method for overriding environment-level feature flags through the URL's query string. The isFeatureEnabled method can be used to check if a feature flag is enabled: if ( isFeatureEnabled ( \"showExample\" )) { return \"This is an example\" ; }","title":"Checking feature flags in the code"},{"location":"portal/feature-flags/#overriding-a-feature-flag-in-the-browser","text":"Cookies are used for overriding an environment's default feature flag. To do so, add a query param of _ff to the site's URL and use JSON notation for its value. A feature flag's value can be true , false , or reset . Setting a flag to reset will restore it to the environment's default. For example: To enable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:true To disable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:false To reset a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:reset You can also manage multiple flags by separating their key/value pairs with a ; . For example: {Site URL}?_ff=unrestrictedClaimFlow:true;anotherFlag:true","title":"Overriding a feature flag in the browser"},{"location":"portal/feature-flags/#preventing-allowing-the-site-to-be-rendered","text":"While the site is under development, we don't want it to be visible to the press or the public . As a low-tech solution, we use a feature flag prefixed with pfml to determine whether the site should be rendered. To render the site, enable the pfmlTerriyay flag by visiting: {Site URL}?_ff=pfmlTerriyay:true \u26a0\ufe0f The exact flag name may change if we need to force the site to be hidden for people who previously enabled the flag through through browser. If the above flag doesn't work, you should check config/featureFlags.js to verify whether it's still the correct flag name we're using.","title":"Preventing / Allowing the site to be rendered"},{"location":"portal/internationalization/","text":"Internationalization All Portal internationalization (i18n) is configured in the portal/src/locales/i18n.js module. The locale selection for both i18n systems is configured in i18n.js to allow locale synchronization across the app. Application This app uses i18next and its React library for application localization. You can find i18next translation resources in portal/locales/app/ . Each language has a file with a set of keys and values for every content string we need to localize for our site. The keys are how we will refer to these strings throughout our codebase, rather than hard coding the content strings. i18next patterns Our application takes advantage of some advanced patterns supported by the i18next library. It can be useful to be aware of these while working in the codebase: Context allows us to have variations of a string based on the value of a context property: t ( \"haveIncome\" , { context : \"married\" }); // -> \"Do you or your spouse have income sources?\" { \"haveIncome\" : \"Do you have income sources?\" , \"haveIncome_married\" : \"Do you or your spouse have income sources?\" } The Trans component allows us to integrate html tags such as links ( <a> tags) and text formatting tags (e.g. <strong> or <em> ) into translated text: <Trans i18nKey=\"userAgreement\" components={{ \"consent-link\": <a href=\"https://www.mass.gov/paidleave-informedconsent\" />, \"privacy-policy-link\": <a href=\"https://www.mass.gov/privacypolicy\" />, }} /> { \"userAgreement\" : \"To find out more about how the Commonwealth might use the information you share with DFML, please read the <consent-link>DFML Informed Consent Agreement</consent-link> and the <privacy-policy-link>Privacy Policy for Mass.gov</privacy-policy-link>.\" , } Note that we are using the alternative usage of Trans introduced in v11.6.0 where components are passed in as props rather than as children of Trans . This method allows the use of named tags in locale strings rather than needing to refer to child components by their index. Formatters are functions that define locale-specific formats for specially-formatted values such as currencies or time durations. t ( \"timeDuration\" , { minutes : 480 }); // -> \"8h\" t ( \"timeDuration\" , { minutes : 475 }); // -> \"7h 55m\" function formatValue ( value , format , locale ) { if ( format === \"hoursMinutesDuration\" ) { // Could also internationalize by using the locale value const { hours , minutes } = convertMinutesToHours ( value ); if ( minutes === 0 ) return ` ${ hours } h` ; return ` ${ hours } h ${ minutes } m` ; } return value ; { t imeDura t io n : \"{{minutes, hoursMinutesDuration}}\" , } Conventions Internationalization content can get messy and lead to hard-to-find bugs during translation. As such we strictly follow the below conventions to preserve readability, maintainability, and avoid errors. Organization Keys are organized under top-level objects by how they're used: components defines content used in specific components pages defines content used in specific pages errors defines content used in error messages, which aren\u2019t page or component specific shared defines content shared between multiple components or pages chars defines special characters (such as non-breaking spaces, which are difficult to distinguish in values) Keys are limited to three levels deep, for example pages.claimsDateOfBirth.title . This makes the structure easier to navigate and the process of finding a specific element more consistent. Naming Prioritize readability, and then brevity. Try to be consistent with existing keys. For example the title content for each page should be under a key, pages.<page-identifier>.title . When a page is related to a larger series of pages you can indicate that with a prefix. For example, the name form page within the claims flow is identified as pages.claimsName . Avoid repeating context in the key. For example, prefer pages.claimsName.sectionHint over pages.claimsName.nameSectionHint . Try to name keys after the purpose of the key, not just an underscored version of your translation. This may result in duplication of translations (i.e. multiple keys for \"First name\"), but this is much more flexible for cases down the line when one of those translations needs to change (i.e \"First name\" changes to \"Spouse's first name\"). Keys should be alphabetical so they're easier for others to find and update. We also considered sorting them by page-order where they occur but alphabetical is more easily enforced (with linting) and doesn\u2019t require re-ordering even if content on a page is re-ordered. Keys must be unique - there can't be two keys with the same name. This only applies to the entire key, for example having pages.claimsName.sectionHint and pages.claimsSsn.sectionHint is fine. Common page element terminology The PFML pages follow a design system that uses common terms for various page elements. It's helpful to use these terms when defining content strings for both the developer experience (when implementing a page design this gives you tips on how to name content strings) and in tracing content from the page back to the i18n key. These terms may change over time so this will need to be updated when they do. Some common element terms include: title - one per page sectionLabel - one per section or fieldset. This is typically either an HTML legend or label, depending on the page/section lead - additional context about an entire page, section, or fieldset legend - context about an embedded fieldset (note that sectionLabel content is always called sectionLabel even if we render it with an HTML legend element) label - typically one per input hint - additional context about a specific input Note: we use snake_case for input field names to match the names used by the API, but we don\u2019t carry that over to i18n keys. For example the state_id field on the claims license page has a label called pages.claimsLicense.stateIdLabel . For visual examples of different text elements on a page see the design team\u2019s page template designs. Sharing content All shared keys are located inside the shared object; this makes it obvious that when you're changing one of them your changes will impact multiple components/pages. This is meant to prevent accidental content changes if someone is only trying to update content in one place. Sharing content should follow this pattern: The shared content is defined with a key inside the shared object. Each page or component that uses the shared content should define its own key and reference the shared key. Shared keys, such as shared.claimsPages.leaveTypeTitle , shouldn't be referenced directly in application code. Example shared: { claimsPages: // Define the shared key here -- anyone changing this will know it affects multiple pages takingLeaveTitle: \"Who is taking leave?\", }, } pages: { claimsName: { // Reference the shared content with a key where it's used. This key, `pages.claimsName.title`, // will be the key used within the page. This makes it easy to stop using the shared content // by changing this to a content string. title: \"$t(shared.claimsPages.takingLeaveTitle)\", }, } Using markup in values Sometimes content necessitates the use of specific HTML tags. However, markup should only include tag names and not tag attributes. A tag's className , href , and other attributes should be set by the page or component that renders the key (see usage example below). Using markup in content strings can be useful but it can also make them more difficult to read and edit without introducing errors. Markup should be used sparingly. Some of the reasons for using markup in content strings include answering yes to any of these questions: Does this content require specific embedded tags for formatting? (e.g. <a> , <em> , <strong> , <ul> , <ol> , or multiple <p> tags) Does this content serve one primary function for the user? Is it helpful to edit this content as one key? (e.g. two paragraphs that serve one purpose for the user, or list items with an explicit order) Would breaking this content into multiple keys make them more difficult to name semantically or add unnecessary structure? Rendering values in pages and components Using the useTranslation hook With simple values, you only need to get the t function to render values in the functional component: import React from \"react\" ; import { useTranslation } from \"react-i18next\" ; export function MyComponent () { const { t } = useTranslation (); return < p > { t ( \"translationResourceKey\" )} < /p>; } The Trans component Since the t function can't be used for values that include markup, the Trans component is used to interpolate or translate complex react elements. Set the key in the language file, excluding all tag attributes: htmlKey: \"<ul><li>List item one</li><li>List item two has <my-link>a link</my-link></li></ul>\", Anchors are given a unique tag name in the language file, and the corresponding href value is defined in routes.js : myLink : \"http://www.example.com\" , The Trans component renders the basic tags, br , strong , i , and p . However, other tags must be explicitly defined. The components object is where attributes such as className , href can be set: import React from \"react\" ; import { Trans } from \"react-i18next\" ; < Trans i18nKey = \"htmlKey\" components = {{ \"my-link\" : ( < a target = \"_blank\" rel = \"noopener\" href = { routes . external . myLink } /> ), ul : < ul className = \"usa-list\" /> , li : < li /> , }} />","title":"Internationalization"},{"location":"portal/internationalization/#internationalization","text":"All Portal internationalization (i18n) is configured in the portal/src/locales/i18n.js module. The locale selection for both i18n systems is configured in i18n.js to allow locale synchronization across the app.","title":"Internationalization"},{"location":"portal/internationalization/#application","text":"This app uses i18next and its React library for application localization. You can find i18next translation resources in portal/locales/app/ . Each language has a file with a set of keys and values for every content string we need to localize for our site. The keys are how we will refer to these strings throughout our codebase, rather than hard coding the content strings.","title":"Application"},{"location":"portal/internationalization/#i18next-patterns","text":"Our application takes advantage of some advanced patterns supported by the i18next library. It can be useful to be aware of these while working in the codebase: Context allows us to have variations of a string based on the value of a context property: t ( \"haveIncome\" , { context : \"married\" }); // -> \"Do you or your spouse have income sources?\" { \"haveIncome\" : \"Do you have income sources?\" , \"haveIncome_married\" : \"Do you or your spouse have income sources?\" } The Trans component allows us to integrate html tags such as links ( <a> tags) and text formatting tags (e.g. <strong> or <em> ) into translated text: <Trans i18nKey=\"userAgreement\" components={{ \"consent-link\": <a href=\"https://www.mass.gov/paidleave-informedconsent\" />, \"privacy-policy-link\": <a href=\"https://www.mass.gov/privacypolicy\" />, }} /> { \"userAgreement\" : \"To find out more about how the Commonwealth might use the information you share with DFML, please read the <consent-link>DFML Informed Consent Agreement</consent-link> and the <privacy-policy-link>Privacy Policy for Mass.gov</privacy-policy-link>.\" , } Note that we are using the alternative usage of Trans introduced in v11.6.0 where components are passed in as props rather than as children of Trans . This method allows the use of named tags in locale strings rather than needing to refer to child components by their index. Formatters are functions that define locale-specific formats for specially-formatted values such as currencies or time durations. t ( \"timeDuration\" , { minutes : 480 }); // -> \"8h\" t ( \"timeDuration\" , { minutes : 475 }); // -> \"7h 55m\" function formatValue ( value , format , locale ) { if ( format === \"hoursMinutesDuration\" ) { // Could also internationalize by using the locale value const { hours , minutes } = convertMinutesToHours ( value ); if ( minutes === 0 ) return ` ${ hours } h` ; return ` ${ hours } h ${ minutes } m` ; } return value ; { t imeDura t io n : \"{{minutes, hoursMinutesDuration}}\" , }","title":"i18next patterns"},{"location":"portal/internationalization/#conventions","text":"Internationalization content can get messy and lead to hard-to-find bugs during translation. As such we strictly follow the below conventions to preserve readability, maintainability, and avoid errors.","title":"Conventions"},{"location":"portal/internationalization/#organization","text":"Keys are organized under top-level objects by how they're used: components defines content used in specific components pages defines content used in specific pages errors defines content used in error messages, which aren\u2019t page or component specific shared defines content shared between multiple components or pages chars defines special characters (such as non-breaking spaces, which are difficult to distinguish in values) Keys are limited to three levels deep, for example pages.claimsDateOfBirth.title . This makes the structure easier to navigate and the process of finding a specific element more consistent.","title":"Organization"},{"location":"portal/internationalization/#naming","text":"Prioritize readability, and then brevity. Try to be consistent with existing keys. For example the title content for each page should be under a key, pages.<page-identifier>.title . When a page is related to a larger series of pages you can indicate that with a prefix. For example, the name form page within the claims flow is identified as pages.claimsName . Avoid repeating context in the key. For example, prefer pages.claimsName.sectionHint over pages.claimsName.nameSectionHint . Try to name keys after the purpose of the key, not just an underscored version of your translation. This may result in duplication of translations (i.e. multiple keys for \"First name\"), but this is much more flexible for cases down the line when one of those translations needs to change (i.e \"First name\" changes to \"Spouse's first name\"). Keys should be alphabetical so they're easier for others to find and update. We also considered sorting them by page-order where they occur but alphabetical is more easily enforced (with linting) and doesn\u2019t require re-ordering even if content on a page is re-ordered. Keys must be unique - there can't be two keys with the same name. This only applies to the entire key, for example having pages.claimsName.sectionHint and pages.claimsSsn.sectionHint is fine.","title":"Naming"},{"location":"portal/internationalization/#common-page-element-terminology","text":"The PFML pages follow a design system that uses common terms for various page elements. It's helpful to use these terms when defining content strings for both the developer experience (when implementing a page design this gives you tips on how to name content strings) and in tracing content from the page back to the i18n key. These terms may change over time so this will need to be updated when they do. Some common element terms include: title - one per page sectionLabel - one per section or fieldset. This is typically either an HTML legend or label, depending on the page/section lead - additional context about an entire page, section, or fieldset legend - context about an embedded fieldset (note that sectionLabel content is always called sectionLabel even if we render it with an HTML legend element) label - typically one per input hint - additional context about a specific input Note: we use snake_case for input field names to match the names used by the API, but we don\u2019t carry that over to i18n keys. For example the state_id field on the claims license page has a label called pages.claimsLicense.stateIdLabel . For visual examples of different text elements on a page see the design team\u2019s page template designs.","title":"Common page element terminology"},{"location":"portal/internationalization/#sharing-content","text":"All shared keys are located inside the shared object; this makes it obvious that when you're changing one of them your changes will impact multiple components/pages. This is meant to prevent accidental content changes if someone is only trying to update content in one place. Sharing content should follow this pattern: The shared content is defined with a key inside the shared object. Each page or component that uses the shared content should define its own key and reference the shared key. Shared keys, such as shared.claimsPages.leaveTypeTitle , shouldn't be referenced directly in application code.","title":"Sharing content"},{"location":"portal/internationalization/#example","text":"shared: { claimsPages: // Define the shared key here -- anyone changing this will know it affects multiple pages takingLeaveTitle: \"Who is taking leave?\", }, } pages: { claimsName: { // Reference the shared content with a key where it's used. This key, `pages.claimsName.title`, // will be the key used within the page. This makes it easy to stop using the shared content // by changing this to a content string. title: \"$t(shared.claimsPages.takingLeaveTitle)\", }, }","title":"Example"},{"location":"portal/internationalization/#using-markup-in-values","text":"Sometimes content necessitates the use of specific HTML tags. However, markup should only include tag names and not tag attributes. A tag's className , href , and other attributes should be set by the page or component that renders the key (see usage example below). Using markup in content strings can be useful but it can also make them more difficult to read and edit without introducing errors. Markup should be used sparingly. Some of the reasons for using markup in content strings include answering yes to any of these questions: Does this content require specific embedded tags for formatting? (e.g. <a> , <em> , <strong> , <ul> , <ol> , or multiple <p> tags) Does this content serve one primary function for the user? Is it helpful to edit this content as one key? (e.g. two paragraphs that serve one purpose for the user, or list items with an explicit order) Would breaking this content into multiple keys make them more difficult to name semantically or add unnecessary structure?","title":"Using markup in values"},{"location":"portal/internationalization/#rendering-values-in-pages-and-components","text":"","title":"Rendering values in pages and components"},{"location":"portal/internationalization/#using-the-usetranslation-hook","text":"With simple values, you only need to get the t function to render values in the functional component: import React from \"react\" ; import { useTranslation } from \"react-i18next\" ; export function MyComponent () { const { t } = useTranslation (); return < p > { t ( \"translationResourceKey\" )} < /p>; }","title":"Using the useTranslation hook"},{"location":"portal/internationalization/#the-trans-component","text":"Since the t function can't be used for values that include markup, the Trans component is used to interpolate or translate complex react elements. Set the key in the language file, excluding all tag attributes: htmlKey: \"<ul><li>List item one</li><li>List item two has <my-link>a link</my-link></li></ul>\", Anchors are given a unique tag name in the language file, and the corresponding href value is defined in routes.js : myLink : \"http://www.example.com\" , The Trans component renders the basic tags, br , strong , i , and p . However, other tags must be explicitly defined. The components object is where attributes such as className , href can be set: import React from \"react\" ; import { Trans } from \"react-i18next\" ; < Trans i18nKey = \"htmlKey\" components = {{ \"my-link\" : ( < a target = \"_blank\" rel = \"noopener\" href = { routes . external . myLink } /> ), ul : < ul className = \"usa-list\" /> , li : < li /> , }} />","title":"The Trans component"},{"location":"portal/maintenance-pages/","text":"Maintenance pages The Portal includes the ability to have maintenance pages that we can turn on in case we need to shut down all or part of the website. Maintenance pages are controlled through the maintenancePageRoutes environment variable . Add the maintenancePageRoutes variable to the environment you want to target. It accepts an array of routes (without trailing slashes) that should render a maintenance page. Enable site wide Use * to match any string: \"maintenancePageRoutes\" : [ \"/*\" ] Enable on a group of pages \"maintenancePageRoutes\" : [ \"/applications/*\" ] Enable on specific pages \"maintenancePageRoutes\" : [ \"/create-account\" , \"/employers/create-account\" ] Enable scheduled maintenance page If the maintenance page is being added because of scheduled down time, you can optionally schedule the beginning and end of this maintenance time by setting the maintenanceStart and/or maintenanceEnd environment variable to an ISO 8601 datetime string. Daylight savings time needs taken into account! For Eastern Daylight Time (EDT), use -04:00 , for Eastern Standard Time (EST), use -05:00 . The start and end time are optional. If start time is not set, the start time begins immediately. If end time is not set, an engineer will need to manually turn off the maintenance page. When end time is set, it will be displayed to the user using their timezone and localization preferences. For example, to enable the maintenance page on all routes, starting at March 25 at 3:30am EDT and ending at March 26 at 8pm EDT. \"maintenancePageRoutes\" : [ \"/*\" ], // required \"maintenanceStart\" : \"2021-03-25T03:30:00-04:00\" , \"maintenanceEnd\" : \"2021-03-26T20:00:00-04:00\" , We're using Eastern time zone above since that's what Massachusetts is, but it could technically be whatever timezone. Testing If you are testing the configuration locally, just note that you need to restart the local development server in order for new environment variables to kick in. Bypassing maintenance pages It can be helpful to bypass a maintenance page to test the page itself. To do so, you can enable the noMaintenance feature flag in the browser. For example, to bypass the maintenance pages in the Test environment: https://paidleave-test.mass.gov?_ff=noMaintenance:true","title":"Maintenance pages"},{"location":"portal/maintenance-pages/#maintenance-pages","text":"The Portal includes the ability to have maintenance pages that we can turn on in case we need to shut down all or part of the website. Maintenance pages are controlled through the maintenancePageRoutes environment variable . Add the maintenancePageRoutes variable to the environment you want to target. It accepts an array of routes (without trailing slashes) that should render a maintenance page.","title":"Maintenance pages"},{"location":"portal/maintenance-pages/#enable-site-wide","text":"Use * to match any string: \"maintenancePageRoutes\" : [ \"/*\" ]","title":"Enable site wide"},{"location":"portal/maintenance-pages/#enable-on-a-group-of-pages","text":"\"maintenancePageRoutes\" : [ \"/applications/*\" ]","title":"Enable on a group of pages"},{"location":"portal/maintenance-pages/#enable-on-specific-pages","text":"\"maintenancePageRoutes\" : [ \"/create-account\" , \"/employers/create-account\" ]","title":"Enable on specific pages"},{"location":"portal/maintenance-pages/#enable-scheduled-maintenance-page","text":"If the maintenance page is being added because of scheduled down time, you can optionally schedule the beginning and end of this maintenance time by setting the maintenanceStart and/or maintenanceEnd environment variable to an ISO 8601 datetime string. Daylight savings time needs taken into account! For Eastern Daylight Time (EDT), use -04:00 , for Eastern Standard Time (EST), use -05:00 . The start and end time are optional. If start time is not set, the start time begins immediately. If end time is not set, an engineer will need to manually turn off the maintenance page. When end time is set, it will be displayed to the user using their timezone and localization preferences. For example, to enable the maintenance page on all routes, starting at March 25 at 3:30am EDT and ending at March 26 at 8pm EDT. \"maintenancePageRoutes\" : [ \"/*\" ], // required \"maintenanceStart\" : \"2021-03-25T03:30:00-04:00\" , \"maintenanceEnd\" : \"2021-03-26T20:00:00-04:00\" , We're using Eastern time zone above since that's what Massachusetts is, but it could technically be whatever timezone.","title":"Enable scheduled maintenance page"},{"location":"portal/maintenance-pages/#testing","text":"If you are testing the configuration locally, just note that you need to restart the local development server in order for new environment variables to kick in.","title":"Testing"},{"location":"portal/maintenance-pages/#bypassing-maintenance-pages","text":"It can be helpful to bypass a maintenance page to test the page itself. To do so, you can enable the noMaintenance feature flag in the browser. For example, to bypass the maintenance pages in the Test environment: https://paidleave-test.mass.gov?_ff=noMaintenance:true","title":"Bypassing maintenance pages"},{"location":"portal/monitoring/","text":"Monitoring We use New Relic Browser to monitor errors in our application. This works by including a JS snippet towards the top of our HTML. This snippet, new-relic.js , works out of the box by wrapping low-level browser APIs, and globally exposes an API ( newrelic ) for explicitly tracking interactions and errors that the app catches itself. New Relic Configuration In order for the New Relic snippet to send data to the correct New Relic Application, we need to set a newRelicAppId environment variable with the New Relic Browser Application ID . We then use the environment variable to configure the New Relic snippet by setting the window.NREUM global variable, in services/tracker.js . \ud83d\udea8 If we ever update the New Relic snippet, we should ensure that the configuration portion at the bottom is removed, so that we're not overwriting window.NREUM.loader_config or window.NREUM.info . Content Security Policy In order for the New Relic script to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: Scripts from https://js-agent.newrelic.com/ Scripts from https://bam.nr-data.net/ Correlating Events Across Systems In order to correlate New Relic events and logs between the frontend and backend, we enabled a New Relic feature called Distributed Tracing . This feature is enabled through the JavaScript snippet at the top of new-relic.js . Note that the ability to enable and disable this feature through the Application Settings in the New Relic Browser UI does not actually have any effect, and is therefore not used. In order for Distributed Tracing to work, all of the API environment origins need to be added to the list of allowed_origins at the top of new-relic.js . JS Errors New Relic Browser is used for monitoring JS Errors. JS Errors are reported to New Relic in a variety of ways: When we add a try/catch statement, we should call the newrelic API's noticeError method. For example: try { await doSomething (); } catch ( error ) { tracker . noticeError ( error ); } Our ErrorBoundary component catches any errors that bubble up from its descendant child components The New Relic snippet should automatically catch errors that our React app doesn't catch Errors caught by our ErrorBoundary component are reported to New Relic and also include a componentStack custom attribute, which provides additional information about which component threw the error. New Relic Browser's UI only displays the error's stack trace, so if you want to view the componentStack value, you'll need to use NRQL to query the JavaScriptError : SELECT timestamp , componentStack , stackTrace FROM JavaScriptError WHERE appName = 'PUT THE APP NAME HERE' Environments Each environment requires its own New Relic Browser \"app\" in order for us to differentiate between data coming in from the different environments. Create these through the New Relic site. Each New Relic app has its own applicationId , which will need set as part of the JS snippet included in our HTML. CP-417 is tracking the work to create New Relic apps for each environment. Enabling New Relic Browser If you have a browser extension installed to block trackers, you may need to safelist the New Relic snippet to allow it to make requests. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe payloads occasionally being sent to bam.nr-data.net Related Error monitoring research","title":"Monitoring"},{"location":"portal/monitoring/#monitoring","text":"We use New Relic Browser to monitor errors in our application. This works by including a JS snippet towards the top of our HTML. This snippet, new-relic.js , works out of the box by wrapping low-level browser APIs, and globally exposes an API ( newrelic ) for explicitly tracking interactions and errors that the app catches itself.","title":"Monitoring"},{"location":"portal/monitoring/#new-relic","text":"","title":"New Relic"},{"location":"portal/monitoring/#configuration","text":"In order for the New Relic snippet to send data to the correct New Relic Application, we need to set a newRelicAppId environment variable with the New Relic Browser Application ID . We then use the environment variable to configure the New Relic snippet by setting the window.NREUM global variable, in services/tracker.js . \ud83d\udea8 If we ever update the New Relic snippet, we should ensure that the configuration portion at the bottom is removed, so that we're not overwriting window.NREUM.loader_config or window.NREUM.info .","title":"Configuration"},{"location":"portal/monitoring/#content-security-policy","text":"In order for the New Relic script to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: Scripts from https://js-agent.newrelic.com/ Scripts from https://bam.nr-data.net/","title":"Content Security Policy"},{"location":"portal/monitoring/#correlating-events-across-systems","text":"In order to correlate New Relic events and logs between the frontend and backend, we enabled a New Relic feature called Distributed Tracing . This feature is enabled through the JavaScript snippet at the top of new-relic.js . Note that the ability to enable and disable this feature through the Application Settings in the New Relic Browser UI does not actually have any effect, and is therefore not used. In order for Distributed Tracing to work, all of the API environment origins need to be added to the list of allowed_origins at the top of new-relic.js .","title":"Correlating Events Across Systems"},{"location":"portal/monitoring/#js-errors","text":"New Relic Browser is used for monitoring JS Errors. JS Errors are reported to New Relic in a variety of ways: When we add a try/catch statement, we should call the newrelic API's noticeError method. For example: try { await doSomething (); } catch ( error ) { tracker . noticeError ( error ); } Our ErrorBoundary component catches any errors that bubble up from its descendant child components The New Relic snippet should automatically catch errors that our React app doesn't catch Errors caught by our ErrorBoundary component are reported to New Relic and also include a componentStack custom attribute, which provides additional information about which component threw the error. New Relic Browser's UI only displays the error's stack trace, so if you want to view the componentStack value, you'll need to use NRQL to query the JavaScriptError : SELECT timestamp , componentStack , stackTrace FROM JavaScriptError WHERE appName = 'PUT THE APP NAME HERE'","title":"JS Errors"},{"location":"portal/monitoring/#environments","text":"Each environment requires its own New Relic Browser \"app\" in order for us to differentiate between data coming in from the different environments. Create these through the New Relic site. Each New Relic app has its own applicationId , which will need set as part of the JS snippet included in our HTML. CP-417 is tracking the work to create New Relic apps for each environment.","title":"Environments"},{"location":"portal/monitoring/#enabling-new-relic-browser","text":"If you have a browser extension installed to block trackers, you may need to safelist the New Relic snippet to allow it to make requests. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe payloads occasionally being sent to bam.nr-data.net","title":"Enabling New Relic Browser"},{"location":"portal/monitoring/#related","text":"Error monitoring research","title":"Related"},{"location":"portal/naming-conventions/","text":"Naming conventions Field names Input and model field names should match the API field names, which means they should be formatted as snake case. \u2705 Like this \ud83d\uded1 Not like this first_name firstName Benefits Eliminates the need for additional code to convert a Portal naming style to the API naming style Consistent with what the field names would need to be if we were submitting the data directly through the HTML <form> rather than through JS.","title":"Naming conventions"},{"location":"portal/naming-conventions/#naming-conventions","text":"","title":"Naming conventions"},{"location":"portal/naming-conventions/#field-names","text":"Input and model field names should match the API field names, which means they should be formatted as snake case. \u2705 Like this \ud83d\uded1 Not like this first_name firstName Benefits Eliminates the need for additional code to convert a Portal naming style to the API naming style Consistent with what the field names would need to be if we were submitting the data directly through the HTML <form> rather than through JS.","title":"Field names"},{"location":"portal/software-architecture/","text":"Software Architecture This page describes the software architecture and design patterns used for the Portal. Overview The primary design pattern used in the Portal is Model-View-Controller , which is the design pattern most commonly used whenever designing any application with a user interface. Data Models Not to be confused with the \"Model\" in Model-View-Controller (MVC), data models represent concepts in the context of the PFML program and the Portal. Data models are used throughout business logic and controller logic, but are not used by lower level View components which should be domain agnostic. Data models should only represent data and not have any business logic or behavior or have any dependencies outside of data models (with the possible exception of simple helper functions). App The App is the \"Controller\" for the application and is the effective entry point to the application as directed by the Nextjs framework. The App controls web application logic, what state to pass to pages, and how to handle application events like submitting claims. Top level application logic should live here. Pages Page components are the primary \"Controller\" unit of the application and represent a single page of the user experience. Page components are the lowest level \"Controller\" in our application (with the exception of more complex View components that have their own nested MVC pattern). Page components control which \"View\" components to render, what state to pass to the components to render, and what event handlers to use to handle \"View\" events. Components Components are the primary \"View\" unit of the application. Simple components simply render data that is passed to them, and exposes events that parent components (the view's controller) can listen to. More complex components can themselves act as controllers and have their own models and nested controllers/views. The QuestionPage component is an example of a more complex component that includes a back button and a \"Save and Continue\" button with onSave and onContinue events that can be listened to. API The API module is reponsible for application-level business logic. This represents the top level \"Model\" in the MVC design pattern. This includes starting claims, submitting claims, identity proofing, setting payment preferences, etc. State Hooks State hooks are modules used by various components to define and update the state of the application. These hooks represent the \"Model\" in the MVC design pattern and should not have any knowledge of controller or view code. An example of a state hook is useFormState . Event Hooks Event hooks are modules that define functions to handle view events. These modules attach view events to the appropriate model update functions, and are the glue that allows us to keep \"Model\" and \"View\" decoupled from each other. Examples of event hooks include useHandleInputChange and useAppLogic . Dependencies To help prevent technical debt, when adding new modules, consider adding assertions to dependencies.test.js to restrict where the module is used, or what dependencies the new module is allowed to have. Services Services expose functionality that offers a coherent feature / addresses a business need (e.g. services/featureFlags.js ). A rule of thumb to use is that services are things that could conceptually be turned into separate microservices. In practice, some of these will actually be served by external services and others will not. Examples include caching, translation/localization, validation, authentication, and any APIs. For example, in our codebase we already have these services: Error monitoring: uses external New Relic API, so this is an \"actual\" service Feature flags: these don't use an external APIs in this implementation but instead just uses the browser APIs (cookies) to accomplish the task. We could conceivably refactor this module to rely on an external API though. For example LaunchDarkly is precisely a product that offers \"feature flags as a service\" but for now we're just using a homegrown \"feature flags service\") Utilities Utilities ( utils ) on the other hand are lightweight functions that are useful but don't really offer any business value on their own. They often fall into the category of filling in gaps of the programming language like manipulating strings or other simple data structures that the language itself doesn't offer functions for, and for which we don't feel the need to introduce a separate library/dependency for. A rule of thumb to use for utilities is that they should be generally small and standalone i.e. they shouldn't have any non-trivial dependencies (like external APIs or large libraries).","title":"Software Architecture"},{"location":"portal/software-architecture/#software-architecture","text":"This page describes the software architecture and design patterns used for the Portal.","title":"Software Architecture"},{"location":"portal/software-architecture/#overview","text":"The primary design pattern used in the Portal is Model-View-Controller , which is the design pattern most commonly used whenever designing any application with a user interface.","title":"Overview"},{"location":"portal/software-architecture/#data-models","text":"Not to be confused with the \"Model\" in Model-View-Controller (MVC), data models represent concepts in the context of the PFML program and the Portal. Data models are used throughout business logic and controller logic, but are not used by lower level View components which should be domain agnostic. Data models should only represent data and not have any business logic or behavior or have any dependencies outside of data models (with the possible exception of simple helper functions).","title":"Data Models"},{"location":"portal/software-architecture/#app","text":"The App is the \"Controller\" for the application and is the effective entry point to the application as directed by the Nextjs framework. The App controls web application logic, what state to pass to pages, and how to handle application events like submitting claims. Top level application logic should live here.","title":"App"},{"location":"portal/software-architecture/#pages","text":"Page components are the primary \"Controller\" unit of the application and represent a single page of the user experience. Page components are the lowest level \"Controller\" in our application (with the exception of more complex View components that have their own nested MVC pattern). Page components control which \"View\" components to render, what state to pass to the components to render, and what event handlers to use to handle \"View\" events.","title":"Pages"},{"location":"portal/software-architecture/#components","text":"Components are the primary \"View\" unit of the application. Simple components simply render data that is passed to them, and exposes events that parent components (the view's controller) can listen to. More complex components can themselves act as controllers and have their own models and nested controllers/views. The QuestionPage component is an example of a more complex component that includes a back button and a \"Save and Continue\" button with onSave and onContinue events that can be listened to.","title":"Components"},{"location":"portal/software-architecture/#api","text":"The API module is reponsible for application-level business logic. This represents the top level \"Model\" in the MVC design pattern. This includes starting claims, submitting claims, identity proofing, setting payment preferences, etc.","title":"API"},{"location":"portal/software-architecture/#state-hooks","text":"State hooks are modules used by various components to define and update the state of the application. These hooks represent the \"Model\" in the MVC design pattern and should not have any knowledge of controller or view code. An example of a state hook is useFormState .","title":"State Hooks"},{"location":"portal/software-architecture/#event-hooks","text":"Event hooks are modules that define functions to handle view events. These modules attach view events to the appropriate model update functions, and are the glue that allows us to keep \"Model\" and \"View\" decoupled from each other. Examples of event hooks include useHandleInputChange and useAppLogic .","title":"Event Hooks"},{"location":"portal/software-architecture/#dependencies","text":"To help prevent technical debt, when adding new modules, consider adding assertions to dependencies.test.js to restrict where the module is used, or what dependencies the new module is allowed to have.","title":"Dependencies"},{"location":"portal/software-architecture/#services","text":"Services expose functionality that offers a coherent feature / addresses a business need (e.g. services/featureFlags.js ). A rule of thumb to use is that services are things that could conceptually be turned into separate microservices. In practice, some of these will actually be served by external services and others will not. Examples include caching, translation/localization, validation, authentication, and any APIs. For example, in our codebase we already have these services: Error monitoring: uses external New Relic API, so this is an \"actual\" service Feature flags: these don't use an external APIs in this implementation but instead just uses the browser APIs (cookies) to accomplish the task. We could conceivably refactor this module to rely on an external API though. For example LaunchDarkly is precisely a product that offers \"feature flags as a service\" but for now we're just using a homegrown \"feature flags service\")","title":"Services"},{"location":"portal/software-architecture/#utilities","text":"Utilities ( utils ) on the other hand are lightweight functions that are useful but don't really offer any business value on their own. They often fall into the category of filling in gaps of the programming language like manipulating strings or other simple data structures that the language itself doesn't offer functions for, and for which we don't feel the need to introduce a separate library/dependency for. A rule of thumb to use for utilities is that they should be generally small and standalone i.e. they shouldn't have any non-trivial dependencies (like external APIs or large libraries).","title":"Utilities"},{"location":"portal/storybook/","text":"Storybook Storybook is a tool for UI development. It makes development faster and easier by isolating components. This allows you to work on one component at a time. You can develop entire UIs without needing to run the Portal application, or navigating around the Portal flow. You can read more below about the process and collaboration goals behind our usage of Storybook. The ultimate output is a static site that currently gets deployed here . (Related: Deploy docs ) Contributing to Storybook First: The Storybook docs are great. If your question isn't answered in this doc, it's likely answered in their docs. This doc is more of a TLDR to get you started contributing to our Storybook. Developers can run Storybook locally by running: npm run docs Adding a new story Each page in Storybook is generated from *.stories.js files located in portal/storybook/stories/ . This file exports functions ( \"stories\" ), each of which is rendered on the page. The functions can render any arbitrary React, so a Story can render a page, component, or plain HTML. For example, here's a Storybook page with a single story that renders a preview of our Button component: // portal/storybook/stories/components/Button.stories.js import Button from \"src/components/Button\"; import React from \"react\"; export default { // this determines where the story shows in the site's sidebar menu: title: \"Components/Button\", // this is used to generate the props table on the page: component: Button, // these are passed into each story and enable nifty // live editing in the site's UI: args: { children: \"Save and continue\", handleClick: () => alert(\"Clicked!\"), }, }; export const Primary = (args) => { return <Button {...args} />; }; Learn more about writing stories \u2192 Previewing entire pages In Next.js, a page is just a React component . This enables some pretty cool capabilities when combined with Storybook. Specifically, this means we can create a Story that renders a Page component, which allows us to preview an entire page without navigating through the entire application flow to preview the page. For the Create Claim flow, we're generating a Storybook page for each page in the flow . Engineers can override the generated story by adding a *.stories.js file for the page in the storybook/stories/pages/claims/ directory. How Storybook interacts with our source code Storybook stories import and render the same components used in the live site. Below is a visualization attempting to demonstrate how assets are shared between our web application and the Storybook site. For engineers For the engineering team, Storybook provides a place to document components and also provides a place outside of our Next.js app to preview new components being developed. For engineers, Storybook aims to answer questions like: What components exist? How does this component behave when rendered? What is this component for? What props does this component expect? Engineers could also look at the source code to answer most of these questions, but Storybook provides a friendly UI to answer them. It also provides a mechanism for producing smaller pull requests \u2014 for instance, rather than creating a component and connecting it to our application logic, an engineer could first create a pull request that adds just the component and a corresponding Storybook story for previewing the component. For designers There's also a world where Storybook can be used as a shared tool, supporting a more collaborative workflow between Engineering, Design, and Product teams. One goal is to have a source of truth for design pattern documentation and guidance. Ideally that lives close to the actual implementation of the patterns. The goals for Storybook in this case would be to answer the questions above, as well as questions like: Why does this design pattern exist? What problems is it addressing? What does this design pattern look like at different screen sizes? What have we learned about this pattern through user research? How has it evolved? This may mean that some Storybook pages are more in-depth than others. These more extensive documentation pages would be a collaboration between Engineering and Design. Engineering's role might be: code the components and setup the Storybook page with an example of the component. It would then be Design's role to contribute design documentation, like any decisions or research that fed into the creation of the pattern, or guidance on different ways the component should be used. There may be times when Engineering and Design pair on a Storybook page to ensure the implementation achieves the intended design.","title":"Storybook"},{"location":"portal/storybook/#storybook","text":"Storybook is a tool for UI development. It makes development faster and easier by isolating components. This allows you to work on one component at a time. You can develop entire UIs without needing to run the Portal application, or navigating around the Portal flow. You can read more below about the process and collaboration goals behind our usage of Storybook. The ultimate output is a static site that currently gets deployed here . (Related: Deploy docs )","title":"Storybook"},{"location":"portal/storybook/#contributing-to-storybook","text":"First: The Storybook docs are great. If your question isn't answered in this doc, it's likely answered in their docs. This doc is more of a TLDR to get you started contributing to our Storybook. Developers can run Storybook locally by running: npm run docs","title":"Contributing to Storybook"},{"location":"portal/storybook/#adding-a-new-story","text":"Each page in Storybook is generated from *.stories.js files located in portal/storybook/stories/ . This file exports functions ( \"stories\" ), each of which is rendered on the page. The functions can render any arbitrary React, so a Story can render a page, component, or plain HTML. For example, here's a Storybook page with a single story that renders a preview of our Button component: // portal/storybook/stories/components/Button.stories.js import Button from \"src/components/Button\"; import React from \"react\"; export default { // this determines where the story shows in the site's sidebar menu: title: \"Components/Button\", // this is used to generate the props table on the page: component: Button, // these are passed into each story and enable nifty // live editing in the site's UI: args: { children: \"Save and continue\", handleClick: () => alert(\"Clicked!\"), }, }; export const Primary = (args) => { return <Button {...args} />; }; Learn more about writing stories \u2192","title":"Adding a new story"},{"location":"portal/storybook/#previewing-entire-pages","text":"In Next.js, a page is just a React component . This enables some pretty cool capabilities when combined with Storybook. Specifically, this means we can create a Story that renders a Page component, which allows us to preview an entire page without navigating through the entire application flow to preview the page. For the Create Claim flow, we're generating a Storybook page for each page in the flow . Engineers can override the generated story by adding a *.stories.js file for the page in the storybook/stories/pages/claims/ directory.","title":"Previewing entire pages"},{"location":"portal/storybook/#how-storybook-interacts-with-our-source-code","text":"Storybook stories import and render the same components used in the live site. Below is a visualization attempting to demonstrate how assets are shared between our web application and the Storybook site.","title":"How Storybook interacts with our source code"},{"location":"portal/storybook/#for-engineers","text":"For the engineering team, Storybook provides a place to document components and also provides a place outside of our Next.js app to preview new components being developed. For engineers, Storybook aims to answer questions like: What components exist? How does this component behave when rendered? What is this component for? What props does this component expect? Engineers could also look at the source code to answer most of these questions, but Storybook provides a friendly UI to answer them. It also provides a mechanism for producing smaller pull requests \u2014 for instance, rather than creating a component and connecting it to our application logic, an engineer could first create a pull request that adds just the component and a corresponding Storybook story for previewing the component.","title":"For engineers"},{"location":"portal/storybook/#for-designers","text":"There's also a world where Storybook can be used as a shared tool, supporting a more collaborative workflow between Engineering, Design, and Product teams. One goal is to have a source of truth for design pattern documentation and guidance. Ideally that lives close to the actual implementation of the patterns. The goals for Storybook in this case would be to answer the questions above, as well as questions like: Why does this design pattern exist? What problems is it addressing? What does this design pattern look like at different screen sizes? What have we learned about this pattern through user research? How has it evolved? This may mean that some Storybook pages are more in-depth than others. These more extensive documentation pages would be a collaboration between Engineering and Design. Engineering's role might be: code the components and setup the Storybook page with an example of the component. It would then be Design's role to contribute design documentation, like any decisions or research that fed into the creation of the pattern, or guidance on different ways the component should be used. There may be times when Engineering and Design pair on a Storybook page to ensure the implementation achieves the intended design.","title":"For designers"},{"location":"portal/tests/","text":"Tests Introduction Jest is used as our JS test runner and is very similar to Jasmine. Read the Jest documentation to learn how to write assertions. A good place to start if you are new to JS testing is to learn about Using Matchers . Below is an example of a test: import sum from \"./sum\" ; describe ( \"sum\" , () => { it ( \"adds 1 + 2 to equal 3\" , () => { const result = sum ( 1 , 2 ); expect ( result ). toBe ( 3 ); }); }); Creating new test files A test file should be placed in the appropriate tests directory (e.g. portal/tests ) rather than alongside the file it tests. These test files should have the same name as the file they're testing, and have .test.js as the extension. For example, pages/index.js and tests/pages/index.test.js . Unit tests Enzyme is a test utility used alongside Jest to test React components. Read the Enzyme documentation to learn how to Render React components and how to pull information from those React components in order to run test assertions against them. For simple, stateless components (which hopefully will be most components), you can \"Shallow render\" the component. Shallow rendering is useful to constrain yourself to testing a component as a unit, and to ensure that your tests aren't indirectly asserting on behavior of child components. It's also helps reduce test suite runtime. Below is an example of a React component test: import UploadForm from \"./UploadForm\" ; import { shallow } from \"enzyme\" ; describe ( \"<UploadForm>\" , () => { describe ( \"when the form is submitted\" , () => { it ( \"calls onSubmit\" , () => { const mockSubmit = jest . fn (); const wrapper = shallow ( < UploadForm onSubmit = { mockSubmit } /> ); wrapper . simulate ( \"submit\" ); expect ( mockSubmit ). toHaveBeenCalled (); }); }); }); Snapshot tests Snapshot tests are useful for testing when a React component or JSON output changes unexpectedly. A typical snapshot test case is to render a UI component, take a snapshot, then compares it to the last snapshot that was taken. The test will fail if the two snapshots do not match. If a snapshot test fails, you should identify whether it failed because the change was unexpected. If it was an unexpected change, then you may have unintentionally broke an expected behavior, in which case you should investigate and fix it before sending the PR out for review. If it failed because you intentionally changed something related to the test, then the snapshot should be updated to reflect the intended change. To update snapshots, run: npm run test:update-snapshots Learn more about Snapshot Testing. JSON snapshot example it ( \"renders the fields with the expected content and attributes\" , () => { const output = callMethodAndReturnJSON (); // You can use inline snapshots if the output is fairly short: expect ( output ). toMatchInlineSnapshot (); }); React snapshot example it ( \"renders the fields with the expected content and attributes\" , () => { const wrapper = shallow ( < UploadFields /> ); // You can output the snapshot to a separate file if its output is verbose: expect ( wrapper ). toMatchSnapshot (); }); Mocks Mock functions make it easy to test the links between code by erasing the actual implementation of a function, capturing calls to the function (and the parameters passed in those calls), capturing instances of constructor functions when instantiated with new, and allowing test-time configuration of return values. The quickest way to mock a module is to call jest.mock('MODULE_NAME_HERE') at the top of your test file. To create a manual mock of a Node module, create a file in a top-level __mocks__ directory. You can also create a Mock function/spy using jest.fn() Learn more about Mock Functions . Test coverage Jest includes built-in support for measuring test coverage , using Istanbul . The coverageReporters Jest setting can be modified for more advanced test coverage use cases.","title":"Tests"},{"location":"portal/tests/#tests","text":"","title":"Tests"},{"location":"portal/tests/#introduction","text":"Jest is used as our JS test runner and is very similar to Jasmine. Read the Jest documentation to learn how to write assertions. A good place to start if you are new to JS testing is to learn about Using Matchers . Below is an example of a test: import sum from \"./sum\" ; describe ( \"sum\" , () => { it ( \"adds 1 + 2 to equal 3\" , () => { const result = sum ( 1 , 2 ); expect ( result ). toBe ( 3 ); }); });","title":"Introduction"},{"location":"portal/tests/#creating-new-test-files","text":"A test file should be placed in the appropriate tests directory (e.g. portal/tests ) rather than alongside the file it tests. These test files should have the same name as the file they're testing, and have .test.js as the extension. For example, pages/index.js and tests/pages/index.test.js .","title":"Creating new test files"},{"location":"portal/tests/#unit-tests","text":"Enzyme is a test utility used alongside Jest to test React components. Read the Enzyme documentation to learn how to Render React components and how to pull information from those React components in order to run test assertions against them. For simple, stateless components (which hopefully will be most components), you can \"Shallow render\" the component. Shallow rendering is useful to constrain yourself to testing a component as a unit, and to ensure that your tests aren't indirectly asserting on behavior of child components. It's also helps reduce test suite runtime. Below is an example of a React component test: import UploadForm from \"./UploadForm\" ; import { shallow } from \"enzyme\" ; describe ( \"<UploadForm>\" , () => { describe ( \"when the form is submitted\" , () => { it ( \"calls onSubmit\" , () => { const mockSubmit = jest . fn (); const wrapper = shallow ( < UploadForm onSubmit = { mockSubmit } /> ); wrapper . simulate ( \"submit\" ); expect ( mockSubmit ). toHaveBeenCalled (); }); }); });","title":"Unit tests"},{"location":"portal/tests/#snapshot-tests","text":"Snapshot tests are useful for testing when a React component or JSON output changes unexpectedly. A typical snapshot test case is to render a UI component, take a snapshot, then compares it to the last snapshot that was taken. The test will fail if the two snapshots do not match. If a snapshot test fails, you should identify whether it failed because the change was unexpected. If it was an unexpected change, then you may have unintentionally broke an expected behavior, in which case you should investigate and fix it before sending the PR out for review. If it failed because you intentionally changed something related to the test, then the snapshot should be updated to reflect the intended change. To update snapshots, run: npm run test:update-snapshots Learn more about Snapshot Testing.","title":"Snapshot tests"},{"location":"portal/tests/#json-snapshot-example","text":"it ( \"renders the fields with the expected content and attributes\" , () => { const output = callMethodAndReturnJSON (); // You can use inline snapshots if the output is fairly short: expect ( output ). toMatchInlineSnapshot (); });","title":"JSON snapshot example"},{"location":"portal/tests/#react-snapshot-example","text":"it ( \"renders the fields with the expected content and attributes\" , () => { const wrapper = shallow ( < UploadFields /> ); // You can output the snapshot to a separate file if its output is verbose: expect ( wrapper ). toMatchSnapshot (); });","title":"React snapshot example"},{"location":"portal/tests/#mocks","text":"Mock functions make it easy to test the links between code by erasing the actual implementation of a function, capturing calls to the function (and the parameters passed in those calls), capturing instances of constructor functions when instantiated with new, and allowing test-time configuration of return values. The quickest way to mock a module is to call jest.mock('MODULE_NAME_HERE') at the top of your test file. To create a manual mock of a Node module, create a file in a top-level __mocks__ directory. You can also create a Mock function/spy using jest.fn() Learn more about Mock Functions .","title":"Mocks"},{"location":"portal/tests/#test-coverage","text":"Jest includes built-in support for measuring test coverage , using Istanbul . The coverageReporters Jest setting can be modified for more advanced test coverage use cases.","title":"Test coverage"},{"location":"portal/web-analytics/","text":"Web Analytics Massachusetts uses Google Analytics for cross-domain tracking on mass.gov properties. This works by including a Google Tag Manager JS snippet in the document head. Google Tag Manager is configured to dynamically insert Google Analytics onto the page when the web page is initially loaded. Viewing analytics Analytics for all of our environments are visible in the \"mass.gov cross domain tracking\" Google Analytics property. From within this property, you can filter results by the environment's domain you're interested in seeing. For example, to view real time traffic for all paidleave environments, you can visit this page . Custom metrics Single-page Application page views and custom event tracking should be setup through Google Tag Manager triggers . Using the Google Analytics ga function isn't recommended when using Google Tag Manager. Environment Configuration Each Portal environment should have a corresponding Google Tag Manager environment. Once a Google Tag Manager environment exists, we need to set the Portal gtmConfig.auth and gtmConfig.preview environment variables based on that Google Tag Manager environment's values. You can find these values through the Google Tag Manager website in the Admin section by navigating to the environment and looking at the JavaScript snippet for that environment. Look for a URL with gtm_auth and gtm_preview query parameters. The values for those query parameters should be set as the corresponding Portal environment variables. Content Security Policy In order for the Google Tag Manager scripts and Google Analytics scripts to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: The inline Google Tag Manager scripts Scripts from https://www.googletagmanager.com/ Scripts from https://www.google-analytics.com/ Tracking images from https://www.google-analytics.com/ The inline scripts need to be allowed by computing a Base64 encoded SHA-256 hash of the inline script and adding the hash to the allowed script sources in the Content Security Policy. To compute a hash, you can use an online tool like SHA256 Hash Generator . Or to use the Terminal, you can save the JavaScript snippet into a file e.g. gtm-snippet and run $ cat gtm-snippet | openssl sha256 -binary | openssl base64 Change Management To make changes to Google Tag Manager, create a new Google Tag Manager version with the desired changes, and publish those changes to the Google Tag Manager Test environment. To test those changes before publishing to Google Tag Manager's Stage and Prod environments, we need to be able to override the environment configuration to use the Google Tag Manager Test environment, which is something that will be implemented as part of CP-645 . In the meantime, you can manually insert the Google Tag Manager Test environment snippet onto the page you're viewing by executing the following lines of code in the browser console. Note the gtm_auth and gtm_preview values below are for the Google Tag Manager Test environment. var myScript = document.createElement('script'); myScript.textContent = \"(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\\n\" + \"new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\\n\" + \"j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\\n\" + \"'https://www.googletagmanager.com/gtm.js?id='+i+dl+ '&gtm_auth=SiSVu0U7VjoUiceaFWQeqA&gtm_preview=env-5&gtm_cookies_win=x';f.parentNode.insertBefore(j,f);\\n\" + \"})(window,document,'script','dataLayer','GTM-MCLNNQC');\"; document.head.appendChild(myScript); Publishing a new GTM version Once you've tested the changes in a Test environment, you can publish a new Google Tag Manager version to other environments. You can do this from the Versions tab, clicking the ... icon, and then clicking \"Publish to...\" Account Set Up In order to update Google Tag Manager configuration, you need a Google account associated with your @mass.gov email, and that account needs to be granted publish access in Google Tag Manager. In order to view Google Analytics data, your @mass.gov Google account needs to be added as a user to the mass.gov Google Analytics account. Enabling Google Tag Manager and Google Analytics If you have a browser extension installed to block trackers, you may need to safelist the google tag manager and google analytics. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe network requests to googletagmanager.com and google-analytics.com . Related Web analytics research","title":"Web Analytics"},{"location":"portal/web-analytics/#web-analytics","text":"Massachusetts uses Google Analytics for cross-domain tracking on mass.gov properties. This works by including a Google Tag Manager JS snippet in the document head. Google Tag Manager is configured to dynamically insert Google Analytics onto the page when the web page is initially loaded.","title":"Web Analytics"},{"location":"portal/web-analytics/#viewing-analytics","text":"Analytics for all of our environments are visible in the \"mass.gov cross domain tracking\" Google Analytics property. From within this property, you can filter results by the environment's domain you're interested in seeing. For example, to view real time traffic for all paidleave environments, you can visit this page .","title":"Viewing analytics"},{"location":"portal/web-analytics/#custom-metrics","text":"Single-page Application page views and custom event tracking should be setup through Google Tag Manager triggers . Using the Google Analytics ga function isn't recommended when using Google Tag Manager.","title":"Custom metrics"},{"location":"portal/web-analytics/#environment-configuration","text":"Each Portal environment should have a corresponding Google Tag Manager environment. Once a Google Tag Manager environment exists, we need to set the Portal gtmConfig.auth and gtmConfig.preview environment variables based on that Google Tag Manager environment's values. You can find these values through the Google Tag Manager website in the Admin section by navigating to the environment and looking at the JavaScript snippet for that environment. Look for a URL with gtm_auth and gtm_preview query parameters. The values for those query parameters should be set as the corresponding Portal environment variables.","title":"Environment Configuration"},{"location":"portal/web-analytics/#content-security-policy","text":"In order for the Google Tag Manager scripts and Google Analytics scripts to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: The inline Google Tag Manager scripts Scripts from https://www.googletagmanager.com/ Scripts from https://www.google-analytics.com/ Tracking images from https://www.google-analytics.com/ The inline scripts need to be allowed by computing a Base64 encoded SHA-256 hash of the inline script and adding the hash to the allowed script sources in the Content Security Policy. To compute a hash, you can use an online tool like SHA256 Hash Generator . Or to use the Terminal, you can save the JavaScript snippet into a file e.g. gtm-snippet and run $ cat gtm-snippet | openssl sha256 -binary | openssl base64","title":"Content Security Policy"},{"location":"portal/web-analytics/#change-management","text":"To make changes to Google Tag Manager, create a new Google Tag Manager version with the desired changes, and publish those changes to the Google Tag Manager Test environment. To test those changes before publishing to Google Tag Manager's Stage and Prod environments, we need to be able to override the environment configuration to use the Google Tag Manager Test environment, which is something that will be implemented as part of CP-645 . In the meantime, you can manually insert the Google Tag Manager Test environment snippet onto the page you're viewing by executing the following lines of code in the browser console. Note the gtm_auth and gtm_preview values below are for the Google Tag Manager Test environment. var myScript = document.createElement('script'); myScript.textContent = \"(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\\n\" + \"new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\\n\" + \"j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\\n\" + \"'https://www.googletagmanager.com/gtm.js?id='+i+dl+ '&gtm_auth=SiSVu0U7VjoUiceaFWQeqA&gtm_preview=env-5&gtm_cookies_win=x';f.parentNode.insertBefore(j,f);\\n\" + \"})(window,document,'script','dataLayer','GTM-MCLNNQC');\"; document.head.appendChild(myScript);","title":"Change Management"},{"location":"portal/web-analytics/#publishing-a-new-gtm-version","text":"Once you've tested the changes in a Test environment, you can publish a new Google Tag Manager version to other environments. You can do this from the Versions tab, clicking the ... icon, and then clicking \"Publish to...\"","title":"Publishing a new GTM version"},{"location":"portal/web-analytics/#account-set-up","text":"In order to update Google Tag Manager configuration, you need a Google account associated with your @mass.gov email, and that account needs to be granted publish access in Google Tag Manager. In order to view Google Analytics data, your @mass.gov Google account needs to be added as a user to the mass.gov Google Analytics account.","title":"Account Set Up"},{"location":"portal/web-analytics/#enabling-google-tag-manager-and-google-analytics","text":"If you have a browser extension installed to block trackers, you may need to safelist the google tag manager and google analytics. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe network requests to googletagmanager.com and google-analytics.com .","title":"Enabling Google Tag Manager and Google Analytics"},{"location":"portal/web-analytics/#related","text":"Web analytics research","title":"Related"},{"location":"releases/api/","text":"environment release deployments test 72d33ebfb ( 62 commits ahead of api/v1.20.0-rc1 ) GitHub stage api/v1.20.1 GitHub prod api/v1.20.1 GitHub performance api/v1.20.1 GitHub training api/v1.20.1 GitHub uat api/v1.20.1-rc1 GitHub Full release notes are available in Confluence . Next Release ( Jira List ): PUB-32 : add CheckReader class to parse PUB check return files ( #3667 ) PUB-159 : Resolve Writeback Issues ( #3745 ) PUB-150 : initial e2e scenario and data generator setup ( #3755 ) PUB-148 : Make certain all reports are created ( #3706 ) PUB-148 : Make SQL reports writeable to S3, fix UUID issue ( #3689 ) PUB-148 : Give default ECS task executor access to the common parameter store path ( #3724 ) PUB-148 : Fix Experian auth token credential path ( #3723 ) PUB-148 : Allow all PUB payment ECS tasks to connect to FINEOS S3 ( #3729 ) PUB-143 : Updated employer reimbursement report. ( #3699 ) PUB-141 : Add Experian auth token ( #3716 ) PUB-140 : Update logic to determine employers and update logic for non-standard payments ( #3744 ) PUB-138 : update memo to include leave type ( #3677 ) PUB-133 : FINEOS Writeback Updates - Check number and transaction status date ( #3722 ) PUB-102 : Add FINEOS writeback 2 ( #3703 ) INFRA-96 : Run Tests in Parallel ( #3618 ) INFRA-385 : Add adhoc workflow for testing API Gateway changes ( #3720 ) INFRA-380 : specify container name for step function overrides ( #3696 ) INFRA-376 : Fix RDS log subscription to New Relic ( #3662 ) INFRA-362 : Allow nested paths for S3 files ( #3710 ) INFRA-362 : API S3 Proxy ( #3664 ) INFRA-348 : api gateway environment for fineos service pack ( #3701 ) INFRA-347 : api environment for cps preview ( #3730 ) INFRA-344 : Add breakfix API ( #3692 ) INFRA-31 : Delete verification_codes infra ( #3691 ) EMPLOYER-857 : Backend work - Add GET /claims endpoint ( #3609 ) EMPLOYER-857 : BE return FEINFormattedStr in EmployerResponse ( #3727 ) EMPLOYER-1138 : Add AdminGetUser permission to ECS Task for bulk import ( #3762 ) EMPLOYER-1124 : Adding correct formatting for notifications errors ( #3704 ) EMPLOYER-1006 : Erroring if file is missing FEIN or email for a row ( #3650 ) CP-2074 : Set Docker platform to AMD64 to fix Apple Silicon issues ( #3732 ) CP-2055 : Grant API permission to cognito-idp:AdminGetUser ( #3709 ) CP-1955 : caring leave data model updates ( #3679 ) API-1537 : Use leave request id in vpeiclaimdetails.csv ( #3747 ) API-1522 : Claimants added to FINEOS manually should be stored back to the employee tables ( #3660 ) API-1511 : Enable LookupTable ID<->Description mapping without DB connection ( #3461 ) Internal: Fix terraform API output IDs ( #3748 ) Raw * [PUB-32](https://lwd.atlassian.net/browse/PUB-32): add CheckReader class to parse PUB check return files ([#3667](https://github.com/EOLWD/pfml/pull/3667)) * [PUB-159](https://lwd.atlassian.net/browse/PUB-159): Resolve Writeback Issues ([#3745](https://github.com/EOLWD/pfml/pull/3745)) * [PUB-150](https://lwd.atlassian.net/browse/PUB-150): initial e2e scenario and data generator setup ([#3755](https://github.com/EOLWD/pfml/pull/3755)) * [PUB-148](https://lwd.atlassian.net/browse/PUB-148): Make certain all reports are created ([#3706](https://github.com/EOLWD/pfml/pull/3706)) * [PUB-148](https://lwd.atlassian.net/browse/PUB-148): Make SQL reports writeable to S3, fix UUID issue ([#3689](https://github.com/EOLWD/pfml/pull/3689)) * [PUB-148](https://lwd.atlassian.net/browse/PUB-148): Give default ECS task executor access to the common parameter store path ([#3724](https://github.com/EOLWD/pfml/pull/3724)) * [PUB-148](https://lwd.atlassian.net/browse/PUB-148): Fix Experian auth token credential path ([#3723](https://github.com/EOLWD/pfml/pull/3723)) * [PUB-148](https://lwd.atlassian.net/browse/PUB-148): Allow all PUB payment ECS tasks to connect to FINEOS S3 ([#3729](https://github.com/EOLWD/pfml/pull/3729)) * [PUB-143](https://lwd.atlassian.net/browse/PUB-143): Updated employer reimbursement report. ([#3699](https://github.com/EOLWD/pfml/pull/3699)) * [PUB-141](https://lwd.atlassian.net/browse/PUB-141): Add Experian auth token ([#3716](https://github.com/EOLWD/pfml/pull/3716)) * [PUB-140](https://lwd.atlassian.net/browse/PUB-140): Update logic to determine employers and update logic for non-standard payments ([#3744](https://github.com/EOLWD/pfml/pull/3744)) * [PUB-138](https://lwd.atlassian.net/browse/PUB-138): update memo to include leave type ([#3677](https://github.com/EOLWD/pfml/pull/3677)) * [PUB-133](https://lwd.atlassian.net/browse/PUB-133): FINEOS Writeback Updates - Check number and transaction status date ([#3722](https://github.com/EOLWD/pfml/pull/3722)) * [PUB-102](https://lwd.atlassian.net/browse/PUB-102): Add FINEOS writeback 2 ([#3703](https://github.com/EOLWD/pfml/pull/3703)) * [INFRA-96](https://lwd.atlassian.net/browse/INFRA-96): Run Tests in Parallel ([#3618](https://github.com/EOLWD/pfml/pull/3618)) * [INFRA-385](https://lwd.atlassian.net/browse/INFRA-385): Add adhoc workflow for testing API Gateway changes ([#3720](https://github.com/EOLWD/pfml/pull/3720)) * [INFRA-380](https://lwd.atlassian.net/browse/INFRA-380): specify container name for step function overrides ([#3696](https://github.com/EOLWD/pfml/pull/3696)) * [INFRA-376](https://lwd.atlassian.net/browse/INFRA-376): Fix RDS log subscription to New Relic ([#3662](https://github.com/EOLWD/pfml/pull/3662)) * [INFRA-362](https://lwd.atlassian.net/browse/INFRA-362): Allow nested paths for S3 files ([#3710](https://github.com/EOLWD/pfml/pull/3710)) * [INFRA-362](https://lwd.atlassian.net/browse/INFRA-362): API S3 Proxy ([#3664](https://github.com/EOLWD/pfml/pull/3664)) * [INFRA-348](https://lwd.atlassian.net/browse/INFRA-348): api gateway environment for fineos service pack ([#3701](https://github.com/EOLWD/pfml/pull/3701)) * [INFRA-347](https://lwd.atlassian.net/browse/INFRA-347): api environment for cps preview ([#3730](https://github.com/EOLWD/pfml/pull/3730)) * [INFRA-344](https://lwd.atlassian.net/browse/INFRA-344): Add breakfix API ([#3692](https://github.com/EOLWD/pfml/pull/3692)) * [INFRA-31](https://lwd.atlassian.net/browse/INFRA-31): Delete verification_codes infra ([#3691](https://github.com/EOLWD/pfml/pull/3691)) * [EMPLOYER-857](https://lwd.atlassian.net/browse/EMPLOYER-857): Backend work - Add GET /claims endpoint ([#3609](https://github.com/EOLWD/pfml/pull/3609)) * [EMPLOYER-857](https://lwd.atlassian.net/browse/EMPLOYER-857): BE return FEINFormattedStr in EmployerResponse ([#3727](https://github.com/EOLWD/pfml/pull/3727)) * [EMPLOYER-1138](https://lwd.atlassian.net/browse/EMPLOYER-1138): Add AdminGetUser permission to ECS Task for bulk import ([#3762](https://github.com/EOLWD/pfml/pull/3762)) * [EMPLOYER-1124](https://lwd.atlassian.net/browse/EMPLOYER-1124): Adding correct formatting for notifications errors ([#3704](https://github.com/EOLWD/pfml/pull/3704)) * [EMPLOYER-1006](https://lwd.atlassian.net/browse/EMPLOYER-1006): Erroring if file is missing FEIN or email for a row ([#3650](https://github.com/EOLWD/pfml/pull/3650)) * [CP-2074](https://lwd.atlassian.net/browse/CP-2074): Set Docker platform to AMD64 to fix Apple Silicon issues ([#3732](https://github.com/EOLWD/pfml/pull/3732)) * [CP-2055](https://lwd.atlassian.net/browse/CP-2055): Grant API permission to cognito-idp:AdminGetUser ([#3709](https://github.com/EOLWD/pfml/pull/3709)) * [CP-1955](https://lwd.atlassian.net/browse/CP-1955): caring leave data model updates ([#3679](https://github.com/EOLWD/pfml/pull/3679)) * [API-1537](https://lwd.atlassian.net/browse/API-1537): Use leave request id in vpeiclaimdetails.csv ([#3747](https://github.com/EOLWD/pfml/pull/3747)) * [API-1522](https://lwd.atlassian.net/browse/API-1522): Claimants added to FINEOS manually should be stored back to the employee tables ([#3660](https://github.com/EOLWD/pfml/pull/3660)) * [API-1511](https://lwd.atlassian.net/browse/API-1511): Enable LookupTable ID < ->Description mapping without DB connection ([#3461](https://github.com/EOLWD/pfml/pull/3461)) * Internal: Fix terraform API output IDs ([#3748](https://github.com/EOLWD/pfml/pull/3748))","title":"Api"},{"location":"releases/portal/","text":"environment release deployments test 72d33ebfb ( 61 commits ahead of portal/v14.0-rc1 ) GitHub stage portal/v14.0-rc1 GitHub prod portal/v13.0 GitHub performance portal/v13.0 GitHub training portal/v14.0-rc1 GitHub uat portal/v13.0 GitHub Full release notes are available in Confluence . Next Release ( Jira List ): INFRA-368 : Add environment to FROM address for outgoing emails ( #3705 ) INFRA-347 : api environment for cps preview ( #3730 ) INFRA-343 : (1/2): Add breakfix portal environment infra ( #3714 ) EMPLOYER-1123 : Add caring leave to en-US file ( #3719 ) EMPLOYER-1114 : Introduce dashboard functionality on the welcome page ( #3728 ) CP-468 : Rename withClaim(s) to withBenefitsApplication(s) ( #3734 ) CP-468 : Rename useClaimsLogic to useBenefitsApplicationsLogic ( #3738 ) CP-468 : Rename \"claim\" data models and API module to \"benefits application\" terminology ( #3718 ) CP-1838 : Refactor schedule-variable test ( #3614 ) CP-1725 : Add Hint Text documentation to Storybook ( #3697 ) Raw * [INFRA-368](https://lwd.atlassian.net/browse/INFRA-368): Add environment to FROM address for outgoing emails ([#3705](https://github.com/EOLWD/pfml/pull/3705)) * [INFRA-347](https://lwd.atlassian.net/browse/INFRA-347): api environment for cps preview ([#3730](https://github.com/EOLWD/pfml/pull/3730)) * [INFRA-343](https://lwd.atlassian.net/browse/INFRA-343): (1/2): Add breakfix portal environment infra ([#3714](https://github.com/EOLWD/pfml/pull/3714)) * [EMPLOYER-1123](https://lwd.atlassian.net/browse/EMPLOYER-1123): Add caring leave to en-US file ([#3719](https://github.com/EOLWD/pfml/pull/3719)) * [EMPLOYER-1114](https://lwd.atlassian.net/browse/EMPLOYER-1114): Introduce dashboard functionality on the welcome page ([#3728](https://github.com/EOLWD/pfml/pull/3728)) * [CP-468](https://lwd.atlassian.net/browse/CP-468): Rename withClaim(s) to withBenefitsApplication(s) ([#3734](https://github.com/EOLWD/pfml/pull/3734)) * [CP-468](https://lwd.atlassian.net/browse/CP-468): Rename useClaimsLogic to useBenefitsApplicationsLogic ([#3738](https://github.com/EOLWD/pfml/pull/3738)) * [CP-468](https://lwd.atlassian.net/browse/CP-468): Rename \"claim\" data models and API module to \"benefits application\" terminology ([#3718](https://github.com/EOLWD/pfml/pull/3718)) * [CP-1838](https://lwd.atlassian.net/browse/CP-1838): Refactor schedule-variable test ([#3614](https://github.com/EOLWD/pfml/pull/3614)) * [CP-1725](https://lwd.atlassian.net/browse/CP-1725): Add Hint Text documentation to Storybook ([#3697](https://github.com/EOLWD/pfml/pull/3697))","title":"Portal"}]}