{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Massachusetts PFML (Portal and API) Applications to support claimants for the Paid Family and Medical Leave program.","title":"Massachusetts PFML (Portal and API)"},{"location":"#massachusetts-pfml-portal-and-api","text":"Applications to support claimants for the Paid Family and Medical Leave program.","title":"Massachusetts PFML (Portal and API)"},{"location":"archiving-code/","text":"Process for Archiving Code When a major feature is deprecated, the code being removed can represent a major engineering effort. It is often useful to be able to reference previously used design patterns and approaches to problems. To archive code: Create a branch at the point in time to snapshot: git checkout -b archive/ctr-payments Push the branch to github: git push origin archive/ctr-payments Update the archive documentation docs/<api or portal>/archive-reference.md with: a link to the git branch a description of what the archived feature is a explanation for why it is being archived Create a PR to remove the code from the main branch and link to the archive branch in the PR description.","title":"Process for Archiving Code"},{"location":"archiving-code/#process-for-archiving-code","text":"When a major feature is deprecated, the code being removed can represent a major engineering effort. It is often useful to be able to reference previously used design patterns and approaches to problems.","title":"Process for Archiving Code"},{"location":"archiving-code/#to-archive-code","text":"Create a branch at the point in time to snapshot: git checkout -b archive/ctr-payments Push the branch to github: git push origin archive/ctr-payments Update the archive documentation docs/<api or portal>/archive-reference.md with: a link to the git branch a description of what the archived feature is a explanation for why it is being archived Create a PR to remove the code from the main branch and link to the archive branch in the PR description.","title":"To archive code:"},{"location":"contributing/","text":"Contributing Delivery Workflow Please see the Delivery Workflow Confluence page for guidance on how to pick up tickets, when to deploy, which environment to deploy to, etc. Local Development For practical instructions on getting set up locally, see the repo README . Below are guidelines for working once you're ready. Commit in your own branch, and include your name in the branch name, e.g. lorenyu/pfml-123-new-feature . Try to keep commits small and focused on one thing. If you find yourself using \u201cand\u201d and \u201calso\u201d in your title or description, that may be a sign to break things up into smaller chunks. You can selectively git add files or use the interactive option: git add -p . Note that after a PR is merged, your commit message history will be squashed and rewritten. Your local commit history will persist on the PR's page unless you force-push a squashed version of the branch. Informative commit messages can help when context-switching between branches, or for PR clarity in larger PRs. See Seven Rules for an external reference on commit message best practices. To keep your branch updated with others' changes, you can: Rebase your changes onto main with git rebase main , or Merge main onto your own branch: git merge main . The favored pathway to keep your branch updated is to use git merge main. It's still possible to rebase and force-push your branch using git rebase main , but this pathway should be avoided if your branch is being worked on by more than one person at a time, due to the risk of causing unnecessary conflicts. Code Reviews Code reviews are intended to help all of us grow as engineers and improve the quality of what we ship. These guidelines are meant to reinforce those two goals. For authors or requesters: Include the JIRA ticket number in the title. For example: PFML-123: Implement API endpoint . A single JIRA ticket may be associated with multiple pull requests. Every change should have an associated JIRA ticket unless it is a documentation change. This makes it easier to search for PRs and generates a link in the Jira ticket to the pull request, which provides the program with an audit trail for verifying the relevant code changes, approvals, and test cases. For tickets that have subtasks, use the parent ticket's id in the title, and include the subtask in the PR description. If it is a documentation change, please prefix with docs: docs: Clean up CONTRIBUTING.md . Include a GitHub label if relevant. These labels can be a helpful wayfinding and context building tool, especially for new engineers onboarding into the codebase. The architecture label should be used for changes to abstractions or interfaces that usually affect multiple existing files. The pattern label should be used for a new structure or interface that will be reused later. The operations label should be used for changes to engineering operations, such as dev set-up or deployment. If your PR is a work-in-progress, or if you are looking for specific feedback on things, create a Draft Pull Request and state what you are looking for in the description. Your PR should be small enough that a reviewer can reasonably respond within 1-2 business days. For larger changes, break them down into a series of PRs. If refactors are included in your changes, try to split them out as recommended below. As a PR writer, you should consider your description and comments as documentation; current and future team members will refer to it to understand your design decisions. Include relevant context and business requirements, and add preemptive comments (in code or PR) for sections of code that may be confusing or worth debate. When you're ready to request a review, consider including folks who are less familiar with that part of the codebase. This allows others to see what you're working on, along with your development/communication style and ways of working. You may also reference a specific team for review using the teams defined in Github Teams and Codeowners . Once you've received feedback, acknowledge each comment with a response or code change. For reviewers: Aim to respond to code reviews within 1 business day. Remember to highlight things that you like and appreciate while reading through the changes, and to make any other feedback clearly actionable by indicating if it is optional preference, an important consideration, or an error. Don't be afraid to comment with a question, or to ask for clarification, or provide a suggestion, whenever you don\u2019t understand what is going on at first glance \u2014 or if you think an approach or decision can be improved. Code reviews give us a chance to learn from one another, and to reflect, iterate on, and document why certain decisions are made. Once you're ready to approve or request changes, err on the side of trust. Send a vote of approval if the PR looks ready except for small minor changes, and trust that the recipient will address your comments before merging by replying via comment or code to any asks. Use \"request changes\" sparingly, unless there's a blocking issue or major refactors that should be done. Merging a PR How to merge When merging a PR, use the PR description as a starting point . Copy it into the merge commit body and clean it up as desired. Please verify that the commit title includes the JIRA ticket. If the PR only has a single commit, GitHub will default to using that commit subject and description. This is often incorrect, missing the JIRA ticket. Example to follow: PFML-540: Implement GET claims endpoint (#145) https://lwd.atlassian.net/browse/PFML-540 Implements /claims endpoint which is a GET that takes a valid user ID and returns their first claim from the database. Demo: Unit tests and adhoc testing via swagger UI. Example to avoid: add claims endpoint (#145) * formatting * fix * change endpoint * initial endpoint When to merge A PR should only be merged when all its automated checks have returned green and when the PR has received at least one approval. Although GitHub does not block the merge of a PR that fails its checks, any failed checks should be treated as a blocking issue and resolved before the PR is merged. Merges & the API database A particular source of danger when merging a PR is the risk that the merge will introduce conflicts to the PFML API's database schema. This scenario is not uncommon, especially for branches that introduce new database migrations and that have not yet been updated with the latest changes to main. The API's test suite cannot detect these conflicts, so they have historically been discovered only after PR merge, when deployment of the API begins to fail. To mitigate this risk, any engineer who introduces new database migrations to the API should pay special attention to the \"Pre-Merge Conflict Check\", an automated check that should appear on any pull request that makes changes to api/massgov/pfml/db/migrations/versions . This special 'pseudo-check' will diff your branch against main, and fail if it detects that your branch is missing any database migrations that are already present on main. To mitigate the risk that updates to main will, occasionally, introduce new database conflicts to unmerged branches, this pseudo-check will automatically re-run itself whenever a new database migration is pushed to main, and additionally whenever new commits are pushed to your PR's feature branch. For these reasons, please make sure you check the status of any database conflicts before merging an approved PR. To avoid these conflicts, any PR that alters the database must be updated with the latest changes to main before being merged.","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#delivery-workflow","text":"Please see the Delivery Workflow Confluence page for guidance on how to pick up tickets, when to deploy, which environment to deploy to, etc.","title":"Delivery Workflow"},{"location":"contributing/#local-development","text":"For practical instructions on getting set up locally, see the repo README . Below are guidelines for working once you're ready. Commit in your own branch, and include your name in the branch name, e.g. lorenyu/pfml-123-new-feature . Try to keep commits small and focused on one thing. If you find yourself using \u201cand\u201d and \u201calso\u201d in your title or description, that may be a sign to break things up into smaller chunks. You can selectively git add files or use the interactive option: git add -p . Note that after a PR is merged, your commit message history will be squashed and rewritten. Your local commit history will persist on the PR's page unless you force-push a squashed version of the branch. Informative commit messages can help when context-switching between branches, or for PR clarity in larger PRs. See Seven Rules for an external reference on commit message best practices. To keep your branch updated with others' changes, you can: Rebase your changes onto main with git rebase main , or Merge main onto your own branch: git merge main . The favored pathway to keep your branch updated is to use git merge main. It's still possible to rebase and force-push your branch using git rebase main , but this pathway should be avoided if your branch is being worked on by more than one person at a time, due to the risk of causing unnecessary conflicts.","title":"Local Development"},{"location":"contributing/#code-reviews","text":"Code reviews are intended to help all of us grow as engineers and improve the quality of what we ship. These guidelines are meant to reinforce those two goals.","title":"Code Reviews"},{"location":"contributing/#for-authors-or-requesters","text":"Include the JIRA ticket number in the title. For example: PFML-123: Implement API endpoint . A single JIRA ticket may be associated with multiple pull requests. Every change should have an associated JIRA ticket unless it is a documentation change. This makes it easier to search for PRs and generates a link in the Jira ticket to the pull request, which provides the program with an audit trail for verifying the relevant code changes, approvals, and test cases. For tickets that have subtasks, use the parent ticket's id in the title, and include the subtask in the PR description. If it is a documentation change, please prefix with docs: docs: Clean up CONTRIBUTING.md . Include a GitHub label if relevant. These labels can be a helpful wayfinding and context building tool, especially for new engineers onboarding into the codebase. The architecture label should be used for changes to abstractions or interfaces that usually affect multiple existing files. The pattern label should be used for a new structure or interface that will be reused later. The operations label should be used for changes to engineering operations, such as dev set-up or deployment. If your PR is a work-in-progress, or if you are looking for specific feedback on things, create a Draft Pull Request and state what you are looking for in the description. Your PR should be small enough that a reviewer can reasonably respond within 1-2 business days. For larger changes, break them down into a series of PRs. If refactors are included in your changes, try to split them out as recommended below. As a PR writer, you should consider your description and comments as documentation; current and future team members will refer to it to understand your design decisions. Include relevant context and business requirements, and add preemptive comments (in code or PR) for sections of code that may be confusing or worth debate. When you're ready to request a review, consider including folks who are less familiar with that part of the codebase. This allows others to see what you're working on, along with your development/communication style and ways of working. You may also reference a specific team for review using the teams defined in Github Teams and Codeowners . Once you've received feedback, acknowledge each comment with a response or code change.","title":"For authors or requesters:"},{"location":"contributing/#for-reviewers","text":"Aim to respond to code reviews within 1 business day. Remember to highlight things that you like and appreciate while reading through the changes, and to make any other feedback clearly actionable by indicating if it is optional preference, an important consideration, or an error. Don't be afraid to comment with a question, or to ask for clarification, or provide a suggestion, whenever you don\u2019t understand what is going on at first glance \u2014 or if you think an approach or decision can be improved. Code reviews give us a chance to learn from one another, and to reflect, iterate on, and document why certain decisions are made. Once you're ready to approve or request changes, err on the side of trust. Send a vote of approval if the PR looks ready except for small minor changes, and trust that the recipient will address your comments before merging by replying via comment or code to any asks. Use \"request changes\" sparingly, unless there's a blocking issue or major refactors that should be done.","title":"For reviewers:"},{"location":"contributing/#merging-a-pr","text":"","title":"Merging a PR"},{"location":"contributing/#how-to-merge","text":"When merging a PR, use the PR description as a starting point . Copy it into the merge commit body and clean it up as desired. Please verify that the commit title includes the JIRA ticket. If the PR only has a single commit, GitHub will default to using that commit subject and description. This is often incorrect, missing the JIRA ticket. Example to follow: PFML-540: Implement GET claims endpoint (#145) https://lwd.atlassian.net/browse/PFML-540 Implements /claims endpoint which is a GET that takes a valid user ID and returns their first claim from the database. Demo: Unit tests and adhoc testing via swagger UI. Example to avoid: add claims endpoint (#145) * formatting * fix * change endpoint * initial endpoint","title":"How to merge"},{"location":"contributing/#when-to-merge","text":"A PR should only be merged when all its automated checks have returned green and when the PR has received at least one approval. Although GitHub does not block the merge of a PR that fails its checks, any failed checks should be treated as a blocking issue and resolved before the PR is merged.","title":"When to merge"},{"location":"contributing/#merges-the-api-database","text":"A particular source of danger when merging a PR is the risk that the merge will introduce conflicts to the PFML API's database schema. This scenario is not uncommon, especially for branches that introduce new database migrations and that have not yet been updated with the latest changes to main. The API's test suite cannot detect these conflicts, so they have historically been discovered only after PR merge, when deployment of the API begins to fail. To mitigate this risk, any engineer who introduces new database migrations to the API should pay special attention to the \"Pre-Merge Conflict Check\", an automated check that should appear on any pull request that makes changes to api/massgov/pfml/db/migrations/versions . This special 'pseudo-check' will diff your branch against main, and fail if it detects that your branch is missing any database migrations that are already present on main. To mitigate the risk that updates to main will, occasionally, introduce new database conflicts to unmerged branches, this pseudo-check will automatically re-run itself whenever a new database migration is pushed to main, and additionally whenever new commits are pushed to your PR's feature branch. For these reasons, please make sure you check the status of any database conflicts before merging an approved PR. To avoid these conflicts, any PR that alters the database must be updated with the latest changes to main before being merged.","title":"Merges &amp; the API database"},{"location":"creating-environments/","text":"Setting up a new environment Full E2E process (SOP): https://lwd.atlassian.net/wiki/spaces/DD/pages/1549860868/SOP+New+Environment+Build The easiest way to set up resources in a new environment is using the templates in /bin/bootstrap-env/ . 1. Initial Setup Update the infra/constants/outputs.tf file, specifically: environment_tags environment_shorthand smartronix_environment_tags You do not need to update domains, api_domains, or cert_domains at this time. For smartronix_environment_tags, reach out to Ron Lovell, Karen Grant, Andy Rodriguez, and Chris Griffith and ask what the new tag should be based on the environment name. They will kick off the Smartronix CAMS infra-monitoring harness for the new environment as well as start any manual DBA steps for the database. If you need a new EOTSS-approved tag for environment_tags, send an email to Tim.L.Sharpe@mass.gov requesting a new tag. The S3 bucket for holding tfstate files must be created first in infra/pfml-aws/s3.tf . Run terraform apply . This may require admin permissions. As well as the replica bucket in LWD account. Send a request to EOTSS (Roy Mounier, cc. Chris Griffith) to update the NonProd Admin role to include arn:aws:s3:::*-pfml-NEW_ENV* and arn:aws:s3:::*-pfml-NEW_ENV-*/* in the S3 permissions. If Roy is out, create a Service Now request. Search for \"AWS Role Access Request/Adjustment\" Select \"Cloud Engineer\" if needed by the form. Select \"EOLWD Core\" for account. Select \"No\" for \"Member of Cloud Operations Team.\" In the justification, request the following: \"Request for the PFML Account (498823821309). The Nonprod-Admin role should allow access to read from new S3 buckets created for the NEW_ENV environment that is being stood up.\" After that, individual terraform components ( env-shared , api , ecs-tasks , or portal ) may be set up. We'll use this pattern: pfml$ bin/bootstrap-env/bootstrap-env.sh <new-env> <component> 2. Set up the API Request Domain certificates for new domain names Refer to the end of this document ## Setting up Custom Domains 2.0 Create the API Gateway Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV env-shared The command above creates a new directory: infra/env-shared/environments/NEW_ENV From the new directory, update variables in main.tf and run: terraform init Create the environment: terraform plan terraform apply 2.1 Create the API Manually create /service/pfml-api-dor-import/NEW_ENV/gpg_decryption_key in Parameter Store. Copy the value from TEST and use a SecureString on the Advanced Tier. Setup the rest of the secrets in Parameter Store by running ./bin/bootstrap-env/copy-parameters.sh NEW_ENV Known and accounted for at time of writing: Copy from stage: - /service/pfml-api-comptroller/NEW_ENV/eolwd-moveit-ssh-key - /service/pfml-api-comptroller/NEW_ENV/eolwd-moveit-ssh-key-password - /service/pfml-api-dor-import/NEW_ENV/gpg_decryption_key_passphrase - /service/pfml-api/NEW_ENV/ctr-data-mart-password - /service/pfml-api/NEW_ENV/rmv_client_certificate_password - /service/pfml-api/NEW_ENV/service_now_username - /service/pfml-api/NEW_ENV/service_now_password Manually generate a string and pop it in there: - /service/pfml-api/NEW_ENV/dashboard_password - /service/pfml-api/NEW_ENV/db-nessus-password Generated automaticallly in terraform (no action needed): - /service/pfml-api/NEW_ENV/db-password Copy from `Cognito AWS Console > app clients` after Portal/Cognito is created: - /service/pfml-api/NEW_ENV/cognito_fineos_app_client_id - /service/pfml-api/NEW_ENV/cognito_internal_fineos_role_app_client_id - /service/pfml-api/NEW_ENV/cognito_internal_fineos_role_app_client_secret Received from FINEOS (more details below): - /service/pfml-api/NEW_ENV/fineos_oauth2_client_secret Add the new environment to api/newrelic.ini to set up New Relic APM: [newrelic:NEW_ENV] app_name = PFML-API-NEW_ENV monitor_mode = true Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV api The command above creates a new directory: infra/api/environments/NEW_ENV From the new directory, update variables in main.tf , namely: nlb_port: same port as specified in the API gateway. cors_origins: set this to be the execute-api URL from API gateway for now. [API_GATEWAY_URL] rmv_client_certificate_arn: Retrieve the secretsmanager ARN from AWS secrets manager or from the copy-parameters script output. Run: terraform init Create the environment, providing an initial application version to deploy. Since this requires a version that is built and pushed to ECR, it's easiest to use the latest version that was deployed in TEST . This is a Docker image tag that is equivalent to a commit hash. $ terraform apply \\ -var = 'service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e' Note that the API will not be working until database migrations are run. 2.2 Create the ECS Tasks Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV ecs-tasks The command above creates a new directory: infra/ecs-tasks/environments/NEW_ENV rename step_functions.tf to step_functions.bak In ecs-tasks/environments/NEW_ENV/main.tf add rmv_client_certificate_binary_arn = '${ARN VALUE FROM SECRETS MANAGER}' Create a PR and run a deployment of this branch to the new environment. See ./deployment.md . This should automatically: Initialize and apply the ECS tasks terraform. Run database migrations. Create the required database users for FINEOS and Smartronix Nessus scans. rename step_functions.bak to step_functions.tf sh $ terraform plan \\ -var='service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e' sh $ terraform apply \\ -var='service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e' 2.3 API pre-work 1.Need to update the bootstrap script to omit these and add some others: - ecs-tasks/main.tf: - st_employer_update_limit = 1500 needs to be added - api/envirnment/$env/:main.tf: - st_use_mock_dor_data needs to be removed - st_decrypt_dor_data needs to be removed - st_file_limit_specified needs to be removed - dor_fineos_etl_schedule_expression needs to be removed - rmv_client_certificate is binary and has to be replicated using cli prior to running terraform 3. Setting up the Portal Environment Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV portal The command above creates a new directory: infra/portal/environments/NEW_ENV 1. From the new directory, run: terraform init Create the environment, providing a specific cloudfront_origin_path since nothing will be deployed yet. You can use a blank path for now: terraform plan -var = 'cloudfront_origin_path=' terraform apply -var = 'cloudfront_origin_path=' Sync the cognito secrets to parameter store: bin/bootstrap-env/sync-cognito-variables.sh $NEW_ENV Update infra/api/environments/NEW_ENV/main.tf and infra/ecs-tasks/environments/NEW_ENV/main.tf to include the cognito variables as output from the previous script. cognito_user_pool_arn = \"arn:aws:cognito-idp:us-east-1:498823821309:userpool/$pool_id\" cognito_user_pool_id = \"$pool_id\" cognito_user_pool_client_id = \"$api_client_id\" cognito_user_pool_keys_url = \"https://cognito-idp.us-east-1.amazonaws.com/$pool_id/.well-known/jwks.json\" Follow additional steps for new Portal environments . Configuring Cognito This is only required for environments that will not run E2E tests. For most environments this is not required. In production, we block high-risk login attempts. This can also be configured for other environments if desired. High-risk login attempts aren't blocked in environments used for testing so that automated test scripts don't get blocked. Our Terraform scripts enable Advanced Security. however at the time of writing, Terraform didn't support more granular configuration of the Advanced Security settings , so there are some manual steps needed: Log into the AWS Console and navigate to the Cognito User Pool for this environment. Click \"Advanced Security\" in the sidebar Configure the adaptive authentication behavior . On the same page in AWS customize the email notification messages by copy-and-pasting the HTML email templates from infra/portal/templates/emails . Ensure the email FROM address is \"NEWENV-SHORTHAND_Department of Family and Medical Leave\" \\<PFML_DoNotReply@eol.mass.gov\\>\" . 4. Integrating with FINEOS Configure PFML credentials for FINEOS Create the FINEOS user in the PFML database. This depends on the cognito_fineos_app_client_id secret being configured in Parameter Store from step 2.2, as part of API setup. ./bin/run-ecs-task/run-task.sh NEW_ENV db-create-fineos-user FIRST_NAME.LAST_NAME Create a file locally with the Cognito FINEOS app client ID and secret. You can view this data by going to AWS Console > AWS Cognito > User Pools > [[ select the environment's user pool ]] > App Clients. Look for the fineos-pfml-{env} client. Send it to FINEOS over interchange, then delete the file from your local computer. Also send the following details: Auth URL: (e.g. https://paidleave-api-cps-preview.eol.mass.gov/api/v1/oauth2/token) Integration URLs (e.g. https://paidleave-api-cps-preview.eol.mass.gov/api/v1/rmv-check, https://paidleave-api-cps-preview.eol.mass.gov/api/v1/financial-eligibility) VPC IP Address (Should be nonprod IP: VPC-EOLWD-PFML-NonProd, vpc-097793bd468be1c65, 10.203.224.0/22) VPC ID (Should be nonprod IP: VPC-EOLWD-PFML-NonProd, vpc-097793bd468be1c65, 10.203.224.0/22) VPC S3 Endpoint ID (Should be nonprod endpoint: vpce-057c4dd758ba5ccb1) Configure FINEOS credentials for PFML FINEOS should send a file to us with an oauth client ID and secret. If not, ping Howard Teasley and Darnel. The client ID should be configured in both : infra/api/environments/NEW_ENV/main.tf and infra/ecs-tasks/environments/NEW_ENV/main.tf. The client secret value should be updated in parameter store ( /service/pfml-api/NEW_ENV/fineos_oauth2_client_secret ). Verify the following details with FINEOS: - Will SSO be enabled? (If so, we need to configure the FINEOS user as OASIS instead of CONTENT) - What is the FINEOS URL? - What is the FINEOS S3 path for data exports/imports? 4. Update CI and Monitoring Add the new environment to the CI build matrix so it can be validated when it's changed. Add the environment to the monitoring module to create API and Portal alarms that are linked to PagerDuty. Setting up Custom Domains If we expect FINEOS to call the PFML API, you'll need a custom mass.gov domain for the environment. Please follow these steps: Request an ACM cert in the AWS Console > AWS Certificate Manager, with Email Verification. Example: Domain Name SANs paidleave-performance.eol.mass.gov paidleave-training.eol.mass.gov, paidleave-api-training.eol.mass.gov, paidleave-api-performance.eol.mass.gov This requires an individual with cloudops/administrator permissions. Send an email to Vijay with the domain name/SANs, so he can request approval from Sarah Bourne / Chris Velluto / Bill Cole. Boilerplate: To: Rajagopalan, Vijay (DFML) <Vijay.Rajagopalan2@mass.gov>; Griffith, Christopher (EOL) <Christopher.Griffith@detma.org>; Bourne, Sarah (EOTSS) <sarah.bourne@mass.gov>; Velluto, Christopher (EOTSS) <christopher.velluto@mass.gov>; Cole, William (EOTSS) <william.cole@mass.gov> cc: Yeh, Kevin (DFML) <Kevin.Yeh@mass.gov> Hi all, We\u2019re setting up the PFML API and Portal breakfix and cps-preview environments to interface with new FINEOS environments. I\u2019ll be requesting an ACM certificate shortly. The certificate request will be as outlined below: domain_name: paidleave-ENV.eol.mass.gov SANs: paidleave-api-ENV.eol.mass.gov, paidleave-OTHER_ENV.eol.mass.gov, paidleave-api-OTHER_ENV.eol.mass.gov @Rajagopalan, Vijay (DFML) @Griffth, Chris (EOL) can you confirm this request from the PFML end? Thanks. Once EOTSS has approved the certificate, update the domains, api_domains, and cert_domains map in the following file: infra/constants/outputs.tf After merging and applying the changes, create a ServiceNow request for CNAME entries for the Portal and Gateway. These should all be cloudfront URLs. Check the AWS Console under API Gateway > Custom Domain Names for the API Gateway Cloudfront URLs. App CNAME URL API perf paidleave-api-performance.eol.mass.gov https://abcd123.cloudfront.net API training paidleave-api-training.eol.mass.gov https://zaww123.cloudfront.net Portal perf paidleave-performance.eol.mass.gov https://vfcs123.cloudfront.net Portal training paidleave-training.eol.mass.gov https://qwer123.cloudfront.net Visit https://massgov.service-now.com/sp and search for DNS (Domain Name System). Create one ticket per record. Fill details in as follows (example): Example Description: Please create a new CName entry to point paidleave-api-breakfix.eol.mass.gov to djoikhjp9s25o.cloudfront.net. Point of Contacts for more info: Rajagopalan, Vijay (DFML) <Vijay.Rajagopalan2@mass.gov> Griffith, Christopher (EOL) <Christopher.Griffith@detma.org> After they create the CNAME entries, the custom domains should direct to the appropriate applications.","title":"Setting up a new environment"},{"location":"creating-environments/#setting-up-a-new-environment","text":"Full E2E process (SOP): https://lwd.atlassian.net/wiki/spaces/DD/pages/1549860868/SOP+New+Environment+Build The easiest way to set up resources in a new environment is using the templates in /bin/bootstrap-env/ .","title":"Setting up a new environment"},{"location":"creating-environments/#1-initial-setup","text":"Update the infra/constants/outputs.tf file, specifically: environment_tags environment_shorthand smartronix_environment_tags You do not need to update domains, api_domains, or cert_domains at this time. For smartronix_environment_tags, reach out to Ron Lovell, Karen Grant, Andy Rodriguez, and Chris Griffith and ask what the new tag should be based on the environment name. They will kick off the Smartronix CAMS infra-monitoring harness for the new environment as well as start any manual DBA steps for the database. If you need a new EOTSS-approved tag for environment_tags, send an email to Tim.L.Sharpe@mass.gov requesting a new tag. The S3 bucket for holding tfstate files must be created first in infra/pfml-aws/s3.tf . Run terraform apply . This may require admin permissions. As well as the replica bucket in LWD account. Send a request to EOTSS (Roy Mounier, cc. Chris Griffith) to update the NonProd Admin role to include arn:aws:s3:::*-pfml-NEW_ENV* and arn:aws:s3:::*-pfml-NEW_ENV-*/* in the S3 permissions. If Roy is out, create a Service Now request. Search for \"AWS Role Access Request/Adjustment\" Select \"Cloud Engineer\" if needed by the form. Select \"EOLWD Core\" for account. Select \"No\" for \"Member of Cloud Operations Team.\" In the justification, request the following: \"Request for the PFML Account (498823821309). The Nonprod-Admin role should allow access to read from new S3 buckets created for the NEW_ENV environment that is being stood up.\" After that, individual terraform components ( env-shared , api , ecs-tasks , or portal ) may be set up. We'll use this pattern: pfml$ bin/bootstrap-env/bootstrap-env.sh <new-env> <component>","title":"1. Initial Setup"},{"location":"creating-environments/#2-set-up-the-api","text":"Request Domain certificates for new domain names Refer to the end of this document ## Setting up Custom Domains","title":"2. Set up the API"},{"location":"creating-environments/#20-create-the-api-gateway","text":"Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV env-shared The command above creates a new directory: infra/env-shared/environments/NEW_ENV From the new directory, update variables in main.tf and run: terraform init Create the environment: terraform plan terraform apply","title":"2.0 Create the API Gateway"},{"location":"creating-environments/#21-create-the-api","text":"Manually create /service/pfml-api-dor-import/NEW_ENV/gpg_decryption_key in Parameter Store. Copy the value from TEST and use a SecureString on the Advanced Tier. Setup the rest of the secrets in Parameter Store by running ./bin/bootstrap-env/copy-parameters.sh NEW_ENV Known and accounted for at time of writing: Copy from stage: - /service/pfml-api-comptroller/NEW_ENV/eolwd-moveit-ssh-key - /service/pfml-api-comptroller/NEW_ENV/eolwd-moveit-ssh-key-password - /service/pfml-api-dor-import/NEW_ENV/gpg_decryption_key_passphrase - /service/pfml-api/NEW_ENV/ctr-data-mart-password - /service/pfml-api/NEW_ENV/rmv_client_certificate_password - /service/pfml-api/NEW_ENV/service_now_username - /service/pfml-api/NEW_ENV/service_now_password Manually generate a string and pop it in there: - /service/pfml-api/NEW_ENV/dashboard_password - /service/pfml-api/NEW_ENV/db-nessus-password Generated automaticallly in terraform (no action needed): - /service/pfml-api/NEW_ENV/db-password Copy from `Cognito AWS Console > app clients` after Portal/Cognito is created: - /service/pfml-api/NEW_ENV/cognito_fineos_app_client_id - /service/pfml-api/NEW_ENV/cognito_internal_fineos_role_app_client_id - /service/pfml-api/NEW_ENV/cognito_internal_fineos_role_app_client_secret Received from FINEOS (more details below): - /service/pfml-api/NEW_ENV/fineos_oauth2_client_secret Add the new environment to api/newrelic.ini to set up New Relic APM: [newrelic:NEW_ENV] app_name = PFML-API-NEW_ENV monitor_mode = true Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV api The command above creates a new directory: infra/api/environments/NEW_ENV From the new directory, update variables in main.tf , namely: nlb_port: same port as specified in the API gateway. cors_origins: set this to be the execute-api URL from API gateway for now. [API_GATEWAY_URL] rmv_client_certificate_arn: Retrieve the secretsmanager ARN from AWS secrets manager or from the copy-parameters script output. Run: terraform init Create the environment, providing an initial application version to deploy. Since this requires a version that is built and pushed to ECR, it's easiest to use the latest version that was deployed in TEST . This is a Docker image tag that is equivalent to a commit hash. $ terraform apply \\ -var = 'service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e' Note that the API will not be working until database migrations are run.","title":"2.1 Create the API"},{"location":"creating-environments/#22-create-the-ecs-tasks","text":"Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV ecs-tasks The command above creates a new directory: infra/ecs-tasks/environments/NEW_ENV rename step_functions.tf to step_functions.bak In ecs-tasks/environments/NEW_ENV/main.tf add rmv_client_certificate_binary_arn = '${ARN VALUE FROM SECRETS MANAGER}' Create a PR and run a deployment of this branch to the new environment. See ./deployment.md . This should automatically: Initialize and apply the ECS tasks terraform. Run database migrations. Create the required database users for FINEOS and Smartronix Nessus scans. rename step_functions.bak to step_functions.tf sh $ terraform plan \\ -var='service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e' sh $ terraform apply \\ -var='service_docker_tag=82043ae1e04621251fb9e7896176b38c884f027e'","title":"2.2 Create the ECS Tasks"},{"location":"creating-environments/#23-api-pre-work","text":"1.Need to update the bootstrap script to omit these and add some others: - ecs-tasks/main.tf: - st_employer_update_limit = 1500 needs to be added - api/envirnment/$env/:main.tf: - st_use_mock_dor_data needs to be removed - st_decrypt_dor_data needs to be removed - st_file_limit_specified needs to be removed - dor_fineos_etl_schedule_expression needs to be removed - rmv_client_certificate is binary and has to be replicated using cli prior to running terraform","title":"2.3 API pre-work"},{"location":"creating-environments/#3-setting-up-the-portal-environment","text":"Generate the terraform environment folder: pfml$ bin/bootstrap-env/bootstrap-env.sh NEW_ENV portal The command above creates a new directory: infra/portal/environments/NEW_ENV 1. From the new directory, run: terraform init Create the environment, providing a specific cloudfront_origin_path since nothing will be deployed yet. You can use a blank path for now: terraform plan -var = 'cloudfront_origin_path=' terraform apply -var = 'cloudfront_origin_path=' Sync the cognito secrets to parameter store: bin/bootstrap-env/sync-cognito-variables.sh $NEW_ENV Update infra/api/environments/NEW_ENV/main.tf and infra/ecs-tasks/environments/NEW_ENV/main.tf to include the cognito variables as output from the previous script. cognito_user_pool_arn = \"arn:aws:cognito-idp:us-east-1:498823821309:userpool/$pool_id\" cognito_user_pool_id = \"$pool_id\" cognito_user_pool_client_id = \"$api_client_id\" cognito_user_pool_keys_url = \"https://cognito-idp.us-east-1.amazonaws.com/$pool_id/.well-known/jwks.json\" Follow additional steps for new Portal environments .","title":"3. Setting up the Portal Environment"},{"location":"creating-environments/#configuring-cognito","text":"This is only required for environments that will not run E2E tests. For most environments this is not required. In production, we block high-risk login attempts. This can also be configured for other environments if desired. High-risk login attempts aren't blocked in environments used for testing so that automated test scripts don't get blocked. Our Terraform scripts enable Advanced Security. however at the time of writing, Terraform didn't support more granular configuration of the Advanced Security settings , so there are some manual steps needed: Log into the AWS Console and navigate to the Cognito User Pool for this environment. Click \"Advanced Security\" in the sidebar Configure the adaptive authentication behavior . On the same page in AWS customize the email notification messages by copy-and-pasting the HTML email templates from infra/portal/templates/emails . Ensure the email FROM address is \"NEWENV-SHORTHAND_Department of Family and Medical Leave\" \\<PFML_DoNotReply@eol.mass.gov\\>\" .","title":"Configuring Cognito"},{"location":"creating-environments/#4-integrating-with-fineos","text":"","title":"4. Integrating with FINEOS"},{"location":"creating-environments/#configure-pfml-credentials-for-fineos","text":"Create the FINEOS user in the PFML database. This depends on the cognito_fineos_app_client_id secret being configured in Parameter Store from step 2.2, as part of API setup. ./bin/run-ecs-task/run-task.sh NEW_ENV db-create-fineos-user FIRST_NAME.LAST_NAME Create a file locally with the Cognito FINEOS app client ID and secret. You can view this data by going to AWS Console > AWS Cognito > User Pools > [[ select the environment's user pool ]] > App Clients. Look for the fineos-pfml-{env} client. Send it to FINEOS over interchange, then delete the file from your local computer. Also send the following details: Auth URL: (e.g. https://paidleave-api-cps-preview.eol.mass.gov/api/v1/oauth2/token) Integration URLs (e.g. https://paidleave-api-cps-preview.eol.mass.gov/api/v1/rmv-check, https://paidleave-api-cps-preview.eol.mass.gov/api/v1/financial-eligibility) VPC IP Address (Should be nonprod IP: VPC-EOLWD-PFML-NonProd, vpc-097793bd468be1c65, 10.203.224.0/22) VPC ID (Should be nonprod IP: VPC-EOLWD-PFML-NonProd, vpc-097793bd468be1c65, 10.203.224.0/22) VPC S3 Endpoint ID (Should be nonprod endpoint: vpce-057c4dd758ba5ccb1)","title":"Configure PFML credentials for FINEOS"},{"location":"creating-environments/#configure-fineos-credentials-for-pfml","text":"FINEOS should send a file to us with an oauth client ID and secret. If not, ping Howard Teasley and Darnel. The client ID should be configured in both : infra/api/environments/NEW_ENV/main.tf and infra/ecs-tasks/environments/NEW_ENV/main.tf. The client secret value should be updated in parameter store ( /service/pfml-api/NEW_ENV/fineos_oauth2_client_secret ). Verify the following details with FINEOS: - Will SSO be enabled? (If so, we need to configure the FINEOS user as OASIS instead of CONTENT) - What is the FINEOS URL? - What is the FINEOS S3 path for data exports/imports?","title":"Configure FINEOS credentials for PFML"},{"location":"creating-environments/#4-update-ci-and-monitoring","text":"Add the new environment to the CI build matrix so it can be validated when it's changed. Add the environment to the monitoring module to create API and Portal alarms that are linked to PagerDuty.","title":"4. Update CI and Monitoring"},{"location":"creating-environments/#setting-up-custom-domains","text":"If we expect FINEOS to call the PFML API, you'll need a custom mass.gov domain for the environment. Please follow these steps: Request an ACM cert in the AWS Console > AWS Certificate Manager, with Email Verification. Example: Domain Name SANs paidleave-performance.eol.mass.gov paidleave-training.eol.mass.gov, paidleave-api-training.eol.mass.gov, paidleave-api-performance.eol.mass.gov This requires an individual with cloudops/administrator permissions. Send an email to Vijay with the domain name/SANs, so he can request approval from Sarah Bourne / Chris Velluto / Bill Cole. Boilerplate: To: Rajagopalan, Vijay (DFML) <Vijay.Rajagopalan2@mass.gov>; Griffith, Christopher (EOL) <Christopher.Griffith@detma.org>; Bourne, Sarah (EOTSS) <sarah.bourne@mass.gov>; Velluto, Christopher (EOTSS) <christopher.velluto@mass.gov>; Cole, William (EOTSS) <william.cole@mass.gov> cc: Yeh, Kevin (DFML) <Kevin.Yeh@mass.gov> Hi all, We\u2019re setting up the PFML API and Portal breakfix and cps-preview environments to interface with new FINEOS environments. I\u2019ll be requesting an ACM certificate shortly. The certificate request will be as outlined below: domain_name: paidleave-ENV.eol.mass.gov SANs: paidleave-api-ENV.eol.mass.gov, paidleave-OTHER_ENV.eol.mass.gov, paidleave-api-OTHER_ENV.eol.mass.gov @Rajagopalan, Vijay (DFML) @Griffth, Chris (EOL) can you confirm this request from the PFML end? Thanks. Once EOTSS has approved the certificate, update the domains, api_domains, and cert_domains map in the following file: infra/constants/outputs.tf After merging and applying the changes, create a ServiceNow request for CNAME entries for the Portal and Gateway. These should all be cloudfront URLs. Check the AWS Console under API Gateway > Custom Domain Names for the API Gateway Cloudfront URLs. App CNAME URL API perf paidleave-api-performance.eol.mass.gov https://abcd123.cloudfront.net API training paidleave-api-training.eol.mass.gov https://zaww123.cloudfront.net Portal perf paidleave-performance.eol.mass.gov https://vfcs123.cloudfront.net Portal training paidleave-training.eol.mass.gov https://qwer123.cloudfront.net Visit https://massgov.service-now.com/sp and search for DNS (Domain Name System). Create one ticket per record. Fill details in as follows (example): Example Description: Please create a new CName entry to point paidleave-api-breakfix.eol.mass.gov to djoikhjp9s25o.cloudfront.net. Point of Contacts for more info: Rajagopalan, Vijay (DFML) <Vijay.Rajagopalan2@mass.gov> Griffith, Christopher (EOL) <Christopher.Griffith@detma.org> After they create the CNAME entries, the custom domains should direct to the appropriate applications.","title":"Setting up Custom Domains"},{"location":"deployment/","text":"Deployment Deployments to all API and Portal environments are managed through GitHub Actions. The main branch is automatically deployed to test as pull requests are merged into it. Deployments to other environmments are triggered manually. Changes to the following components take effect immediately in all environments once they go into main : Alerts and Pagerduty configuration in infra/monitoring Feature flags managed in the feature_flags folder Github action workflows, as defined in .github/workflows . Deploying to an API or Portal environment Visit the API or Portal Github Actions homepage: API Deploy Portal Deploy Click the \"Run workflow\" button, and fill in the inputs: Provide the deployment_env from the list of environments. Provide a git branch or tag to deploy. This can be: the name of a git tag, e.g. api/v1.7.0-rc1 , or the name of a git branch, like release/api/v1.7.0 or kev/my-feature-branch . Click the green \"Run workflow\" button. Test Environment Coordination If you are testing a feature branch on the test environment, please go through the following additional steps: During Testing Communicate to the Deployments channel in Teams: \"\u26a0\ufe0f I'll be using the API/Portal test environment soon, please let me know if you have any concerns.\" After running the workflow, click the \"Disable Workflow\" button to prevent auto-deploys from overriding your deployment. After Testing Click the \"Enable Workflow\" button. Re-deploy main to test. Notify the Teams channel. Branch-to-environment mapping At a quick glance, you can view the commit history for any environment based on the branch. Name of API deploy branch Name of Portal deploy branch Corresponding env main main test deploy/api/stage deploy/portal/stage stage deploy/api/prod deploy/portal/prod prod deploy/api/performance deploy/portal/performance performance deploy/api/training deploy/portal/training training deploy/api/uat deploy/portal/uat uat","title":"Deployment"},{"location":"deployment/#deployment","text":"Deployments to all API and Portal environments are managed through GitHub Actions. The main branch is automatically deployed to test as pull requests are merged into it. Deployments to other environmments are triggered manually. Changes to the following components take effect immediately in all environments once they go into main : Alerts and Pagerduty configuration in infra/monitoring Feature flags managed in the feature_flags folder Github action workflows, as defined in .github/workflows .","title":"Deployment"},{"location":"deployment/#deploying-to-an-api-or-portal-environment","text":"Visit the API or Portal Github Actions homepage: API Deploy Portal Deploy Click the \"Run workflow\" button, and fill in the inputs: Provide the deployment_env from the list of environments. Provide a git branch or tag to deploy. This can be: the name of a git tag, e.g. api/v1.7.0-rc1 , or the name of a git branch, like release/api/v1.7.0 or kev/my-feature-branch . Click the green \"Run workflow\" button.","title":"Deploying to an API or Portal environment"},{"location":"deployment/#test-environment-coordination","text":"If you are testing a feature branch on the test environment, please go through the following additional steps:","title":"Test Environment Coordination"},{"location":"deployment/#during-testing","text":"Communicate to the Deployments channel in Teams: \"\u26a0\ufe0f I'll be using the API/Portal test environment soon, please let me know if you have any concerns.\" After running the workflow, click the \"Disable Workflow\" button to prevent auto-deploys from overriding your deployment.","title":"During Testing"},{"location":"deployment/#after-testing","text":"Click the \"Enable Workflow\" button. Re-deploy main to test. Notify the Teams channel.","title":"After Testing"},{"location":"deployment/#branch-to-environment-mapping","text":"At a quick glance, you can view the commit history for any environment based on the branch. Name of API deploy branch Name of Portal deploy branch Corresponding env main main test deploy/api/stage deploy/portal/stage stage deploy/api/prod deploy/portal/prod prod deploy/api/performance deploy/portal/performance performance deploy/api/training deploy/portal/training training deploy/api/uat deploy/portal/uat uat","title":"Branch-to-environment mapping"},{"location":"github-teams-and-codeowners/","text":"Github Teams and Code Owners Code Owners Many parts of the code have owners, defined in the CODEOWNERS file . Code owners help to improve the quality of the code by: Reviewing changes (see below) Encouraging consistency and maintainability Refactoring the code as it grows Normally each module or file has two or more owners that share these tasks. Auto-Assignment Code owners will be automatically assigned for review when the relevant code paths are edited in a pull request. Github Teams We use GitHub teams to make it easier to request reviews and specify code owners. Teams exist for select disciplines: API engineers: @pfml-api Portal engineers: @pfml-portal Infra engineers: @pfml-infra E2E engineers: @pfml-e2e Additionally, certain scrum teams have their own Github team: @pfml-acorn-street @pfml-beacon-hill @pfml-dorchester @pfml-newbury Some teams are configured so that PR review requests are automatically assigned to a member of the team via a round robin algorithm, with the goal of equally distributing PR reviews across the team.","title":"Github Teams and Code Owners"},{"location":"github-teams-and-codeowners/#github-teams-and-code-owners","text":"","title":"Github Teams and Code Owners"},{"location":"github-teams-and-codeowners/#code-owners","text":"Many parts of the code have owners, defined in the CODEOWNERS file . Code owners help to improve the quality of the code by: Reviewing changes (see below) Encouraging consistency and maintainability Refactoring the code as it grows Normally each module or file has two or more owners that share these tasks.","title":"Code Owners"},{"location":"github-teams-and-codeowners/#auto-assignment","text":"Code owners will be automatically assigned for review when the relevant code paths are edited in a pull request.","title":"Auto-Assignment"},{"location":"github-teams-and-codeowners/#github-teams","text":"We use GitHub teams to make it easier to request reviews and specify code owners. Teams exist for select disciplines: API engineers: @pfml-api Portal engineers: @pfml-portal Infra engineers: @pfml-infra E2E engineers: @pfml-e2e Additionally, certain scrum teams have their own Github team: @pfml-acorn-street @pfml-beacon-hill @pfml-dorchester @pfml-newbury Some teams are configured so that PR review requests are automatically assigned to a member of the team via a round robin algorithm, with the goal of equally distributing PR reviews across the team.","title":"Github Teams"},{"location":"test_fineos_from_local_setup/","text":"Test Fineos server from local setup This documentation shows how you can configure your local set up to interact with the FINEOS server of your choice. Test Fineos server from local setup 1. Set up env values 2. Local authentication 2.1 If you are using Local API only (use Swagger UI) 2.2 If you are using Local Portal to Local API Link Local Portal to Local API environment Login through a user account 3. Data restrictions when interacting with Fineos 3.1 If you are testing as a Claimant 3.2 If you are testing as a Leave Admin 3.2.1: Change current user account to a leave admin account 3.2.2: Verify current user as a leave admin for the claim's employer 1. Set up env values Decide which FINEOS environment you wish to use. Paste and replace the following values in your docker-compose.yml test - DT2 - FINEOS_CLIENT_CUSTOMER_API_URL=https://dt2-api.masspfml.fineos.com/customerapi/ - FINEOS_CLIENT_WSCOMPOSER_API_URL=https://dt2-api.masspfml.fineos.com/integration-services/wscomposer/ - FINEOS_CLIENT_GROUP_CLIENT_API_URL=https://dt2-api.masspfml.fineos.com/groupclientapi/ - FINEOS_CLIENT_INTEGRATION_SERVICES_API_URL=https://dt2-api.masspfml.fineos.com/integration-services/ - FINEOS_CLIENT_OAUTH2_URL=https://dt2-api.masspfml.fineos.com/oauth2/token - FINEOS_CLIENT_OAUTH2_CLIENT_ID=1ral5e957i0l9shul52bhk0037 // no need to replace the following if you are using Swagger UI - COGNITO_USER_POOL_ID=us-east-1_HhQSLYSIe - COGNITO_USER_POOL_CLIENT_ID=7sjb96tvg8251lrq5vdk7de9 - COGNITO_USER_POOL_KEYS_URL=https://cognito-idp.us-east-1.amazonaws.com/us-east-1_HhQSLYSIe/.well-known/jwks.json cps-preview - DT3 - FINEOS_CLIENT_CUSTOMER_API_URL=https://dt3-api.masspfml.fineos.com/customerapi/ - FINEOS_CLIENT_WSCOMPOSER_API_URL=https://dt3-api.masspfml.fineos.com/integration-services/wscomposer/ - FINEOS_CLIENT_GROUP_CLIENT_API_URL=https://dt3-api.masspfml.fineos.com/groupclientapi/ - FINEOS_CLIENT_INTEGRATION_SERVICES_API_URL=https://dt3-api.masspfml.fineos.com/integration-services/ - FINEOS_CLIENT_OAUTH2_URL=https://dt3-api.masspfml.fineos.com/oauth2/token - FINEOS_CLIENT_OAUTH2_CLIENT_ID=2gptm2870hlo9ouq70poib8d5g // no need to replace the following if you are using Swagger UI - COGNITO_USER_POOL_ID=us-east-1_1OVYp4aZo - COGNITO_USER_POOL_CLIENT_ID=59oeobfn0759c8166pjh381joc - COGNITO_USER_POOL_KEYS_URL=https://cognito-idp.us-east-1.amazonaws.com/us-east-1_1OVYp4aZo/.well-known/jwks.json stage - IDT - FINEOS_CLIENT_CUSTOMER_API_URL=https://idt-api.masspfml.fineos.com/customerapi/ - FINEOS_CLIENT_WSCOMPOSER_API_URL=https://idt-api.masspfml.fineos.com/integration-services/wscomposer/ - FINEOS_CLIENT_GROUP_CLIENT_API_URL=https://idt-api.masspfml.fineos.com/groupclientapi/ - FINEOS_CLIENT_INTEGRATION_SERVICES_API_URL=https://idt-api.masspfml.fineos.com/integration-services/ - FINEOS_CLIENT_OAUTH2_URL=https://idt-api.masspfml.fineos.com/oauth2/token - FINEOS_CLIENT_OAUTH2_CLIENT_ID=1fa281uto9tjuqtm21jle7loam // no need to replace the following if you are using Swagger UI - COGNITO_USER_POOL_ID=us-east-1_HpL4XslLg - COGNITO_USER_POOL_CLIENT_ID=10rjcp71r8bnk4459c67bn18t8 - COGNITO_USER_POOL_KEYS_URL=https://cognito-idp.us-east-1.amazonaws.com/us-east-1_HpL4XslLg/.well-known/jwks.json Paste FINEOS_CLIENT_OAUTH2_CLIENT_SECRET from 1password7: Fineos Client ID and Secrets These are also available in AWS SSM Parameter Store under /service/pfml-api/${env}/fineos_oauth2_client_secret - FINEOS_CLIENT_OAUTH2_CLIENT_SECRET=... 2. Local authentication 2.1 If you are using Local API only (use Swagger UI) Follow the instructions in /api/README.md 2.2 If you are using Local Portal to Local API Link Local Portal to Local API environment in portal/config use the same Cognito ID information you used in api/docker-compose.yml // development.js apiUrl: \"<http://localhost:1550/v1\"> // default.js cognitoUserPoolId: \"{COGNITO_USER_POOL_ID}\", cognitoUserPoolWebClientId: \"{COGNITO_USER_POOL_CLIENT_ID}\", More detailed explanation of how to setup local to local here Login through a user account Option 1: Create a new account through the local portal directly Option 2: Use an existing account from a deployed environment If there is a Portal account you'd like to use which has existing information in the deployed FINEOS environment, we'll need to replicate the account in our local database using the email address and Cognito sub_id. 1. Log into the deployed Portal website; e.g. if you are targeting Fineos DT3, then use https://paidleave-cps-preview.eol.mass.gov/. For other environments, see [Environment URLs](https://lwd.atlassian.net/wiki/x/2oBEF). 2. Open console and run following command to get `sub_id` userDataCookie = document . cookie . split ( '; ' ) . find ( cookie => cookie . match ( /^CognitoIdentityServiceProvider.*userData=/ )) . split ( \"=\" )[ 1 ]; JSON . parse ( decodeURIComponent ( userDataCookie ))[ \"UserAttributes\" ][ 0 ][ \"Value\" ] 3. Update user in local API DB - run `make create-user` if no user in your database - Use a SQL client or database shell(`make login-db`) to update the user row with email address you use in step 1 and sub_id from step 2 $ UPDATE user SET email_address = \"<EMAIL_YOU_USE_IN_STEP_1>\" , sub_id = \"<SUB_ID_FROM_STEP_2>\" WHERE user_id = \"<CURRENT_USER_ID>\" ; 1. Go to localhost:3000 and login with the same email and password ## 3. Data restrictions when interacting with Fineos At this point, we are ready to interact with Fineos from our local setup. Since Fineos is quite strict on the data they intake, we need to do a mark up of the data we use. 3.1 If you are testing as a Claimant Find a name, SSN & FEIN employee-employer combo to use from the spreadsheet Run make generate-wagesandcontributions employer_fein=<employer_fein> employee_ssn=<employee_ssn> IMPORTANT: FEIN and SSN don\u2019t require dashes and quotation marks, just numbers; This command will create/update the employer record with fineos_employer_id: 1, so if you want to try a new combo, change the existing fineos_employer_id to other number. Otherwise you will get error like (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"ix_employer_fineos_employer_id\". TODO (EMPLOYER-1677): https://lwd.atlassian.net/browse/EMPLOYER-1677 Now you can create an application with this name, SSN and FEIN, and test the flow through local portal or swagger UI. After submitting part 1, you can find the absence case in the Fineos Dashboard. 3.2 If you are testing as a Leave Admin We don't have a script for this part, so please go to your database shell( make login-db ) or use any PostgreSQL clients to modify the data. 3.2.1: Change current user account to a leave admin account Create a record in link_user_role table: INSERT INTO link_user_role ( user_id , role_id ) VALUES ( < current_user_id > , 3 ); 3.2.2: Verify current user as a leave admin for the claim's employer When Leave admin reviews a claim, the claim data is from Fineos database and only accessible to a linked and verified employer account. So first, we need to link our current leave admin account to the employer/organization of the claim. Find an absence case from Fineos dashboard and go to the related employer page If you are using a leave admin account linked to the employer of the claim, you could skip the following steps and retrieve the claim directly. find the Identification Number, which is employer_fein find a Portal User ID from the points of contact table choose a row and click edit, and then copy Portal User ID from the edit page Create/update an employer record INSERT INTO employer ( employer_id , employer_dba , employer_fein ) VALUES ( < random uuid > , \"Fake name\" , < FEIN from step 1 > ); Create a record in verification table: INSERT INTO verification ( verification_id , verification_type_id ) VALUES ( < random uuid > , 2 ); Create a record in link_user_leave_administrator table: INSERT INTO link_user_leave_administrator ( user_id , employer_id , fineos_web_id , user_leave_administrator_id , verification_id ) VALUES ( < current_user_id > , < employer_id from step 2 > , < Portal User ID from step 1 > , < random uuid > , < verification_id from step 3 > ); This block can be tweaked based on the data model diagram below to address various test cases. Examples: - Multiple records can be created to connect your leave admin user to multiple employers. - The fineos_web_id can be left NULL to indicate a leave admin not yet registered in FINEOS. - The verification_id can be left NULL to indicate an unverified leave admin. Note in the last case that the leave admin will be unverifiable if there are no employer quarterly contributions for the employer in the DB, AKA taxes have not been paid yet or have not been filed and reported. Create a record in claim table: INSERT INTO claim ( claim_id , employer_id , fineos_absence_id ) VALUES ( < random uuid > , < employer_id from step 2 > , < absence case id from step 1 > ); At this point, you can get this claim from swagger UI or local portal. More information about how the Leave Admin data model tracks FINEOS registration and verification is below: Leave Administrator User States","title":"Test Fineos server from local setup"},{"location":"test_fineos_from_local_setup/#test-fineos-server-from-local-setup","text":"This documentation shows how you can configure your local set up to interact with the FINEOS server of your choice. Test Fineos server from local setup 1. Set up env values 2. Local authentication 2.1 If you are using Local API only (use Swagger UI) 2.2 If you are using Local Portal to Local API Link Local Portal to Local API environment Login through a user account 3. Data restrictions when interacting with Fineos 3.1 If you are testing as a Claimant 3.2 If you are testing as a Leave Admin 3.2.1: Change current user account to a leave admin account 3.2.2: Verify current user as a leave admin for the claim's employer","title":"Test Fineos server from local setup"},{"location":"test_fineos_from_local_setup/#1-set-up-env-values","text":"Decide which FINEOS environment you wish to use. Paste and replace the following values in your docker-compose.yml test - DT2 - FINEOS_CLIENT_CUSTOMER_API_URL=https://dt2-api.masspfml.fineos.com/customerapi/ - FINEOS_CLIENT_WSCOMPOSER_API_URL=https://dt2-api.masspfml.fineos.com/integration-services/wscomposer/ - FINEOS_CLIENT_GROUP_CLIENT_API_URL=https://dt2-api.masspfml.fineos.com/groupclientapi/ - FINEOS_CLIENT_INTEGRATION_SERVICES_API_URL=https://dt2-api.masspfml.fineos.com/integration-services/ - FINEOS_CLIENT_OAUTH2_URL=https://dt2-api.masspfml.fineos.com/oauth2/token - FINEOS_CLIENT_OAUTH2_CLIENT_ID=1ral5e957i0l9shul52bhk0037 // no need to replace the following if you are using Swagger UI - COGNITO_USER_POOL_ID=us-east-1_HhQSLYSIe - COGNITO_USER_POOL_CLIENT_ID=7sjb96tvg8251lrq5vdk7de9 - COGNITO_USER_POOL_KEYS_URL=https://cognito-idp.us-east-1.amazonaws.com/us-east-1_HhQSLYSIe/.well-known/jwks.json cps-preview - DT3 - FINEOS_CLIENT_CUSTOMER_API_URL=https://dt3-api.masspfml.fineos.com/customerapi/ - FINEOS_CLIENT_WSCOMPOSER_API_URL=https://dt3-api.masspfml.fineos.com/integration-services/wscomposer/ - FINEOS_CLIENT_GROUP_CLIENT_API_URL=https://dt3-api.masspfml.fineos.com/groupclientapi/ - FINEOS_CLIENT_INTEGRATION_SERVICES_API_URL=https://dt3-api.masspfml.fineos.com/integration-services/ - FINEOS_CLIENT_OAUTH2_URL=https://dt3-api.masspfml.fineos.com/oauth2/token - FINEOS_CLIENT_OAUTH2_CLIENT_ID=2gptm2870hlo9ouq70poib8d5g // no need to replace the following if you are using Swagger UI - COGNITO_USER_POOL_ID=us-east-1_1OVYp4aZo - COGNITO_USER_POOL_CLIENT_ID=59oeobfn0759c8166pjh381joc - COGNITO_USER_POOL_KEYS_URL=https://cognito-idp.us-east-1.amazonaws.com/us-east-1_1OVYp4aZo/.well-known/jwks.json stage - IDT - FINEOS_CLIENT_CUSTOMER_API_URL=https://idt-api.masspfml.fineos.com/customerapi/ - FINEOS_CLIENT_WSCOMPOSER_API_URL=https://idt-api.masspfml.fineos.com/integration-services/wscomposer/ - FINEOS_CLIENT_GROUP_CLIENT_API_URL=https://idt-api.masspfml.fineos.com/groupclientapi/ - FINEOS_CLIENT_INTEGRATION_SERVICES_API_URL=https://idt-api.masspfml.fineos.com/integration-services/ - FINEOS_CLIENT_OAUTH2_URL=https://idt-api.masspfml.fineos.com/oauth2/token - FINEOS_CLIENT_OAUTH2_CLIENT_ID=1fa281uto9tjuqtm21jle7loam // no need to replace the following if you are using Swagger UI - COGNITO_USER_POOL_ID=us-east-1_HpL4XslLg - COGNITO_USER_POOL_CLIENT_ID=10rjcp71r8bnk4459c67bn18t8 - COGNITO_USER_POOL_KEYS_URL=https://cognito-idp.us-east-1.amazonaws.com/us-east-1_HpL4XslLg/.well-known/jwks.json Paste FINEOS_CLIENT_OAUTH2_CLIENT_SECRET from 1password7: Fineos Client ID and Secrets These are also available in AWS SSM Parameter Store under /service/pfml-api/${env}/fineos_oauth2_client_secret - FINEOS_CLIENT_OAUTH2_CLIENT_SECRET=...","title":"1. Set up env values"},{"location":"test_fineos_from_local_setup/#2-local-authentication","text":"","title":"2. Local authentication"},{"location":"test_fineos_from_local_setup/#21-if-you-are-using-local-api-only-use-swagger-ui","text":"Follow the instructions in /api/README.md","title":"2.1 If you are using Local API only (use Swagger UI)"},{"location":"test_fineos_from_local_setup/#22-if-you-are-using-local-portal-to-local-api","text":"","title":"2.2 If you are using Local Portal to Local API"},{"location":"test_fineos_from_local_setup/#link-local-portal-to-local-api-environment","text":"in portal/config use the same Cognito ID information you used in api/docker-compose.yml // development.js apiUrl: \"<http://localhost:1550/v1\"> // default.js cognitoUserPoolId: \"{COGNITO_USER_POOL_ID}\", cognitoUserPoolWebClientId: \"{COGNITO_USER_POOL_CLIENT_ID}\", More detailed explanation of how to setup local to local here","title":"Link Local Portal to Local API environment"},{"location":"test_fineos_from_local_setup/#login-through-a-user-account","text":"Option 1: Create a new account through the local portal directly Option 2: Use an existing account from a deployed environment If there is a Portal account you'd like to use which has existing information in the deployed FINEOS environment, we'll need to replicate the account in our local database using the email address and Cognito sub_id. 1. Log into the deployed Portal website; e.g. if you are targeting Fineos DT3, then use https://paidleave-cps-preview.eol.mass.gov/. For other environments, see [Environment URLs](https://lwd.atlassian.net/wiki/x/2oBEF). 2. Open console and run following command to get `sub_id` userDataCookie = document . cookie . split ( '; ' ) . find ( cookie => cookie . match ( /^CognitoIdentityServiceProvider.*userData=/ )) . split ( \"=\" )[ 1 ]; JSON . parse ( decodeURIComponent ( userDataCookie ))[ \"UserAttributes\" ][ 0 ][ \"Value\" ] 3. Update user in local API DB - run `make create-user` if no user in your database - Use a SQL client or database shell(`make login-db`) to update the user row with email address you use in step 1 and sub_id from step 2 $ UPDATE user SET email_address = \"<EMAIL_YOU_USE_IN_STEP_1>\" , sub_id = \"<SUB_ID_FROM_STEP_2>\" WHERE user_id = \"<CURRENT_USER_ID>\" ; 1. Go to localhost:3000 and login with the same email and password ## 3. Data restrictions when interacting with Fineos At this point, we are ready to interact with Fineos from our local setup. Since Fineos is quite strict on the data they intake, we need to do a mark up of the data we use.","title":"Login through a user account"},{"location":"test_fineos_from_local_setup/#31-if-you-are-testing-as-a-claimant","text":"Find a name, SSN & FEIN employee-employer combo to use from the spreadsheet Run make generate-wagesandcontributions employer_fein=<employer_fein> employee_ssn=<employee_ssn> IMPORTANT: FEIN and SSN don\u2019t require dashes and quotation marks, just numbers; This command will create/update the employer record with fineos_employer_id: 1, so if you want to try a new combo, change the existing fineos_employer_id to other number. Otherwise you will get error like (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"ix_employer_fineos_employer_id\". TODO (EMPLOYER-1677): https://lwd.atlassian.net/browse/EMPLOYER-1677 Now you can create an application with this name, SSN and FEIN, and test the flow through local portal or swagger UI. After submitting part 1, you can find the absence case in the Fineos Dashboard.","title":"3.1 If you are testing as a Claimant"},{"location":"test_fineos_from_local_setup/#32-if-you-are-testing-as-a-leave-admin","text":"We don't have a script for this part, so please go to your database shell( make login-db ) or use any PostgreSQL clients to modify the data.","title":"3.2 If you are testing as a Leave Admin"},{"location":"test_fineos_from_local_setup/#321-change-current-user-account-to-a-leave-admin-account","text":"Create a record in link_user_role table: INSERT INTO link_user_role ( user_id , role_id ) VALUES ( < current_user_id > , 3 );","title":"3.2.1: Change current user account to a leave admin account"},{"location":"test_fineos_from_local_setup/#322-verify-current-user-as-a-leave-admin-for-the-claims-employer","text":"When Leave admin reviews a claim, the claim data is from Fineos database and only accessible to a linked and verified employer account. So first, we need to link our current leave admin account to the employer/organization of the claim. Find an absence case from Fineos dashboard and go to the related employer page If you are using a leave admin account linked to the employer of the claim, you could skip the following steps and retrieve the claim directly. find the Identification Number, which is employer_fein find a Portal User ID from the points of contact table choose a row and click edit, and then copy Portal User ID from the edit page Create/update an employer record INSERT INTO employer ( employer_id , employer_dba , employer_fein ) VALUES ( < random uuid > , \"Fake name\" , < FEIN from step 1 > ); Create a record in verification table: INSERT INTO verification ( verification_id , verification_type_id ) VALUES ( < random uuid > , 2 ); Create a record in link_user_leave_administrator table: INSERT INTO link_user_leave_administrator ( user_id , employer_id , fineos_web_id , user_leave_administrator_id , verification_id ) VALUES ( < current_user_id > , < employer_id from step 2 > , < Portal User ID from step 1 > , < random uuid > , < verification_id from step 3 > ); This block can be tweaked based on the data model diagram below to address various test cases. Examples: - Multiple records can be created to connect your leave admin user to multiple employers. - The fineos_web_id can be left NULL to indicate a leave admin not yet registered in FINEOS. - The verification_id can be left NULL to indicate an unverified leave admin. Note in the last case that the leave admin will be unverifiable if there are no employer quarterly contributions for the employer in the DB, AKA taxes have not been paid yet or have not been filed and reported. Create a record in claim table: INSERT INTO claim ( claim_id , employer_id , fineos_absence_id ) VALUES ( < random uuid > , < employer_id from step 2 > , < absence case id from step 1 > ); At this point, you can get this claim from swagger UI or local portal. More information about how the Leave Admin data model tracks FINEOS registration and verification is below: Leave Administrator User States","title":"3.2.2: Verify current user as a leave admin for the claim's employer"},{"location":"tools/","text":"Introduction The root of the monorepo includes tools for code style formatting and linting. Both linting and formatting are enabled in a pre-commit hook set up with husky and lint-staged . Linting This project uses ESLint for JavaScript code. npm run lint Code Formatting This project uses Prettier for opinionated code formatting. All .js and .jsx files are automatically re-written using prettier formatting as part of the pre-commit hook.","title":"Tools"},{"location":"tools/#introduction","text":"The root of the monorepo includes tools for code style formatting and linting. Both linting and formatting are enabled in a pre-commit hook set up with husky and lint-staged .","title":"Introduction"},{"location":"tools/#linting","text":"This project uses ESLint for JavaScript code. npm run lint","title":"Linting"},{"location":"tools/#code-formatting","text":"This project uses Prettier for opinionated code formatting. All .js and .jsx files are automatically re-written using prettier formatting as part of the pre-commit hook.","title":"Code Formatting"},{"location":"api/archive-reference/","text":"Archived Features Overview (API) In accordance with the archive spec , this document serves as documentation for archived API features. Bank of America Integration https://github.com/EOLWD/pfml/tree/archive/bank-of-america DFML initially had plans to issue debit cards through Bank of America. Code was written to generate mock data files for the integration. Then the Bank of America work was put on indefinite hold as Bank of America suspended any new integrations with benefit programs in late 2020. Bulk user import https://github.com/EOLWD/pfml/tree/archive/bulk-user-import Before Leave Admins could self-register and verify, we had two manual processes for reaching out to leave admins and importing them into the system: one using a Formstack survey, and another was a tool that supported importing Leave Admin users from one or more CSV files stored locally or in S3. The Formstack importer was related to the leave admin outreach work, and while it did get deployed, it was sidestepped with the bulk imports instead. After self-registration/verification launched, the bulk-imports process was discontinued as well. Bulk import data was duplicated to Sharepoint at EOL-PFMLPROJECT > Shared Documents > General > Service Desk > POC Loads . MMARS integration https://github.com/EOLWD/pfml/tree/archive/mmars-integration Manual/MMARS/payment voucher was the original payment processing approach/integration for PFML, but PUB payments took over in late June/early July 2021.","title":"Archived Features Overview (API)"},{"location":"api/archive-reference/#archived-features-overview-api","text":"In accordance with the archive spec , this document serves as documentation for archived API features.","title":"Archived Features Overview (API)"},{"location":"api/archive-reference/#bank-of-america-integration","text":"https://github.com/EOLWD/pfml/tree/archive/bank-of-america DFML initially had plans to issue debit cards through Bank of America. Code was written to generate mock data files for the integration. Then the Bank of America work was put on indefinite hold as Bank of America suspended any new integrations with benefit programs in late 2020.","title":"Bank of America Integration"},{"location":"api/archive-reference/#bulk-user-import","text":"https://github.com/EOLWD/pfml/tree/archive/bulk-user-import Before Leave Admins could self-register and verify, we had two manual processes for reaching out to leave admins and importing them into the system: one using a Formstack survey, and another was a tool that supported importing Leave Admin users from one or more CSV files stored locally or in S3. The Formstack importer was related to the leave admin outreach work, and while it did get deployed, it was sidestepped with the bulk imports instead. After self-registration/verification launched, the bulk-imports process was discontinued as well. Bulk import data was duplicated to Sharepoint at EOL-PFMLPROJECT > Shared Documents > General > Service Desk > POC Loads .","title":"Bulk user import"},{"location":"api/archive-reference/#mmars-integration","text":"https://github.com/EOLWD/pfml/tree/archive/mmars-integration Manual/MMARS/payment voucher was the original payment processing approach/integration for PFML, but PUB payments took over in late June/early July 2021.","title":"MMARS integration"},{"location":"api/automated-tests/","text":"Automated Tests For writing tests see /docs/api/writing-tests.md . There are various test* make targets set up for convenience. To run the entire test suite: make test Ultimately the targets just wrap the test runner pytest with minor tweaks and wrapping it in helper tools. To pass arguments to pytest through make test you can set the args variable. For example, to run only the tests in test_user.py : make test args = tests/api/test_users.py To run only a single test: make test args = tests/api/test_users.py::test_users_get To pass multiple arguments: make test args = \"-x tests/api/test_users.py\" For a more complete description of the many ways you can select tests to run and different flags, refer to the pytest docs (and/or pytest --help ). During Development While working on a part of the system, you may not be interested in running the entire test suite all the time. To just run the test impacted by the changes made since they last ran, you can use: make test-changed Note that it will run the entire test suite the first time that command is run to collect which source files touch which tests, subsequent runs should only run the tests that need to run. And instead of running that command manually, you can kick off a process to automatically run the tests when files change with: make test-watch-changed To run tests decorated with @pytest.mark.dev_focus whenever a file is saved: make test-watch-focus test-watch-focus is a wrapper around the more flexible test-watch Makefile target that we can use to re-run specific tests whenever a file changes, for instance. make test-watch args = tests/api/test_users.py::test_users_get Arguments for test-watch are the same as args for make test as discussed in the section above. To run only unit tests: make test-unit","title":"Automated Tests"},{"location":"api/automated-tests/#automated-tests","text":"For writing tests see /docs/api/writing-tests.md . There are various test* make targets set up for convenience. To run the entire test suite: make test Ultimately the targets just wrap the test runner pytest with minor tweaks and wrapping it in helper tools. To pass arguments to pytest through make test you can set the args variable. For example, to run only the tests in test_user.py : make test args = tests/api/test_users.py To run only a single test: make test args = tests/api/test_users.py::test_users_get To pass multiple arguments: make test args = \"-x tests/api/test_users.py\" For a more complete description of the many ways you can select tests to run and different flags, refer to the pytest docs (and/or pytest --help ).","title":"Automated Tests"},{"location":"api/automated-tests/#during-development","text":"While working on a part of the system, you may not be interested in running the entire test suite all the time. To just run the test impacted by the changes made since they last ran, you can use: make test-changed Note that it will run the entire test suite the first time that command is run to collect which source files touch which tests, subsequent runs should only run the tests that need to run. And instead of running that command manually, you can kick off a process to automatically run the tests when files change with: make test-watch-changed To run tests decorated with @pytest.mark.dev_focus whenever a file is saved: make test-watch-focus test-watch-focus is a wrapper around the more flexible test-watch Makefile target that we can use to re-run specific tests whenever a file changes, for instance. make test-watch args = tests/api/test_users.py::test_users_get Arguments for test-watch are the same as args for make test as discussed in the section above. To run only unit tests: make test-unit","title":"During Development"},{"location":"api/database-migrations/","text":"Database Migrations Running migrations Creating new migrations Multi-head situations Deployment Removing a column Removing a table Running migrations When you're first setting up your environment, ensure that migrations are run against your db so it has all the required tables. make init does this, but if needing to work with the migrations directly, some common commands: make db-upgrade # Apply pending migrations to db make db-downgrade # Rollback last migration to db make db-downgrade-all # Rollback all migrations Creating new migrations If you've changed a python object model, auto-generate a migration file for the database and run it: $ make db-migrate-create MIGRATE_MSG = \"<brief description of change>\" $ make db-upgrade Example: Adding a new column to an existing table: 1. Manually update the database models with the changes (`massgov/pfml/db/models/employees.py` in this example) class Address ( Base ): ... created_at = Column ( TIMESTAMP ( timezone = True )) # Newly added line 2. Automatically generate a migration file with `make db-migrate-create MIGRATE_MSG=\"Add created_at timestamp to address table\"` ... def upgrade (): # ### commands auto generated by Alembic - please adjust! ### op . add_column ( \"address\" , sa . Column ( \"created_at\" , sa . TIMESTAMP ( timezone = True ), nullable = True )) # ### end Alembic commands ### def downgrade (): # ### commands auto generated by Alembic - please adjust! ### op . drop_column ( \"address\" , \"created_at\" ) # ### end Alembic commands ### 3. Manually adjust the migration file as needed. Some changes will not fully auto-generate (like foreign keys), so make sure that all desired changes are included. Multi-head situations Alembic migrations form an ordered history, with each migration having at least one parent migration as specified by the down_revision variable. This can be visualized by: make db-migrate-history When multiple migrations are created that point to the same down_revision a branch is created, with the tip of each branch being a \"head\". The above history command will show this, but a list of just the heads can been retrieved with: make db-migrate-heads CI/CD runs migrations to reach the \"head\". When there are multiple, Alembic can't resolve which migrations need to be run. If you run into this error, you'll need to fix the migration branches/heads before merging to main . If the migrations don't depend on each other, which is likely if they've branched, then you can just run: make db-migrate-merge-heads Which will create a new migration pointing to all current \"head\"s, effectively pulling them all together. When branched migrations do need to happen in a defined order, then manually update the down_revision of one that should happen second to reference to the migration that should happen first. Deployment Our deployment is a two-step process: Run database migrations, which will update table schemas. Update API code. Since we're updating two independent services, we should consider how code changes across the API and database might cause integration issues during rollout (similar to code changes across the portal and api). Part of the reason DB migrations run before app deploys (instead of after) is so that additions don\u2019t need to be split out, i.e. migrations that add columns for new features don\u2019t need delayed treatment. A new column can be added to the DB before the app code using it is deployed so they can go out in the same commit/release. However, if we are removing a column or table from the DB, we need to split this across two deploys. Otherwise, the app is likely to break, since in between steps 1 & 2 above, the app may try to access a table or column that no longer exists in our databse. Removing a column First Deploy In order to remove a column, we'll first need to remove all usage of the column in the API code base. However, since the column will still be in the database model, SQLAlchemy will still include it in SELECT s and INSERT s for the table. We'll need to set this column to deferred , in order to exclude it from queries, and to evaluates_none , to stop inserting null for it. We have a function deprecated_column that handles both of those changes, so we can easily make this change by updating column_to_remove = Column(...) to column_to_remove = deprecated_column(...) . Second Deploy Once the column usage removal has been deployed (including deprecating the column in the database model), we can safely remove the column from the database model and the actual database. To do so, simply delete the deprecated column from the database model and create a corresponding database migration. Removing a table First Deploy Removing a table is similar to, but simpler than, removing a column. You'll still start by removing all usage of the table in the API code base. No further steps are required at this point, since SQLAlchemy won't be interacting with the table at this point. Second Deploy Once the table usage removal has been deployed, we can safely remove the table from the database model and the actual database. Again, simply delete the table from the database model and create a corresponding migration.","title":"Database Migrations"},{"location":"api/database-migrations/#database-migrations","text":"Running migrations Creating new migrations Multi-head situations Deployment Removing a column Removing a table","title":"Database Migrations"},{"location":"api/database-migrations/#running-migrations","text":"When you're first setting up your environment, ensure that migrations are run against your db so it has all the required tables. make init does this, but if needing to work with the migrations directly, some common commands: make db-upgrade # Apply pending migrations to db make db-downgrade # Rollback last migration to db make db-downgrade-all # Rollback all migrations","title":"Running migrations"},{"location":"api/database-migrations/#creating-new-migrations","text":"If you've changed a python object model, auto-generate a migration file for the database and run it: $ make db-migrate-create MIGRATE_MSG = \"<brief description of change>\" $ make db-upgrade Example: Adding a new column to an existing table: 1. Manually update the database models with the changes (`massgov/pfml/db/models/employees.py` in this example) class Address ( Base ): ... created_at = Column ( TIMESTAMP ( timezone = True )) # Newly added line 2. Automatically generate a migration file with `make db-migrate-create MIGRATE_MSG=\"Add created_at timestamp to address table\"` ... def upgrade (): # ### commands auto generated by Alembic - please adjust! ### op . add_column ( \"address\" , sa . Column ( \"created_at\" , sa . TIMESTAMP ( timezone = True ), nullable = True )) # ### end Alembic commands ### def downgrade (): # ### commands auto generated by Alembic - please adjust! ### op . drop_column ( \"address\" , \"created_at\" ) # ### end Alembic commands ### 3. Manually adjust the migration file as needed. Some changes will not fully auto-generate (like foreign keys), so make sure that all desired changes are included.","title":"Creating new migrations"},{"location":"api/database-migrations/#multi-head-situations","text":"Alembic migrations form an ordered history, with each migration having at least one parent migration as specified by the down_revision variable. This can be visualized by: make db-migrate-history When multiple migrations are created that point to the same down_revision a branch is created, with the tip of each branch being a \"head\". The above history command will show this, but a list of just the heads can been retrieved with: make db-migrate-heads CI/CD runs migrations to reach the \"head\". When there are multiple, Alembic can't resolve which migrations need to be run. If you run into this error, you'll need to fix the migration branches/heads before merging to main . If the migrations don't depend on each other, which is likely if they've branched, then you can just run: make db-migrate-merge-heads Which will create a new migration pointing to all current \"head\"s, effectively pulling them all together. When branched migrations do need to happen in a defined order, then manually update the down_revision of one that should happen second to reference to the migration that should happen first.","title":"Multi-head situations"},{"location":"api/database-migrations/#deployment","text":"Our deployment is a two-step process: Run database migrations, which will update table schemas. Update API code. Since we're updating two independent services, we should consider how code changes across the API and database might cause integration issues during rollout (similar to code changes across the portal and api). Part of the reason DB migrations run before app deploys (instead of after) is so that additions don\u2019t need to be split out, i.e. migrations that add columns for new features don\u2019t need delayed treatment. A new column can be added to the DB before the app code using it is deployed so they can go out in the same commit/release. However, if we are removing a column or table from the DB, we need to split this across two deploys. Otherwise, the app is likely to break, since in between steps 1 & 2 above, the app may try to access a table or column that no longer exists in our databse.","title":"Deployment"},{"location":"api/database-migrations/#removing-a-column","text":"First Deploy In order to remove a column, we'll first need to remove all usage of the column in the API code base. However, since the column will still be in the database model, SQLAlchemy will still include it in SELECT s and INSERT s for the table. We'll need to set this column to deferred , in order to exclude it from queries, and to evaluates_none , to stop inserting null for it. We have a function deprecated_column that handles both of those changes, so we can easily make this change by updating column_to_remove = Column(...) to column_to_remove = deprecated_column(...) . Second Deploy Once the column usage removal has been deployed (including deprecating the column in the database model), we can safely remove the column from the database model and the actual database. To do so, simply delete the deprecated column from the database model and create a corresponding database migration.","title":"Removing a column"},{"location":"api/database-migrations/#removing-a-table","text":"First Deploy Removing a table is similar to, but simpler than, removing a column. You'll still start by removing all usage of the table in the API code base. No further steps are required at this point, since SQLAlchemy won't be interacting with the table at this point. Second Deploy Once the table usage removal has been deployed, we can safely remove the table from the database model and the actual database. Again, simply delete the table from the database model and create a corresponding migration.","title":"Removing a table"},{"location":"api/db-roles-and-users/","text":"Database Users Access to the database is broken into two parts: Roles, which are the permission sets for data access Users, what authenticates with the database and are granted Roles Roles Database roles are managed through the migrations (search for CREATE ROLE or GRANT / REVOKE in the migrations). Current roles: app , general purpose role with wide access, automatically granted all permissions on any table created in the application schema (currently public ) Users Database users are declared in /api/massgov/pfml/db/admin.py in the user_configs variable, where the list of roles (as created in migrations) they should have. For AWS environments, an IAM policy needs to be created and connected to the systems that should be able to connect as that user. There is a helper script, /api/bin/create-db-user-config.py , that will print out the configuration required for adding a new user. Local environment To create the database users locally, run: make db-create-users This will create the all users with the password specified by the environment variable DB_PASSWORD (since the actual password doesn't matter in local development). AWS environments CI will automatically run the db-admin-create-db-users task to create users after migrations on every deploy. So should generally not require manually action. But if needed, to run the task manually: ../../bin/run-ecs-task/run-task.sh <env> db-admin-create-db-users <first name>.<last name> AWS database users use IAM authentication instead of regular passwords. High level implications of IAM auth: RDS IAM authentication requires connecting to DB over SSL DB users needing to authenticate with IAM must be granted the rds_iam role IAM policy attached to service IAM roles/users allowing connection to DB as given user Output of the RDS SDK generate_db_auth_token() function used as password when connecting as user. These tokens are only valid for 15 minutes, so this function should be called to get a valid token any time a connection is to be made.","title":"Database Users"},{"location":"api/db-roles-and-users/#database-users","text":"Access to the database is broken into two parts: Roles, which are the permission sets for data access Users, what authenticates with the database and are granted Roles","title":"Database Users"},{"location":"api/db-roles-and-users/#roles","text":"Database roles are managed through the migrations (search for CREATE ROLE or GRANT / REVOKE in the migrations). Current roles: app , general purpose role with wide access, automatically granted all permissions on any table created in the application schema (currently public )","title":"Roles"},{"location":"api/db-roles-and-users/#users","text":"Database users are declared in /api/massgov/pfml/db/admin.py in the user_configs variable, where the list of roles (as created in migrations) they should have. For AWS environments, an IAM policy needs to be created and connected to the systems that should be able to connect as that user. There is a helper script, /api/bin/create-db-user-config.py , that will print out the configuration required for adding a new user.","title":"Users"},{"location":"api/db-roles-and-users/#local-environment","text":"To create the database users locally, run: make db-create-users This will create the all users with the password specified by the environment variable DB_PASSWORD (since the actual password doesn't matter in local development).","title":"Local environment"},{"location":"api/db-roles-and-users/#aws-environments","text":"CI will automatically run the db-admin-create-db-users task to create users after migrations on every deploy. So should generally not require manually action. But if needed, to run the task manually: ../../bin/run-ecs-task/run-task.sh <env> db-admin-create-db-users <first name>.<last name> AWS database users use IAM authentication instead of regular passwords. High level implications of IAM auth: RDS IAM authentication requires connecting to DB over SSL DB users needing to authenticate with IAM must be granted the rds_iam role IAM policy attached to service IAM roles/users allowing connection to DB as given user Output of the RDS SDK generate_db_auth_token() function used as password when connecting as user. These tokens are only valid for 15 minutes, so this function should be called to get a valid token any time a connection is to be made.","title":"AWS environments"},{"location":"api/development-environment-setup/","text":"Development Environment Setup Setup Methods (Preferred) Docker + GNU Make Native Developer Setup Environment Configuration Development Workflow Makefile utilities Managing Python dependencies poetry.lock conflicts Setup Methods You can set up your development environment in one of the following ways: (Preferred) Docker + GNU Make Native developer setup: Install dependencies directly on your OS (Preferred) Docker + GNU Make Docker is heavily recommended for a local development environment. It sets up Python, Poetry and installs development dependencies automatically, and can create a full environment including the API application and a backing database. Follow instructions here to install Docker for your OS. In a docker-centric environment, we support a couple different developer workflows: Start command from: Container Lifetime RUN_CMD_OPT your host \ud83d\ude4b\u200d\u2640\ufe0f Long-running DOCKER_EXEC inside docker \ud83d\udc33 Long-running N/A your host \ud83d\ude4b\u200d\u2640\ufe0f Single-use DOCKER_RUN Mixed Mixed Mixed The default is DOCKER_RUN and should always just work\u2122. But this spins up a new container for every command, which can be slow. If you are working on the API a lot, you may want to consider one of the alternative setups below and/or the Native Developer Setup . Note: Some tasks, including initial setup, will usually need to be run with DOCKER_RUN regardless of the workflow used otherwise. Examples include generating an initial local JWKS ( make jwks.json ) or running migrations ( make db-upgrade ). Send commands from your host to a long-running container If you want to re-use a docker application for various Python and development make commands (e.g. linting and testing), you should set RUN_CMD_OPT to DOCKER_EXEC . $ export RUN_CMD_OPT = DOCKER_EXEC $ make test Log into a docker container to run commands If you intend to start a Docker environment and log into it like a remote server, you can leave RUN_CMD_OPT alone and use make login instead. $ make login > make test Start a new docker container for every command If you plan to run commands through temporary, single-use Docker containers, you should set your RUN_CMD_OPT to DOCKER_RUN : $ export RUN_CMD_OPT = DOCKER_RUN $ make test Note this is the default setting. Mix and Match If you plan to mix and match things, you'll have to juggle RUN_CMD_OPT yourself. For example: - running static lints outside of Docker with native developer setup: RUN_CMD_OPT=NATIVE make lint - running tests inside of Docker after a make start : RUN_CMD_OPT=DOCKER_EXEC make test Native Developer Setup To setup a development environment outside of Docker, you'll need to install a few things. Install at least Python 3.8. pyenv is one popular option for installing Python, or asdf . After installing and activating the right version of Python, install poetry . Set RUN_CMD_OPT to NATIVE in your development environment. Run make deps to install Python dependencies and development tooling. You should now be able to run developer tooling natively, like linting. To run the application you'll need some environment variables set. You can largely copy-paste the env vars in docker-compose.yml to your native environment. DB_HOST should be changed to localhost . You can then start up just the PostgreSQL database via Docker with make start-db and then the API server with make run-native . Environment Configuration Environment variables for the local app are in the docker-compose.yml file. services : ... mass-pfml-api : ... environment : - ENVIRONMENT=local - DB_HOST=mass-pfml-db - ... Changes here will be picked up next time make start is run. Development Workflow All mandatory checks run as part of the Github CI pull request process. See the api-*.yml files in /.github/workflows to see what gets run. You can run these checks on your local machine before the PR stage (such as before each commit or before pushing to Github): make format to fix any formatting issues make check to run checks (e.g. linting, testing) See the other check-static and lint-* make targets, as well as the Tests section below to run more targeted versions of these checks Makefile utilities Many of the utilities and scripts to developing and managing the application have been standardized into a ( GNU make/Makefile . This makes the command format more consistent, and centralizes where the tools are managed. If you need an automated script to perform a task, check the Makefile! It may already exist! Each component of the system may have its own set of make \"targets\" with their own in-depth documentation. TODO: Update w/ link to more detailed documentation ( API-1477 ) Managing Python dependencies Python dependencies are managed through poetry . See its docs for adding/removing/running Python things (or see poetry --help ). Cheatsheet: Action Command Add a dependency poetry add <package name> [--dev] Update a dependency poetry update <package name> Remove a dependency poetry remove <package name> See installed package info poetry show [package name] Run a command in Python environment poetry run <cmd> See all package info poetry show [--outdated] [--latest] Upgrade package to latest version poetry add <package name>@latest [--dev] poetry.lock conflicts Poetry maintains a metadata.content-hash value in its lock file ( poetry.lock ) which is the hash of the sorted contents of the pyproject.toml (helps keep track of if the lock file is out of date with declared dependencies and such). This can be a cause of merge conflicts. To resolve: Pick either side of the conflict (i.e., edit the content-hash line to either hash) Then run make update-poetry-lock-hash to update the hash to the correct value","title":"Development Environment Setup"},{"location":"api/development-environment-setup/#development-environment-setup","text":"Setup Methods (Preferred) Docker + GNU Make Native Developer Setup Environment Configuration Development Workflow Makefile utilities Managing Python dependencies poetry.lock conflicts","title":"Development Environment Setup"},{"location":"api/development-environment-setup/#setup-methods","text":"You can set up your development environment in one of the following ways: (Preferred) Docker + GNU Make Native developer setup: Install dependencies directly on your OS","title":"Setup Methods"},{"location":"api/development-environment-setup/#preferred-docker-gnu-make","text":"Docker is heavily recommended for a local development environment. It sets up Python, Poetry and installs development dependencies automatically, and can create a full environment including the API application and a backing database. Follow instructions here to install Docker for your OS. In a docker-centric environment, we support a couple different developer workflows: Start command from: Container Lifetime RUN_CMD_OPT your host \ud83d\ude4b\u200d\u2640\ufe0f Long-running DOCKER_EXEC inside docker \ud83d\udc33 Long-running N/A your host \ud83d\ude4b\u200d\u2640\ufe0f Single-use DOCKER_RUN Mixed Mixed Mixed The default is DOCKER_RUN and should always just work\u2122. But this spins up a new container for every command, which can be slow. If you are working on the API a lot, you may want to consider one of the alternative setups below and/or the Native Developer Setup . Note: Some tasks, including initial setup, will usually need to be run with DOCKER_RUN regardless of the workflow used otherwise. Examples include generating an initial local JWKS ( make jwks.json ) or running migrations ( make db-upgrade ). Send commands from your host to a long-running container If you want to re-use a docker application for various Python and development make commands (e.g. linting and testing), you should set RUN_CMD_OPT to DOCKER_EXEC . $ export RUN_CMD_OPT = DOCKER_EXEC $ make test Log into a docker container to run commands If you intend to start a Docker environment and log into it like a remote server, you can leave RUN_CMD_OPT alone and use make login instead. $ make login > make test Start a new docker container for every command If you plan to run commands through temporary, single-use Docker containers, you should set your RUN_CMD_OPT to DOCKER_RUN : $ export RUN_CMD_OPT = DOCKER_RUN $ make test Note this is the default setting. Mix and Match If you plan to mix and match things, you'll have to juggle RUN_CMD_OPT yourself. For example: - running static lints outside of Docker with native developer setup: RUN_CMD_OPT=NATIVE make lint - running tests inside of Docker after a make start : RUN_CMD_OPT=DOCKER_EXEC make test","title":"(Preferred) Docker + GNU Make"},{"location":"api/development-environment-setup/#native-developer-setup","text":"To setup a development environment outside of Docker, you'll need to install a few things. Install at least Python 3.8. pyenv is one popular option for installing Python, or asdf . After installing and activating the right version of Python, install poetry . Set RUN_CMD_OPT to NATIVE in your development environment. Run make deps to install Python dependencies and development tooling. You should now be able to run developer tooling natively, like linting. To run the application you'll need some environment variables set. You can largely copy-paste the env vars in docker-compose.yml to your native environment. DB_HOST should be changed to localhost . You can then start up just the PostgreSQL database via Docker with make start-db and then the API server with make run-native .","title":"Native Developer Setup"},{"location":"api/development-environment-setup/#environment-configuration","text":"Environment variables for the local app are in the docker-compose.yml file. services : ... mass-pfml-api : ... environment : - ENVIRONMENT=local - DB_HOST=mass-pfml-db - ... Changes here will be picked up next time make start is run.","title":"Environment Configuration"},{"location":"api/development-environment-setup/#development-workflow","text":"All mandatory checks run as part of the Github CI pull request process. See the api-*.yml files in /.github/workflows to see what gets run. You can run these checks on your local machine before the PR stage (such as before each commit or before pushing to Github): make format to fix any formatting issues make check to run checks (e.g. linting, testing) See the other check-static and lint-* make targets, as well as the Tests section below to run more targeted versions of these checks","title":"Development Workflow"},{"location":"api/development-environment-setup/#makefile-utilities","text":"Many of the utilities and scripts to developing and managing the application have been standardized into a ( GNU make/Makefile . This makes the command format more consistent, and centralizes where the tools are managed. If you need an automated script to perform a task, check the Makefile! It may already exist! Each component of the system may have its own set of make \"targets\" with their own in-depth documentation. TODO: Update w/ link to more detailed documentation ( API-1477 )","title":"Makefile utilities"},{"location":"api/development-environment-setup/#managing-python-dependencies","text":"Python dependencies are managed through poetry . See its docs for adding/removing/running Python things (or see poetry --help ). Cheatsheet: Action Command Add a dependency poetry add <package name> [--dev] Update a dependency poetry update <package name> Remove a dependency poetry remove <package name> See installed package info poetry show [package name] Run a command in Python environment poetry run <cmd> See all package info poetry show [--outdated] [--latest] Upgrade package to latest version poetry add <package name>@latest [--dev]","title":"Managing Python dependencies"},{"location":"api/development-environment-setup/#poetrylock-conflicts","text":"Poetry maintains a metadata.content-hash value in its lock file ( poetry.lock ) which is the hash of the sorted contents of the pyproject.toml (helps keep track of if the lock file is out of date with declared dependencies and such). This can be a cause of merge conflicts. To resolve: Pick either side of the conflict (i.e., edit the content-hash line to either hash) Then run make update-poetry-lock-hash to update the hash to the correct value","title":"poetry.lock conflicts"},{"location":"api/environment-variables/","text":"Environment Variables We configure the application by using environment variables . Local Development During local development, we specify environment variables through docker-compose.yml . mass-pfml-api : ... environment : - ENVIRONMENT=local - DB_HOST=mass-pfml-db - DB_NAME=pfml - DB_USERNAME=pfml - DB_PASSWORD=secret123 When updating these variables, you'll need to run make build in order to rebuild your container and pick up the new values. Overriding AWS credentials To use your AWS credentials locally: Run aws sso login Override the container's AWS credentials path in docker-compose.override.yml (there's a commented line showing an example) Set the AWS_PROFILE in docker-compose.yml to the AWS profile you want to use Rerun make build Deployed Environments In deployed environments, variables are pulled in through AWS Elastic Container Service (ECS) as listed in the container definition . Non-sensitive values are encoded into the definition when Terraform generates it: \"environment\" : [ { \"name\" : \"ENVIRONMENT\" , \"value\" : \"${environment_name}\" }, { \"name\" : \"DB_HOST\" , \"value\" : \"${db_host}\" }, { \"name\" : \"DB_NAME\" , \"value\" : \"${db_name}\" }, { \"name\" : \"DB_USERNAME\" , \"value\" : \"${db_username}\" }, { ... } ] ...and sensitive values are pulled in from AWS SSM Parameter Store when the container starts: \"secrets\" : [ { \"name\" : \"DB_PASSWORD\" , \"valueFrom\" : \"/service/${app_name}/${environment_name}/db-password\" }, { \"name\" : \"NEW_RELIC_LICENSE_KEY\" , \"valueFrom\" : \"/service/${app_name}/common/newrelic-license-key\" }, { ... } ] To view or update non-sensitive values in the container definition file, set them in the container_definitions resource in service.tf . To recap for non-sensitive values: If it's a variable that should be configured explicitly for each environment: Add new variable to the API template variables.tf Set the new variables in each environment configuration Add and set variable in service.tf , either referring to variables from step 1 or to other Terraform resources. Use variable set in service.tf to set environment variable in container_definitions.json To view or update sensitive values: Go to the key in the AWS Systems Manager/Parameter Store console . Create or update the sensitive string with the default KMS key, matching the valueFrom field that you specify in the container definition above. In both cases, the application will need to be redeployed before any changes to the environment variables are picked up.","title":"Environment Variables"},{"location":"api/environment-variables/#environment-variables","text":"We configure the application by using environment variables .","title":"Environment Variables"},{"location":"api/environment-variables/#local-development","text":"During local development, we specify environment variables through docker-compose.yml . mass-pfml-api : ... environment : - ENVIRONMENT=local - DB_HOST=mass-pfml-db - DB_NAME=pfml - DB_USERNAME=pfml - DB_PASSWORD=secret123 When updating these variables, you'll need to run make build in order to rebuild your container and pick up the new values.","title":"Local Development"},{"location":"api/environment-variables/#overriding-aws-credentials","text":"To use your AWS credentials locally: Run aws sso login Override the container's AWS credentials path in docker-compose.override.yml (there's a commented line showing an example) Set the AWS_PROFILE in docker-compose.yml to the AWS profile you want to use Rerun make build","title":"Overriding AWS credentials"},{"location":"api/environment-variables/#deployed-environments","text":"In deployed environments, variables are pulled in through AWS Elastic Container Service (ECS) as listed in the container definition . Non-sensitive values are encoded into the definition when Terraform generates it: \"environment\" : [ { \"name\" : \"ENVIRONMENT\" , \"value\" : \"${environment_name}\" }, { \"name\" : \"DB_HOST\" , \"value\" : \"${db_host}\" }, { \"name\" : \"DB_NAME\" , \"value\" : \"${db_name}\" }, { \"name\" : \"DB_USERNAME\" , \"value\" : \"${db_username}\" }, { ... } ] ...and sensitive values are pulled in from AWS SSM Parameter Store when the container starts: \"secrets\" : [ { \"name\" : \"DB_PASSWORD\" , \"valueFrom\" : \"/service/${app_name}/${environment_name}/db-password\" }, { \"name\" : \"NEW_RELIC_LICENSE_KEY\" , \"valueFrom\" : \"/service/${app_name}/common/newrelic-license-key\" }, { ... } ] To view or update non-sensitive values in the container definition file, set them in the container_definitions resource in service.tf . To recap for non-sensitive values: If it's a variable that should be configured explicitly for each environment: Add new variable to the API template variables.tf Set the new variables in each environment configuration Add and set variable in service.tf , either referring to variables from step 1 or to other Terraform resources. Use variable set in service.tf to set environment variable in container_definitions.json To view or update sensitive values: Go to the key in the AWS Systems Manager/Parameter Store console . Create or update the sensitive string with the default KMS key, matching the valueFrom field that you specify in the container definition above. In both cases, the application will need to be redeployed before any changes to the environment variables are picked up.","title":"Deployed Environments"},{"location":"api/fields-and-validations/","text":"During development of the Portal, there are common tasks an API or Portal engineer may need to perform within the API codebase. Adding fields Add the field to the DB model: db/models/applications.py Generate a new migration file See below for how to allow this field in API requests and responses. Accept fields in requests Application fields should be nullable / Optional to support the Portal sending partial request bodies in its multi-page flows. Add the field to the openapi.yaml spec. Add the field to the requests.py Pydantic model Add test coverage to assert the new field is persisted in the DB. Include fields in responses Add the field to the openapi.yaml spec Add the field to the responses.py Pydantic model Add test coverage to assert the new field is included in responses. Validation rules Validation Rules vs. Eligibility Rules We should distinguish validation rules from eligibility rules. It\u2019s one thing to automatically deny someone eligibility based on some eligibility criteria (like child birth date being within 12 months) and then allowing that person to appeal (if they have some valid extenuating circumstances), and it\u2019s an entirely different thing to prevent them from applying altogether claiming that the application isn\u2019t even a valid application. Validation should only serve to help prevent incorrect data (typos, etc) not prevent ineligible applications . We should be careful not to accidentally deny a claimant\u2019s right to an appeal or otherwise make the system less flexible than it needs to be as is so often the case with government systems. Adding validation rules Validation rules currently are enforced at three different layers: OpenAPI generally enforces a subset of rules on individual fields Pydantic models enforce some business rules on individual fields Custom code enforces presence of required fields or rules requiring context of multiple fields OpenAPI OpenAPI is the first layer a request flows through, and enforces a subset of validation rules: type format pattern enum For all endpoints, these validations always result in a 400 status code with errors when a rule is not fulfilled. These validations are located in the openapi.yml spec file. Why we avoid OpenAPI's required property To display a user friendly internationalized error message to Portal users, the error must include the field that is missing and a consistent type . For example: { \"errors\" : [ { \"field\" : \"password\" , \"type\" : \"required\" } ] } OpenAPI pitfalls: The main deal breaker is when using the OpenAPI's required property, the error response doesn't return the name of the specific field that is missing . It just returns the full array of fields that are required. This prevents Portal from displaying a useful error message. Some endpoints are split across multiple pages in the Portal so it will be expected that not all required fields are present at once in a request. Pydantic field validator Individual fields on a Pydantic model can have custom validation logic to enforce business rules or reality checks (e.g a birthdate must be in the past). These validators can raise a ValidationException to cause the API endpoint to respond with a 400 response with errors . Required fields and cross-field validations Our current approach is to use custom code is enforce the presence of fields, or more complex validation logic dependent on the values of multiple fields. The current convention for this set of validations is to: Create a *_rules.py module (e.g user_rules.py ) for the endpoints' validation logic Add get_*_issues method for validating an endpoint Call get_*_issues with the API request and check if there are any issues returned How Application validation works A get_application_submit_issues function exists for reporting potential \"warnings\" on an application at the time part 1 is submitted. These are primarily rules related to required or conditionally required fields, but may also relate to rules that span multiple fields. For GET and PATCH requests to the /applications/:application_id endpoint, these validations result in data still saving to our database and a 200 status code with warnings rather than errors , since we expect requests to not always have a complete application, since a user will be filling out the application through a multi-page experience. For POST requests to /applications/:application_id/* endpoints, these validations result in a 400 status code with errors when a rule is not fulfilled. These validations are located in application_rules.py .","title":"Fields and validations"},{"location":"api/fields-and-validations/#adding-fields","text":"Add the field to the DB model: db/models/applications.py Generate a new migration file See below for how to allow this field in API requests and responses.","title":"Adding fields"},{"location":"api/fields-and-validations/#accept-fields-in-requests","text":"Application fields should be nullable / Optional to support the Portal sending partial request bodies in its multi-page flows. Add the field to the openapi.yaml spec. Add the field to the requests.py Pydantic model Add test coverage to assert the new field is persisted in the DB.","title":"Accept fields in requests"},{"location":"api/fields-and-validations/#include-fields-in-responses","text":"Add the field to the openapi.yaml spec Add the field to the responses.py Pydantic model Add test coverage to assert the new field is included in responses.","title":"Include fields in responses"},{"location":"api/fields-and-validations/#validation-rules","text":"","title":"Validation rules"},{"location":"api/fields-and-validations/#validation-rules-vs-eligibility-rules","text":"We should distinguish validation rules from eligibility rules. It\u2019s one thing to automatically deny someone eligibility based on some eligibility criteria (like child birth date being within 12 months) and then allowing that person to appeal (if they have some valid extenuating circumstances), and it\u2019s an entirely different thing to prevent them from applying altogether claiming that the application isn\u2019t even a valid application. Validation should only serve to help prevent incorrect data (typos, etc) not prevent ineligible applications . We should be careful not to accidentally deny a claimant\u2019s right to an appeal or otherwise make the system less flexible than it needs to be as is so often the case with government systems.","title":"Validation Rules vs. Eligibility Rules"},{"location":"api/fields-and-validations/#adding-validation-rules","text":"Validation rules currently are enforced at three different layers: OpenAPI generally enforces a subset of rules on individual fields Pydantic models enforce some business rules on individual fields Custom code enforces presence of required fields or rules requiring context of multiple fields","title":"Adding validation rules"},{"location":"api/fields-and-validations/#openapi","text":"OpenAPI is the first layer a request flows through, and enforces a subset of validation rules: type format pattern enum For all endpoints, these validations always result in a 400 status code with errors when a rule is not fulfilled. These validations are located in the openapi.yml spec file. Why we avoid OpenAPI's required property To display a user friendly internationalized error message to Portal users, the error must include the field that is missing and a consistent type . For example: { \"errors\" : [ { \"field\" : \"password\" , \"type\" : \"required\" } ] } OpenAPI pitfalls: The main deal breaker is when using the OpenAPI's required property, the error response doesn't return the name of the specific field that is missing . It just returns the full array of fields that are required. This prevents Portal from displaying a useful error message. Some endpoints are split across multiple pages in the Portal so it will be expected that not all required fields are present at once in a request.","title":"OpenAPI"},{"location":"api/fields-and-validations/#pydantic-field-validator","text":"Individual fields on a Pydantic model can have custom validation logic to enforce business rules or reality checks (e.g a birthdate must be in the past). These validators can raise a ValidationException to cause the API endpoint to respond with a 400 response with errors .","title":"Pydantic field validator"},{"location":"api/fields-and-validations/#required-fields-and-cross-field-validations","text":"Our current approach is to use custom code is enforce the presence of fields, or more complex validation logic dependent on the values of multiple fields. The current convention for this set of validations is to: Create a *_rules.py module (e.g user_rules.py ) for the endpoints' validation logic Add get_*_issues method for validating an endpoint Call get_*_issues with the API request and check if there are any issues returned","title":"Required fields and cross-field validations"},{"location":"api/fields-and-validations/#how-application-validation-works","text":"A get_application_submit_issues function exists for reporting potential \"warnings\" on an application at the time part 1 is submitted. These are primarily rules related to required or conditionally required fields, but may also relate to rules that span multiple fields. For GET and PATCH requests to the /applications/:application_id endpoint, these validations result in data still saving to our database and a 200 status code with warnings rather than errors , since we expect requests to not always have a complete application, since a user will be filling out the application through a multi-page experience. For POST requests to /applications/:application_id/* endpoints, these validations result in a 400 status code with errors when a rule is not fulfilled. These validations are located in application_rules.py .","title":"How Application validation works"},{"location":"api/monitoring/","text":"API Monitoring We use New Relic (for performance monitoring, synthetics, and general queries) and AWS CloudWatch (for infrastructure-level concerns) to monitor the PFML API. We get CloudWatch metrics for free just by deploying the API on AWS Fargate, but the New Relic metrics are sourced from a Python agent that is initialized during server startup. AWS CloudWatch Alarms index SNS topics index Available CloudWatch metrics The API's AWS monitoring is defined in Terraform under infra/api/template/alarms.tf . When an alarm triggers, it will push an event to one or more SNS topics, also defined in alarms.tf . If configured to do so, SNS events can also be generated when an alarm stops alarming and returns to a normal state. To subscribe to AWS alerts, you can do one of two things. You can use the AWS console to create a temporary subscription to one or more SNS topics, but these subscriptions will only last until someone re-applies the canonical Terraform config. You can define a new aws_sns_topic_subscription object in alarms.tf . New Relic Main index Note: Includes applications not owned by Nava. Filter by pfml to see only the PFML API. Config variables and env-specific settings for New Relic, except for the license key, live inside newrelic.ini and container_definitions.json . See environment-variables.md for more about how env vars are pulled in at runtime. At this time, New Relic API monitoring is limited only to data collection. Alarms have not yet been configured here.","title":"API Monitoring"},{"location":"api/monitoring/#api-monitoring","text":"We use New Relic (for performance monitoring, synthetics, and general queries) and AWS CloudWatch (for infrastructure-level concerns) to monitor the PFML API. We get CloudWatch metrics for free just by deploying the API on AWS Fargate, but the New Relic metrics are sourced from a Python agent that is initialized during server startup.","title":"API Monitoring"},{"location":"api/monitoring/#aws-cloudwatch","text":"Alarms index SNS topics index Available CloudWatch metrics The API's AWS monitoring is defined in Terraform under infra/api/template/alarms.tf . When an alarm triggers, it will push an event to one or more SNS topics, also defined in alarms.tf . If configured to do so, SNS events can also be generated when an alarm stops alarming and returns to a normal state. To subscribe to AWS alerts, you can do one of two things. You can use the AWS console to create a temporary subscription to one or more SNS topics, but these subscriptions will only last until someone re-applies the canonical Terraform config. You can define a new aws_sns_topic_subscription object in alarms.tf .","title":"AWS CloudWatch"},{"location":"api/monitoring/#new-relic","text":"Main index Note: Includes applications not owned by Nava. Filter by pfml to see only the PFML API. Config variables and env-specific settings for New Relic, except for the license key, live inside newrelic.ini and container_definitions.json . See environment-variables.md for more about how env vars are pulled in at runtime. At this time, New Relic API monitoring is limited only to data collection. Alarms have not yet been configured here.","title":"New Relic"},{"location":"api/running-the-application-docker/","text":"Running the Application (Docker) Initializing system dependencies Common commands Managing the container environment Create application DB users Initializing system dependencies The application requires a running database with some minimum level of migrations already run as well as database users created and various other things. make init will perform the prep tasks necessary to get the application off the ground. Common commands make start # Start the API make logs # View API logs make login # Login to the container, where you can run development tools make login-db # Start a psql prompt in the container, where you can run SQL queries. requires make login. make build # Rebuild container and pick up new environment variables make stop # Stop all running containers Managing the container environment Under most circumstances, a local copy of the API will run with use_reloader=True , a flag given to Connexion that automatically restarts the the API's Python process if any modules have changed. Most of the time, the use_reloader flag should take care of things for you, but there are a few scenarios where you'll need to make a manual intervention in order to properly manage your container's state. If you're doing work on a local environment, please be aware that any changes to environment variables or config files will require a rebuild of the API's Docker image. Do this with make stop followed by make build. If you're only changing application code, you won't need to rebuild anything, unless you're changing code that runs before the connexion_app.run command in main .py . In this case only, you'll need to restart the Docker containers with make stop followed by make start . These scenarios are most relevant to developers who habitually work in DOCKER_EXEC mode, with long-lived application and DB containers. Create application DB users The migrations set up the DB roles and permissions. After the migrations have been run, actual DB users need to be connected for the application to use. This is typically done as part of the make init or make init-db commands and does not need to be run manually. make db-create-users","title":"Running the Application (Docker)"},{"location":"api/running-the-application-docker/#running-the-application-docker","text":"Initializing system dependencies Common commands Managing the container environment Create application DB users","title":"Running the Application (Docker)"},{"location":"api/running-the-application-docker/#initializing-system-dependencies","text":"The application requires a running database with some minimum level of migrations already run as well as database users created and various other things. make init will perform the prep tasks necessary to get the application off the ground.","title":"Initializing system dependencies"},{"location":"api/running-the-application-docker/#common-commands","text":"make start # Start the API make logs # View API logs make login # Login to the container, where you can run development tools make login-db # Start a psql prompt in the container, where you can run SQL queries. requires make login. make build # Rebuild container and pick up new environment variables make stop # Stop all running containers","title":"Common commands"},{"location":"api/running-the-application-docker/#managing-the-container-environment","text":"Under most circumstances, a local copy of the API will run with use_reloader=True , a flag given to Connexion that automatically restarts the the API's Python process if any modules have changed. Most of the time, the use_reloader flag should take care of things for you, but there are a few scenarios where you'll need to make a manual intervention in order to properly manage your container's state. If you're doing work on a local environment, please be aware that any changes to environment variables or config files will require a rebuild of the API's Docker image. Do this with make stop followed by make build. If you're only changing application code, you won't need to rebuild anything, unless you're changing code that runs before the connexion_app.run command in main .py . In this case only, you'll need to restart the Docker containers with make stop followed by make start . These scenarios are most relevant to developers who habitually work in DOCKER_EXEC mode, with long-lived application and DB containers.","title":"Managing the container environment"},{"location":"api/running-the-application-docker/#create-application-db-users","text":"The migrations set up the DB roles and permissions. After the migrations have been run, actual DB users need to be connected for the application to use. This is typically done as part of the make init or make init-db commands and does not need to be run manually. make db-create-users","title":"Create application DB users"},{"location":"api/security/","text":"API Security Notes and recommendations on securing the API against common types of attacks. SQL Injection SQL Injection SQL Injection is a type of attack where SQL commands are injected into data-plane input in order to modify the execution of predefined SQL commands. This type of attack can allow disclosure of all data on the system, as well as tampering and destruction of data. These types of attacks are considered high-impact severity. Mitigation SQL Injection attacks can be mitigated using defense-in-depth countermeasures, many of which can be (and are) implemented using existing tools in our stack. The following are recommendations for how we can use our existing tools to further mitigate this type of attack. Using OpenAPI specification Using the OpenAPI specification we define in openapi.yaml , we can configure the API to restrict user input, decreasing the opportunities for injection: - use the most specific data types possible when defining schema - when applicable, use enums to specify an allow-list of known values - use the pattern keyword for string inputs to exclude any unneeded (and potentially malicious) characters from input where appropriate ( < , > , = and ; , for example) Using Application Code Pydantic and SQLAlchemy provide features that can help mitigate SQL Injection that should be leveraged where SQL injection is possible: - Pydantic - use validators for pydantic model fields that originate as user input and are used to interact with the database - SQLAlchemy - avoid using user input in raw SQL; the SQLAlchemy Query API handles the escaping of special characters and using the input in prepared statements using parameterized queries Using Database Schema We define our database schema using SQLAlchemy models. It is recommended to use the most specific types available for data columns. This serves as another layer of user input validation, and should mitigate injections that attempt to modify data. Other Resources SQL Injection | OWASP SQL Injection Prevention - OWASP Cheat Sheet Series","title":"API Security"},{"location":"api/security/#api-security","text":"Notes and recommendations on securing the API against common types of attacks. SQL Injection","title":"API Security"},{"location":"api/security/#sql-injection","text":"SQL Injection is a type of attack where SQL commands are injected into data-plane input in order to modify the execution of predefined SQL commands. This type of attack can allow disclosure of all data on the system, as well as tampering and destruction of data. These types of attacks are considered high-impact severity.","title":"SQL Injection"},{"location":"api/security/#mitigation","text":"SQL Injection attacks can be mitigated using defense-in-depth countermeasures, many of which can be (and are) implemented using existing tools in our stack. The following are recommendations for how we can use our existing tools to further mitigate this type of attack.","title":"Mitigation"},{"location":"api/security/#using-openapi-specification","text":"Using the OpenAPI specification we define in openapi.yaml , we can configure the API to restrict user input, decreasing the opportunities for injection: - use the most specific data types possible when defining schema - when applicable, use enums to specify an allow-list of known values - use the pattern keyword for string inputs to exclude any unneeded (and potentially malicious) characters from input where appropriate ( < , > , = and ; , for example)","title":"Using OpenAPI specification"},{"location":"api/security/#using-application-code","text":"Pydantic and SQLAlchemy provide features that can help mitigate SQL Injection that should be leveraged where SQL injection is possible: - Pydantic - use validators for pydantic model fields that originate as user input and are used to interact with the database - SQLAlchemy - avoid using user input in raw SQL; the SQLAlchemy Query API handles the escaping of special characters and using the input in prepared statements using parameterized queries","title":"Using Application Code"},{"location":"api/security/#using-database-schema","text":"We define our database schema using SQLAlchemy models. It is recommended to use the most specific types available for data columns. This serves as another layer of user input validation, and should mitigate injections that attempt to modify data.","title":"Using Database Schema"},{"location":"api/security/#other-resources","text":"SQL Injection | OWASP SQL Injection Prevention - OWASP Cheat Sheet Series","title":"Other Resources"},{"location":"api/ses/","text":"Configuring ECS Tasks to send emails In certain cases, it may make sense to send emails from a running application. The instructions below list out the required steps for getting your ECS task configured with the appropriate permissions. Allow your task IAM Role to send emails By default, your ECS task does not have any AWS permissions. IAM roles define what they are allowed to do, with which resources (like an SES email!) There are two IAM roles for a task: the task IAM role and the execution IAM role. The task IAM role defines the permissions used during application runtime. The execution IAM role defines the permissions needed before the task starts. Since you'll be sending emails during an application's runtime, the task IAM role policy will need to be configured with permissions to send emails. This might look like this: statement { sid = \"AllowSESSendEmail\" effect = \"Allow\" actions = [ \"ses:SendEmail\" , \"ses:SendRawEmail\" ] condition { test = \"ForAllValues:StringLike\" variable = \"ses:Recipients\" values = [ # Any email addresses you expect to communicate with. ] } resources = [ \"*\" ] } Add your IAM role name to the SES allowlist Additionally, we maintain a strict list of resources that are allowed to send emails with each of our email addresses in ses.tf . You'll want to update the condition in data.aws_iam_policy_document.restrict_ses_senders to include your IAM roles in this pattern: arn : aws : sts :: $ { data.aws_caller_identity.current.account_id }: assumed-role/MY_IAM_ROLE_PATTERN /*\" Note that this is the name of the IAM role itself, not the policy. e.g. use the name in this block: resource \"aws_iam_role\" \"payments_fineos_process_task_role\" { name = \"${local.app_name}-${var.environment_name}-ecs-tasks-payments-fineos-process\" ... }","title":"Ses"},{"location":"api/ses/#configuring-ecs-tasks-to-send-emails","text":"In certain cases, it may make sense to send emails from a running application. The instructions below list out the required steps for getting your ECS task configured with the appropriate permissions.","title":"Configuring ECS Tasks to send emails"},{"location":"api/ses/#allow-your-task-iam-role-to-send-emails","text":"By default, your ECS task does not have any AWS permissions. IAM roles define what they are allowed to do, with which resources (like an SES email!) There are two IAM roles for a task: the task IAM role and the execution IAM role. The task IAM role defines the permissions used during application runtime. The execution IAM role defines the permissions needed before the task starts. Since you'll be sending emails during an application's runtime, the task IAM role policy will need to be configured with permissions to send emails. This might look like this: statement { sid = \"AllowSESSendEmail\" effect = \"Allow\" actions = [ \"ses:SendEmail\" , \"ses:SendRawEmail\" ] condition { test = \"ForAllValues:StringLike\" variable = \"ses:Recipients\" values = [ # Any email addresses you expect to communicate with. ] } resources = [ \"*\" ] }","title":"Allow your task IAM Role to send emails"},{"location":"api/ses/#add-your-iam-role-name-to-the-ses-allowlist","text":"Additionally, we maintain a strict list of resources that are allowed to send emails with each of our email addresses in ses.tf . You'll want to update the condition in data.aws_iam_policy_document.restrict_ses_senders to include your IAM roles in this pattern: arn : aws : sts :: $ { data.aws_caller_identity.current.account_id }: assumed-role/MY_IAM_ROLE_PATTERN /*\" Note that this is the name of the IAM role itself, not the policy. e.g. use the name in this block: resource \"aws_iam_role\" \"payments_fineos_process_task_role\" { name = \"${local.app_name}-${var.environment_name}-ecs-tasks-payments-fineos-process\" ... }","title":"Add your IAM role name to the SES allowlist"},{"location":"api/technical-overview/","text":"Technical Overview Key Technologies Request operations Authentication Authorization Running In Hosted Environments ECS Key Technologies The API is written in Python, utilizing Connexion as the web application framework (with Flask serving as the backend for Connexion). The API is described in the OpenAPI Specification format in the file openapi.yaml . SQLAlchemy is the ORM, with migrations driven by Alembic. pydantic is used in many spots for parsing data (and often serializing it to json or plain dictionaries). Where pydantic is not used, plain Python dataclasses are generally preferred. OpenAPI Specification Connexion ( source code ) SQLAlchemy ( source code ) Alembic ( source code ) pydantic ( source code ) poetry - Python dependency management Request operations OpenAPI spec ( openapi.yaml ) defines API interface: routes, requests, responses, etc. Connexion connects routes to handlers via operationId property on a route Connexion will run OAS validations before reaching handler Connexion will run authentication code and set user in the request context The handlers generally live in the top level files at massgov/pfml/api , with the operationId pointing to the specific module and function Handlers check if user in request context is authorized to perform the action the request represents Handlers use pydantic models to parse request bodies and construct response data Connexion will run OAS validations on response format from handler Authentication Authentication methods are defined in the securitySchemes block in openapi.yaml . A particular security scheme is enabled for a route via a security block on that route. Note the jwt scheme is enabled by default for every route via the global security block at the top of openapi.yaml . Connexion runs the authentication before passing the request to the route handler. In the jwt security scheme, the x-bearerInfoFunc points to the function that is run to do the authentication. Authorization Authorization is handled via the bouncer and flask-bouncer libraries. The rules are defined in massgov/pfml/api/authorization/rules.py , and request handlers use the ensure and can functions provided by flask-bouncer (re-exported in massgov/pfml/api/authorization/flask.py for use in the application, as we may eventually want to provide slightly modified versions of the functions provided by flask-bouncer directly) to check if the user in the current request context is authorized to perform the given action on the given subject. Running In Hosted Environments Application code gets run as tasks on AWS Elastic Container Service (ECS) clusters , backed by AWS Fargate For actually performing deploys, see /docs/deployment.md . ECS ECS tasks all share the same container as built by the app stage of the Dockerfile . This stage builds the project Python package (basically zip up massgov/ ) and installs it into the container environment. This also has the effect of making the package entrypoints as listed in the [tool.poetry.scripts] section of pyproject.toml available on PATH , so ECS tasks besides the Paid Leave API server tend to set one of these as their Docker command. The Paid Leave API server ECS task is declared in /infra/api/template/service.tf as well as its ECS service, which is a wrapper for ECS tasks to ensure a specified number of the tasks are running, provides mechanisms for autoscaling, and various other \"service\" features. Other ECS tasks are in /infra/ecs-tasks/template/tasks.tf . These are not a part of an ECS service, the tasks are launched/started on demand (either by an automated process like a scheduled AWS EventBridge event, Step Function, or manually through the /bin/run-ecs-task tool). Database diagram These are auto-generated as part of our api-create-erds GitHub action. Applications Employees Payments Verifications","title":"Technical Overview"},{"location":"api/technical-overview/#technical-overview","text":"Key Technologies Request operations Authentication Authorization Running In Hosted Environments ECS","title":"Technical Overview"},{"location":"api/technical-overview/#key-technologies","text":"The API is written in Python, utilizing Connexion as the web application framework (with Flask serving as the backend for Connexion). The API is described in the OpenAPI Specification format in the file openapi.yaml . SQLAlchemy is the ORM, with migrations driven by Alembic. pydantic is used in many spots for parsing data (and often serializing it to json or plain dictionaries). Where pydantic is not used, plain Python dataclasses are generally preferred. OpenAPI Specification Connexion ( source code ) SQLAlchemy ( source code ) Alembic ( source code ) pydantic ( source code ) poetry - Python dependency management","title":"Key Technologies"},{"location":"api/technical-overview/#request-operations","text":"OpenAPI spec ( openapi.yaml ) defines API interface: routes, requests, responses, etc. Connexion connects routes to handlers via operationId property on a route Connexion will run OAS validations before reaching handler Connexion will run authentication code and set user in the request context The handlers generally live in the top level files at massgov/pfml/api , with the operationId pointing to the specific module and function Handlers check if user in request context is authorized to perform the action the request represents Handlers use pydantic models to parse request bodies and construct response data Connexion will run OAS validations on response format from handler","title":"Request operations"},{"location":"api/technical-overview/#authentication","text":"Authentication methods are defined in the securitySchemes block in openapi.yaml . A particular security scheme is enabled for a route via a security block on that route. Note the jwt scheme is enabled by default for every route via the global security block at the top of openapi.yaml . Connexion runs the authentication before passing the request to the route handler. In the jwt security scheme, the x-bearerInfoFunc points to the function that is run to do the authentication.","title":"Authentication"},{"location":"api/technical-overview/#authorization","text":"Authorization is handled via the bouncer and flask-bouncer libraries. The rules are defined in massgov/pfml/api/authorization/rules.py , and request handlers use the ensure and can functions provided by flask-bouncer (re-exported in massgov/pfml/api/authorization/flask.py for use in the application, as we may eventually want to provide slightly modified versions of the functions provided by flask-bouncer directly) to check if the user in the current request context is authorized to perform the given action on the given subject.","title":"Authorization"},{"location":"api/technical-overview/#running-in-hosted-environments","text":"Application code gets run as tasks on AWS Elastic Container Service (ECS) clusters , backed by AWS Fargate For actually performing deploys, see /docs/deployment.md .","title":"Running In Hosted Environments"},{"location":"api/technical-overview/#ecs","text":"ECS tasks all share the same container as built by the app stage of the Dockerfile . This stage builds the project Python package (basically zip up massgov/ ) and installs it into the container environment. This also has the effect of making the package entrypoints as listed in the [tool.poetry.scripts] section of pyproject.toml available on PATH , so ECS tasks besides the Paid Leave API server tend to set one of these as their Docker command. The Paid Leave API server ECS task is declared in /infra/api/template/service.tf as well as its ECS service, which is a wrapper for ECS tasks to ensure a specified number of the tasks are running, provides mechanisms for autoscaling, and various other \"service\" features. Other ECS tasks are in /infra/ecs-tasks/template/tasks.tf . These are not a part of an ECS service, the tasks are launched/started on demand (either by an automated process like a scheduled AWS EventBridge event, Step Function, or manually through the /bin/run-ecs-task tool).","title":"ECS"},{"location":"api/technical-overview/#database-diagram","text":"These are auto-generated as part of our api-create-erds GitHub action.","title":"Database diagram"},{"location":"api/technical-overview/#applications","text":"","title":"Applications"},{"location":"api/technical-overview/#employees","text":"","title":"Employees"},{"location":"api/technical-overview/#payments","text":"","title":"Payments"},{"location":"api/technical-overview/#verifications","text":"","title":"Verifications"},{"location":"api/working-with-releases/","text":"Working With Releases More details about how to handle releases are available in the release runbook . As a part of the release process it is useful to include some technical notes on what the release includes. There is a make target to help automate some of this: make release-notes This will generate a list of the commits impacting an API release. For the commits that follow the project convention for commit messages, the Jira ticket will be linked. Everyone does not follow the convention nor will every commit have a Jira ticket associated. But this will provide a starting point. By default it will generate the list of commits that are different between what is deployed to stage (indicated by the deploy/api/stage branch) and what is on main . You can change the range of commits it considers by passing in refs , for example only looking for changes between release candidates: make release-notes refs = \"api/v1.3.0-rc1..api/v1.3.0-rc2\" The work will generally fall into one of a number of categories, with changes to: - ECS tasks for background jobs - The API service itself - CI tweaks It's useful to group the release notes broadly by these buckets to clarify what this particular release will impact. It's also usually useful to group the tickets by team, which piping to sort can help facilitate: make release-notes | sort Ultimately culminating in something like the notes for api/v1.3.0 . Figuring out what's released where There are a couple other make targets that could be useful. Note these all work off of your local git repo, so can only be as accurate as your local checkout is. You will generally want to run git fetch origin before these if you want the most up-to-date info. where-ticket will search the release branches for references to the provided ticket number: $ make where-ticket ticket = API-1000 ## origin/main ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/stage ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/prod ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/performance ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/training ## So in this example, API-1000 has been deployed to every environment but training . whats-released lists some info about the latest commits on the release branches: $ make whats-released ## origin/main ## * Closest tag: api/v1.3.0-rc2-48-g4465cfb72 * Latest commit: 4465cfb72 ( origin/main, main ) END-338: Convert employer response and remove notification checking ( #2386) ## origin/deploy/api/stage ## * Closest tag: api/v1.3.0 * Latest commit: 6e86eab29 ( tag: api/v1.3.0-rc3, tag: api/v1.3.0, origin/deploy/api/stage, origin/deploy/api/prod ) EMPLOYER-685 Add logging reqs to LA FINEOS registration script ( #2349) ## origin/deploy/api/prod ## * Closest tag: api/v1.3.0 * Latest commit: 6e86eab29 ( tag: api/v1.3.0-rc3, tag: api/v1.3.0, origin/deploy/api/stage, origin/deploy/api/prod ) EMPLOYER-685 Add logging reqs to LA FINEOS registration script ( #2349) ## origin/deploy/api/performance ## * Closest tag: api/v1.3.0-rc2 * Latest commit: 13ba0f2c3 ( tag: api/v1.3.0-rc2, origin/deploy/api/performance ) remove ecr scan github action ( #2333) ## origin/deploy/api/training ## * Closest tag: api/v1.1.0-rc1-48-ga6fb1f6bc * Latest commit: a6fb1f6bc ( origin/deploy/api/training ) API-999 Prod-Check Not Working as Expected ( #2322) So here can see api/v1.3.0 is on both stage and prod , performance is on api/v1.3.0-rc2 , and something a little ahead of api/v1.1.0-rc1 is on training .","title":"Working With Releases"},{"location":"api/working-with-releases/#working-with-releases","text":"More details about how to handle releases are available in the release runbook . As a part of the release process it is useful to include some technical notes on what the release includes. There is a make target to help automate some of this: make release-notes This will generate a list of the commits impacting an API release. For the commits that follow the project convention for commit messages, the Jira ticket will be linked. Everyone does not follow the convention nor will every commit have a Jira ticket associated. But this will provide a starting point. By default it will generate the list of commits that are different between what is deployed to stage (indicated by the deploy/api/stage branch) and what is on main . You can change the range of commits it considers by passing in refs , for example only looking for changes between release candidates: make release-notes refs = \"api/v1.3.0-rc1..api/v1.3.0-rc2\" The work will generally fall into one of a number of categories, with changes to: - ECS tasks for background jobs - The API service itself - CI tweaks It's useful to group the release notes broadly by these buckets to clarify what this particular release will impact. It's also usually useful to group the tickets by team, which piping to sort can help facilitate: make release-notes | sort Ultimately culminating in something like the notes for api/v1.3.0 .","title":"Working With Releases"},{"location":"api/working-with-releases/#figuring-out-whats-released-where","text":"There are a couple other make targets that could be useful. Note these all work off of your local git repo, so can only be as accurate as your local checkout is. You will generally want to run git fetch origin before these if you want the most up-to-date info. where-ticket will search the release branches for references to the provided ticket number: $ make where-ticket ticket = API-1000 ## origin/main ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/stage ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/prod ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/performance ## e7fb31752 API-1000: Do not add \"MA PFML - Limit\" plan to FINEOS service agreements ( #2272) ## origin/deploy/api/training ## So in this example, API-1000 has been deployed to every environment but training . whats-released lists some info about the latest commits on the release branches: $ make whats-released ## origin/main ## * Closest tag: api/v1.3.0-rc2-48-g4465cfb72 * Latest commit: 4465cfb72 ( origin/main, main ) END-338: Convert employer response and remove notification checking ( #2386) ## origin/deploy/api/stage ## * Closest tag: api/v1.3.0 * Latest commit: 6e86eab29 ( tag: api/v1.3.0-rc3, tag: api/v1.3.0, origin/deploy/api/stage, origin/deploy/api/prod ) EMPLOYER-685 Add logging reqs to LA FINEOS registration script ( #2349) ## origin/deploy/api/prod ## * Closest tag: api/v1.3.0 * Latest commit: 6e86eab29 ( tag: api/v1.3.0-rc3, tag: api/v1.3.0, origin/deploy/api/stage, origin/deploy/api/prod ) EMPLOYER-685 Add logging reqs to LA FINEOS registration script ( #2349) ## origin/deploy/api/performance ## * Closest tag: api/v1.3.0-rc2 * Latest commit: 13ba0f2c3 ( tag: api/v1.3.0-rc2, origin/deploy/api/performance ) remove ecr scan github action ( #2333) ## origin/deploy/api/training ## * Closest tag: api/v1.1.0-rc1-48-ga6fb1f6bc * Latest commit: a6fb1f6bc ( origin/deploy/api/training ) API-999 Prod-Check Not Working as Expected ( #2322) So here can see api/v1.3.0 is on both stage and prod , performance is on api/v1.3.0-rc2 , and something a little ahead of api/v1.1.0-rc1 is on training .","title":"Figuring out what's released where"},{"location":"api/writing-tests/","text":"Writing Tests pytest is our test runner, which is simple but powerful. If you are new to pytest, reading up on how fixtures work in particular might be helpful as it's one area that is a bit different than is common with other runners (and languages). Naming pytest automatically discovers tests by following a number of conventions (what it calls \"collection\"). For this project specifically: All tests live under tests/ Under tests/ , the organization mirrors the source code structure, but without the massgov/pfml/ part, so for example: The tests for massgov/pfml/api/ live under tests/api/ massgov/pfml/util/aws/ under test/util/aws/ . Create __init__.py files for each directory. This helps avoid name conflicts when pytest is resolving tests . Test files should begin with the test_ prefix, followed by the module the tests cover, for example, a file foo.py will have tests in a file test_foo.py . Tests for massgov/pfml/api/applications.py live at tests/api/test_applications.py Test cases should begin with the test_ prefix, followed by the function it's testing and some description of what about the function it is testing. In tests/api/test_users.py , the test_users_patch_404 function is a test (because it begins with test_ ), that covers the users_patch function's behavior around 404 responses. Tests can be grouped in classes starting with Test , methods that start with test_ will be picked up as tests, for example TestFeature::test_scenario . There are occasions where tests may not line up exactly with a single source file, function, or otherwise may need to deviate from this exact structure, but this is the setup in general. conftest files conftest.py files are automatically loaded by pytest, making their contents available to tests without needing to be imported. They are an easy place to put shared test fixtures as well as define other pytest configuration (define hooks, load plugins, define new/override assert behavior, etc.). They should never be imported directly. The main tests/conftest.py holds widely useful fixtures included for all tests. Scoped conftest.py files can be created that apply only to the tests below them in the directory hierarchy, for example, the tests/db/conftest.py file is only loaded for tests under tests/db/ . More info: https://docs.pytest.org/en/latest/how-to/fixtures.html?highlight=conftest#scope-sharing-fixtures-across-classes-modules-packages-or-session Helpers If there is useful functionality that needs to be shared between tests, but is only applicable to testing and is not a fixture, create modules under tests/helpers/ . They can be imported into tests from the path tests.helpers , for example, from tests.helpers.foo import helper_func . Using Factories To facilitate easier setup of test data, most database models have factories via factory_boy in massgov/pfml/db/models/factories.py . There are a few different ways of using the factories , termed \"strategies\": build, create, and stub. Most notably for this project: The build strategy via FooFactory.build() populates a model class with the generated data, but does not attempt to write it to the database The create strategy via FooFactory.create() writes a generated model to the database (can think of it like FooFactory.build() then db_session.add() and db_session.commit() ) The build strategy is useful if the code under test just needs the data on the model and doesn't actually perform any database interactions. In order to use the create strategy, pull in the initialize_factories_session fixture. Regardless of the strategy, can override the values for attributes on the generated models by passing them into the factory call, for example: FooFactory . build ( foo_id = 5 , name = \"Bar\" ) would set foo_id=5 and name=\"Bar\" on the generated model, while all other attributes would use what's configured on the factory class. For creating a collection of generated models, there are batch methods, build_batch() and create_batch() , which will create multiple instances. For example: FooFactory . build_batch ( size = 5 ) will return 5 Foo instances with different data. Attributes set in the batch call will be shared among all the instances, so: FooFactory . build_batch ( size = 2 , parent_widget = widget ) would create 2 Foo instances with widget set as their parent_widget . Integration test marker An integration marker is configured in pytest for the project. Any test that requires a real database connection or any concrete \"resource\" outside of the code itself should be tagged with the integration marker. It indicates an \"integration\" test, as opposed to a \"unit\" test, in a somewhat loose sense. A few common situations are easy cases, if a test is covering API behavior via fixtures like app or client or testing state in the database with test_db_session , the test should be marked with integration . Accessing real files is a bit of a gray area. If testing code that needs file-like objects, should generally prefer using in-memory constructs like StringIO or BytesIO to avoid ever touching the filesystem. But currently if a test needs to load test fixture files or use tmp_path to work with a real file for some purpose, those do not need to be tagged integration . In the vast majority of cases, the appropriate tests will get automatically tagged with the integration marker behind the scenes via the fixtures it uses. Automatic marking Test fixtures that use real resources should request the fixture has_external_dependencies . @pytest . fixture def some_fixture_using_real_resources ( has_external_dependencies ): ... Fixtures that have the has_external_dependencies fixture in their dependency graph do not need to explicitly request the has_external_dependencies fixture. All tests that request a fixture with has_external_dependencies in their dependency graph will be automatically marked with the integration marker. Manual marking To manually mark integration tests, decorate any individual test with @pytest.mark.integration . If all (or almost all) tests in a given test file are integration tests, they can be tagged all at once with a declaration like the following at the top of the file (after imports): # every test in here requires real resources pytestmark = pytest . mark . integration If a test file has a large mix of integration and unit tests that don't make sense to separate, integration tests can be bundled into a test class which can then be tagged, for example: @pytest . mark . integration class TestIntegrations : (but tagging each individual function with @pytest.mark.integration is also acceptable)","title":"Writing Tests"},{"location":"api/writing-tests/#writing-tests","text":"pytest is our test runner, which is simple but powerful. If you are new to pytest, reading up on how fixtures work in particular might be helpful as it's one area that is a bit different than is common with other runners (and languages).","title":"Writing Tests"},{"location":"api/writing-tests/#naming","text":"pytest automatically discovers tests by following a number of conventions (what it calls \"collection\"). For this project specifically: All tests live under tests/ Under tests/ , the organization mirrors the source code structure, but without the massgov/pfml/ part, so for example: The tests for massgov/pfml/api/ live under tests/api/ massgov/pfml/util/aws/ under test/util/aws/ . Create __init__.py files for each directory. This helps avoid name conflicts when pytest is resolving tests . Test files should begin with the test_ prefix, followed by the module the tests cover, for example, a file foo.py will have tests in a file test_foo.py . Tests for massgov/pfml/api/applications.py live at tests/api/test_applications.py Test cases should begin with the test_ prefix, followed by the function it's testing and some description of what about the function it is testing. In tests/api/test_users.py , the test_users_patch_404 function is a test (because it begins with test_ ), that covers the users_patch function's behavior around 404 responses. Tests can be grouped in classes starting with Test , methods that start with test_ will be picked up as tests, for example TestFeature::test_scenario . There are occasions where tests may not line up exactly with a single source file, function, or otherwise may need to deviate from this exact structure, but this is the setup in general.","title":"Naming"},{"location":"api/writing-tests/#conftest-files","text":"conftest.py files are automatically loaded by pytest, making their contents available to tests without needing to be imported. They are an easy place to put shared test fixtures as well as define other pytest configuration (define hooks, load plugins, define new/override assert behavior, etc.). They should never be imported directly. The main tests/conftest.py holds widely useful fixtures included for all tests. Scoped conftest.py files can be created that apply only to the tests below them in the directory hierarchy, for example, the tests/db/conftest.py file is only loaded for tests under tests/db/ . More info: https://docs.pytest.org/en/latest/how-to/fixtures.html?highlight=conftest#scope-sharing-fixtures-across-classes-modules-packages-or-session","title":"conftest files"},{"location":"api/writing-tests/#helpers","text":"If there is useful functionality that needs to be shared between tests, but is only applicable to testing and is not a fixture, create modules under tests/helpers/ . They can be imported into tests from the path tests.helpers , for example, from tests.helpers.foo import helper_func .","title":"Helpers"},{"location":"api/writing-tests/#using-factories","text":"To facilitate easier setup of test data, most database models have factories via factory_boy in massgov/pfml/db/models/factories.py . There are a few different ways of using the factories , termed \"strategies\": build, create, and stub. Most notably for this project: The build strategy via FooFactory.build() populates a model class with the generated data, but does not attempt to write it to the database The create strategy via FooFactory.create() writes a generated model to the database (can think of it like FooFactory.build() then db_session.add() and db_session.commit() ) The build strategy is useful if the code under test just needs the data on the model and doesn't actually perform any database interactions. In order to use the create strategy, pull in the initialize_factories_session fixture. Regardless of the strategy, can override the values for attributes on the generated models by passing them into the factory call, for example: FooFactory . build ( foo_id = 5 , name = \"Bar\" ) would set foo_id=5 and name=\"Bar\" on the generated model, while all other attributes would use what's configured on the factory class. For creating a collection of generated models, there are batch methods, build_batch() and create_batch() , which will create multiple instances. For example: FooFactory . build_batch ( size = 5 ) will return 5 Foo instances with different data. Attributes set in the batch call will be shared among all the instances, so: FooFactory . build_batch ( size = 2 , parent_widget = widget ) would create 2 Foo instances with widget set as their parent_widget .","title":"Using Factories"},{"location":"api/writing-tests/#integration-test-marker","text":"An integration marker is configured in pytest for the project. Any test that requires a real database connection or any concrete \"resource\" outside of the code itself should be tagged with the integration marker. It indicates an \"integration\" test, as opposed to a \"unit\" test, in a somewhat loose sense. A few common situations are easy cases, if a test is covering API behavior via fixtures like app or client or testing state in the database with test_db_session , the test should be marked with integration . Accessing real files is a bit of a gray area. If testing code that needs file-like objects, should generally prefer using in-memory constructs like StringIO or BytesIO to avoid ever touching the filesystem. But currently if a test needs to load test fixture files or use tmp_path to work with a real file for some purpose, those do not need to be tagged integration . In the vast majority of cases, the appropriate tests will get automatically tagged with the integration marker behind the scenes via the fixtures it uses.","title":"Integration test marker"},{"location":"api/writing-tests/#automatic-marking","text":"Test fixtures that use real resources should request the fixture has_external_dependencies . @pytest . fixture def some_fixture_using_real_resources ( has_external_dependencies ): ... Fixtures that have the has_external_dependencies fixture in their dependency graph do not need to explicitly request the has_external_dependencies fixture. All tests that request a fixture with has_external_dependencies in their dependency graph will be automatically marked with the integration marker.","title":"Automatic marking"},{"location":"api/writing-tests/#manual-marking","text":"To manually mark integration tests, decorate any individual test with @pytest.mark.integration . If all (or almost all) tests in a given test file are integration tests, they can be tagged all at once with a declaration like the following at the top of the file (after imports): # every test in here requires real resources pytestmark = pytest . mark . integration If a test file has a large mix of integration and unit tests that don't make sense to separate, integration tests can be bundled into a test class which can then be tagged, for example: @pytest . mark . integration class TestIntegrations : (but tagging each individual function with @pytest.mark.integration is also acceptable)","title":"Manual marking"},{"location":"infra/1-first-time-setup/","text":"First-time Setup If you're working with the infra directory, these steps are required before running terraform or test commands locally on your machine. 1. Configure AWS Since we manage AWS resources using Terraform, AWS credentials are needed to run terraform commands. Access to the AWS CLI is federated by AWS SSO, backed by Azure AD. Using AWS CLI: If you've already configured AWS SSO in your CLI, run aws sso login . Configuring AWS SSO For first-time setup, run aws configure sso using the following config: URL: https://coma.awsapps.com/start Region: us-east-1 It will open up a browser to complete sign-in. Log in using your Azure AD credentials. It will eventually direct you to close your browser. In your terminal, select a role and use defaults for the rest of the values. It will show a message like this: To use this profile, specify the profile name using --profile, as shown: aws s3 ls --profile eolwd-pfml-infrastructure-admin-498823821309 You can set the profile for all future commands using export AWS_PROFILE=eolwd-pfml-infrastructure-admin-498823821309 . This can be added by default to your .bashrc, .zshrc, or other appropriate tooling. Using the Browser Visit https://coma.awsapps.com/start and log in with your Azure AD credentials. The UI will provide CLI credentials via export commands that you can paste into your terminal. 2. Install Terraform Refer to the root-level README for instructions on installing terraform. 3. Optionally install NPM dependencies To locally run tests for JS lambdas, you'll also need to run the following with infra/ as the working directory: npm install Next Steps After getting set up with local tooling, it is recommended that you familiarize yourself with the technology stack we use and the general code structure before moving forward. Note that most of the infra/* subfolders also contain a high-level architectural code overview for navigation.","title":"First-time Setup"},{"location":"infra/1-first-time-setup/#first-time-setup","text":"If you're working with the infra directory, these steps are required before running terraform or test commands locally on your machine.","title":"First-time Setup"},{"location":"infra/1-first-time-setup/#1-configure-aws","text":"Since we manage AWS resources using Terraform, AWS credentials are needed to run terraform commands. Access to the AWS CLI is federated by AWS SSO, backed by Azure AD.","title":"1. Configure AWS"},{"location":"infra/1-first-time-setup/#using-aws-cli","text":"If you've already configured AWS SSO in your CLI, run aws sso login .","title":"Using AWS CLI:"},{"location":"infra/1-first-time-setup/#configuring-aws-sso","text":"For first-time setup, run aws configure sso using the following config: URL: https://coma.awsapps.com/start Region: us-east-1 It will open up a browser to complete sign-in. Log in using your Azure AD credentials. It will eventually direct you to close your browser. In your terminal, select a role and use defaults for the rest of the values. It will show a message like this: To use this profile, specify the profile name using --profile, as shown: aws s3 ls --profile eolwd-pfml-infrastructure-admin-498823821309 You can set the profile for all future commands using export AWS_PROFILE=eolwd-pfml-infrastructure-admin-498823821309 . This can be added by default to your .bashrc, .zshrc, or other appropriate tooling.","title":"Configuring AWS SSO"},{"location":"infra/1-first-time-setup/#using-the-browser","text":"Visit https://coma.awsapps.com/start and log in with your Azure AD credentials. The UI will provide CLI credentials via export commands that you can paste into your terminal.","title":"Using the Browser"},{"location":"infra/1-first-time-setup/#2-install-terraform","text":"Refer to the root-level README for instructions on installing terraform.","title":"2. Install Terraform"},{"location":"infra/1-first-time-setup/#3-optionally-install-npm-dependencies","text":"To locally run tests for JS lambdas, you'll also need to run the following with infra/ as the working directory: npm install","title":"3. Optionally install NPM dependencies"},{"location":"infra/1-first-time-setup/#next-steps","text":"After getting set up with local tooling, it is recommended that you familiarize yourself with the technology stack we use and the general code structure before moving forward. Note that most of the infra/* subfolders also contain a high-level architectural code overview for navigation.","title":"Next Steps"},{"location":"infra/2-testing-infra-changes/","text":"Testing Infrastructure changes Terraform Testing Currently, we do not use any infrastructure unit or integration testing tools like terratest and kitchen terraform as mentioned in Terraform: Module Testing Experiments . Additionally, we do not use validation enhancements such as Open Policy Agent (further reference here ), tflint , and tfsec . However, we do run terraform plan on affected modules whenever relevant code is updated. This is run using the Infra CI Validation Github Actions workflow (definition here ). This is a static validation for terraform formatting and module correctness, although certain domain-specific errors may occur when changes are actually applied. For most infrastructure changes, it is recommended that you follow the instructions for deploying your feature branch to the TEST environment in Deployment . For monitoring changes, follow the steps in Making Changes to Alerts . Common failure modes that are not caught by automated testing include: Invalid AWS configuration options Missing environment variables or parameter store secrets for a given environment Creating environment-bound resources that don't differentiate their name or configuration by environment Insufficient or incorrect IAM privileges granted to, or for, an arbitrary AWS resource (e.g. ECS task, lambda, CloudWatch rule, security group, S3 bucket...) JS Lambda Tests Some Portal infrastructure runs within lambda functions. These are associated with small test suites in the __tests__ directory which are run automatically on pull requests by the Portal Infra CI workflow. To run the test suite locally: npm test Update all Jest snapshots, accepting any updates as expected changes: npm run test:update-snapshot Run the project's test suite in watch mode: npm run test:watch By default, this will attempt to identify which tests to run based on which files have changed in the current repository. After running, you can interact with the prompt to configure or filter which test files are ran. For more details, see Portal Test Suite and Jest Snapshots . While the linked documentation is in the context of the Portal application, the same test libraries apply for JS lambda tests.","title":"Testing Infrastructure changes"},{"location":"infra/2-testing-infra-changes/#testing-infrastructure-changes","text":"","title":"Testing Infrastructure changes"},{"location":"infra/2-testing-infra-changes/#terraform-testing","text":"Currently, we do not use any infrastructure unit or integration testing tools like terratest and kitchen terraform as mentioned in Terraform: Module Testing Experiments . Additionally, we do not use validation enhancements such as Open Policy Agent (further reference here ), tflint , and tfsec . However, we do run terraform plan on affected modules whenever relevant code is updated. This is run using the Infra CI Validation Github Actions workflow (definition here ). This is a static validation for terraform formatting and module correctness, although certain domain-specific errors may occur when changes are actually applied. For most infrastructure changes, it is recommended that you follow the instructions for deploying your feature branch to the TEST environment in Deployment . For monitoring changes, follow the steps in Making Changes to Alerts . Common failure modes that are not caught by automated testing include: Invalid AWS configuration options Missing environment variables or parameter store secrets for a given environment Creating environment-bound resources that don't differentiate their name or configuration by environment Insufficient or incorrect IAM privileges granted to, or for, an arbitrary AWS resource (e.g. ECS task, lambda, CloudWatch rule, security group, S3 bucket...)","title":"Terraform Testing"},{"location":"infra/2-testing-infra-changes/#js-lambda-tests","text":"Some Portal infrastructure runs within lambda functions. These are associated with small test suites in the __tests__ directory which are run automatically on pull requests by the Portal Infra CI workflow. To run the test suite locally: npm test Update all Jest snapshots, accepting any updates as expected changes: npm run test:update-snapshot Run the project's test suite in watch mode: npm run test:watch By default, this will attempt to identify which tests to run based on which files have changed in the current repository. After running, you can interact with the prompt to configure or filter which test files are ran. For more details, see Portal Test Suite and Jest Snapshots . While the linked documentation is in the context of the Portal application, the same test libraries apply for JS lambda tests.","title":"JS Lambda Tests"},{"location":"infra/3-creating-ecs-tasks/","text":"Creating ECS Tasks This page describes the technical architecture for defining an ECS task. Creating a Barebones ECS Task Building the Entrypoint All ECS tasks rely on and eventually run one of the Poetry scripts defined in pyproject.toml . These scripts reference a specific entrypoint method in the codebase, e.g. # Run the \"up\" entrypoint method in massgov/pfml/db/migrations/run.py db-migrate-up = \"massgov.pfml.db.migrations.run:up\" To create a new entrypoint, it is sufficient to add a new method in the API codebase: # massgov/pfml/some/task.py from massgov.pfml.util.bg import background_task @background_task ( \"my-new-task-name\" ) def main (): pass and to reference it in a new Poetry script: my-new-task-name = \"massgov.pfml.some.task:main\" Creating the ECS Task Definition Once you have a poetry script, you can build the ECS task infrastructure following the instructions in tasks.tf . Scheduling and Triggers If you need to run the ECS task on a schedule or trigger it based off of an AWS S3 event, you can do so using the existing patterns in cloudwatch.tf and s3_subscriptions.tf . Testing Testing the Entrypoint The entrypoint is runnable locally using poetry, outside of the context of an ECS task, and is the fastest way to test functionality live without a deployment. Log into the docker container if needed with make login then run the following command: $ poetry run my-new-task-name This should run the main method and print some standardized log output. If your ECS task requires custom environment variables, you can provide it in the docker-compose.yml file temporarily. Personal AWS credentials can also be passed into the docker container following the instructions in docker-compose.override.yml . Testing the ECS Task All ECS tasks, including the Paid Leave API, are built on top of a shared set of code compiled into a single docker image. This docker image is built and pushed to ECR using make build-and-publish . In normal scenarios, the docker build process is managed by the API Deploy Github Actions workflow. However, if you need a tighter test loop, it is recommended that you follow the steps in Deployment to disable API TEST auto-deploys and do the docker image updates manually: # Build and publish the docker image to ECR $ make --directory=api build-and-publish # Update ECS tasks in the TEST environment to use the new docker image $ terraform -chdir=infra/ecs-tasks/environments/test/ apply --var service_docker_tag=$(git rev-parse HEAD) Once ECS tasks are updated in TEST, you can manually run your ECS task. We have a custom script to do this easily: $ ./bin/run-ecs-task/run-task.sh test my-new-task-name kevin.yeh If your task is scheduled or based on a trigger, you can wait for the task to run or do the appropriate action for triggering the task. If your schedule is infrequent, change it to a frequent schedule for testing. This is recommended to ensure that permissions are properly set up for the ECS task to run automatically based on a time or S3 event trigger. Viewing Task logs See Viewing ECS Task Logs .","title":"Creating ECS Tasks"},{"location":"infra/3-creating-ecs-tasks/#creating-ecs-tasks","text":"This page describes the technical architecture for defining an ECS task.","title":"Creating ECS Tasks"},{"location":"infra/3-creating-ecs-tasks/#creating-a-barebones-ecs-task","text":"","title":"Creating a Barebones ECS Task"},{"location":"infra/3-creating-ecs-tasks/#building-the-entrypoint","text":"All ECS tasks rely on and eventually run one of the Poetry scripts defined in pyproject.toml . These scripts reference a specific entrypoint method in the codebase, e.g. # Run the \"up\" entrypoint method in massgov/pfml/db/migrations/run.py db-migrate-up = \"massgov.pfml.db.migrations.run:up\" To create a new entrypoint, it is sufficient to add a new method in the API codebase: # massgov/pfml/some/task.py from massgov.pfml.util.bg import background_task @background_task ( \"my-new-task-name\" ) def main (): pass and to reference it in a new Poetry script: my-new-task-name = \"massgov.pfml.some.task:main\"","title":"Building the Entrypoint"},{"location":"infra/3-creating-ecs-tasks/#creating-the-ecs-task-definition","text":"Once you have a poetry script, you can build the ECS task infrastructure following the instructions in tasks.tf .","title":"Creating the ECS Task Definition"},{"location":"infra/3-creating-ecs-tasks/#scheduling-and-triggers","text":"If you need to run the ECS task on a schedule or trigger it based off of an AWS S3 event, you can do so using the existing patterns in cloudwatch.tf and s3_subscriptions.tf .","title":"Scheduling and Triggers"},{"location":"infra/3-creating-ecs-tasks/#testing","text":"","title":"Testing"},{"location":"infra/3-creating-ecs-tasks/#testing-the-entrypoint","text":"The entrypoint is runnable locally using poetry, outside of the context of an ECS task, and is the fastest way to test functionality live without a deployment. Log into the docker container if needed with make login then run the following command: $ poetry run my-new-task-name This should run the main method and print some standardized log output. If your ECS task requires custom environment variables, you can provide it in the docker-compose.yml file temporarily. Personal AWS credentials can also be passed into the docker container following the instructions in docker-compose.override.yml .","title":"Testing the Entrypoint"},{"location":"infra/3-creating-ecs-tasks/#testing-the-ecs-task","text":"All ECS tasks, including the Paid Leave API, are built on top of a shared set of code compiled into a single docker image. This docker image is built and pushed to ECR using make build-and-publish . In normal scenarios, the docker build process is managed by the API Deploy Github Actions workflow. However, if you need a tighter test loop, it is recommended that you follow the steps in Deployment to disable API TEST auto-deploys and do the docker image updates manually: # Build and publish the docker image to ECR $ make --directory=api build-and-publish # Update ECS tasks in the TEST environment to use the new docker image $ terraform -chdir=infra/ecs-tasks/environments/test/ apply --var service_docker_tag=$(git rev-parse HEAD) Once ECS tasks are updated in TEST, you can manually run your ECS task. We have a custom script to do this easily: $ ./bin/run-ecs-task/run-task.sh test my-new-task-name kevin.yeh If your task is scheduled or based on a trigger, you can wait for the task to run or do the appropriate action for triggering the task. If your schedule is infrequent, change it to a frequent schedule for testing. This is recommended to ensure that permissions are properly set up for the ECS task to run automatically based on a time or S3 event trigger.","title":"Testing the ECS Task"},{"location":"infra/3-creating-ecs-tasks/#viewing-task-logs","text":"See Viewing ECS Task Logs .","title":"Viewing Task logs"},{"location":"infra/4-viewing-ecs-task-logs/","text":"Viewing ECS Task logs The logs for all background ECS tasks are sent to Cloudwatch Logs and New Relic. There are two distinct methods that people use: Streaming logs to your terminal Lots of folks use saw and pipe it through a developer utility for pretty-printing logs: saw watch --raw service/pfml-api-test/ecs-tasks | python3 api/massgov/pfml/util/logging/decodelog.py You can also use the AWS CLI v2, which has built in support for tailing Cloudwatch logs: aws logs tail service/pfml-api-test/ecs-tasks --follow Common log groups that may be useful: Log Group Pattern Description service/pfml-api-ENV Application logs from the PFML API ECS tasks service/pfml-api-ENV/ecs-tasks Application logs from all background job ECS tasks service/pfml-api-ENV/ecs-tasks/events State change events from all ECS tasks Searching logs in New Relic Alternatively, you can use the New Relic Logs UI to search and filter for logs. See New Relic's documentation for more information on the query syntax for log data. There is a saved view for ECS task logs in TEST , and you can further filter based on the task name: aws.logStream:*my-new-task-name* Viewing start failure reasons If your task does not start successfully, there will be no logs. Instead, there will be a Slack notification in #mass-pfml-pd-warnings with a description of your error. These failure messages will also be available for up to an hour in the ECS AWS console , and more indefinitely in the state change logs stored under service/pfml-api-ENV/ecs-tasks/events . Use one of the methods above to view logs in this log group.","title":"Viewing ECS Task logs"},{"location":"infra/4-viewing-ecs-task-logs/#viewing-ecs-task-logs","text":"The logs for all background ECS tasks are sent to Cloudwatch Logs and New Relic. There are two distinct methods that people use:","title":"Viewing ECS Task logs"},{"location":"infra/4-viewing-ecs-task-logs/#streaming-logs-to-your-terminal","text":"Lots of folks use saw and pipe it through a developer utility for pretty-printing logs: saw watch --raw service/pfml-api-test/ecs-tasks | python3 api/massgov/pfml/util/logging/decodelog.py You can also use the AWS CLI v2, which has built in support for tailing Cloudwatch logs: aws logs tail service/pfml-api-test/ecs-tasks --follow Common log groups that may be useful: Log Group Pattern Description service/pfml-api-ENV Application logs from the PFML API ECS tasks service/pfml-api-ENV/ecs-tasks Application logs from all background job ECS tasks service/pfml-api-ENV/ecs-tasks/events State change events from all ECS tasks","title":"Streaming logs to your terminal"},{"location":"infra/4-viewing-ecs-task-logs/#searching-logs-in-new-relic","text":"Alternatively, you can use the New Relic Logs UI to search and filter for logs. See New Relic's documentation for more information on the query syntax for log data. There is a saved view for ECS task logs in TEST , and you can further filter based on the task name: aws.logStream:*my-new-task-name*","title":"Searching logs in New Relic"},{"location":"infra/4-viewing-ecs-task-logs/#viewing-start-failure-reasons","text":"If your task does not start successfully, there will be no logs. Instead, there will be a Slack notification in #mass-pfml-pd-warnings with a description of your error. These failure messages will also be available for up to an hour in the ECS AWS console , and more indefinitely in the state change logs stored under service/pfml-api-ENV/ecs-tasks/events . Use one of the methods above to view logs in this log group.","title":"Viewing start failure reasons"},{"location":"infra/5-making-changes-to-alerts/","text":"Making Changes to Alerts Application monitors and alerts are managed via Terraform in the infra/monitoring folder. This folder accounts for monitoring across all environments and is automatically deployed when changes are merged into the main branch. Alarm Modules The core code for building Cloudwatch and New Relic alerts are stored in re-usable modules under the following directories: infra/modules/alarms_api infra/modules/alarms_portal Testing Monitoring Changes in TEST It's often desirable to test changes to monitoring in the TEST environment during feature development. Since monitoring changes for all environments are combined into a single configuration, terraform actions must specifically target certain resources. Almost all alarms are built through the infra/modules/alarms_api and infra/modules/alarms_portal modules, which are used within infra/monitoring/alarms.tf : module \"alarms_api\" { for_each = toset ( local.environments ) source = \"../modules/alarms_api\" ... } To update the set of alarms for a single environment, go into the infra/monitoring root module and target the alarms module with a particular key: $ terraform apply --target module.alarms_api [ \\\" test \\\" ] --target module.alarms_portal [ \\\" test \\\" ] # or $ terraform apply -target 'module.alarms_api[\"test\"]' -target 'module.alarms_portal[\"test\"]' The terraform plan should indicate that only test alarms will be updated. Resources Here is a list of external documentation resources that could prove useful while making changes: Building New Relic Alarms - Kevin Yeh - Confluence Streaming alerts: key terms and concepts | New Relic Documentation Create NRQL alert conditions | New Relic Documenation Docs overview | newrelic/newrelic | Terraform Registry Getting Access The documentation here assumes the reader has access to AWS and has Terraform configured locally. Additionally, it is useful to have access to New Relic to view and test queries. View the Tools Access documentation for instructions on how to get access, then see the First Time Setup for setup instructions.","title":"Making Changes to Alerts"},{"location":"infra/5-making-changes-to-alerts/#making-changes-to-alerts","text":"Application monitors and alerts are managed via Terraform in the infra/monitoring folder. This folder accounts for monitoring across all environments and is automatically deployed when changes are merged into the main branch.","title":"Making Changes to Alerts"},{"location":"infra/5-making-changes-to-alerts/#alarm-modules","text":"The core code for building Cloudwatch and New Relic alerts are stored in re-usable modules under the following directories: infra/modules/alarms_api infra/modules/alarms_portal","title":"Alarm Modules"},{"location":"infra/5-making-changes-to-alerts/#testing-monitoring-changes-in-test","text":"It's often desirable to test changes to monitoring in the TEST environment during feature development. Since monitoring changes for all environments are combined into a single configuration, terraform actions must specifically target certain resources. Almost all alarms are built through the infra/modules/alarms_api and infra/modules/alarms_portal modules, which are used within infra/monitoring/alarms.tf : module \"alarms_api\" { for_each = toset ( local.environments ) source = \"../modules/alarms_api\" ... } To update the set of alarms for a single environment, go into the infra/monitoring root module and target the alarms module with a particular key: $ terraform apply --target module.alarms_api [ \\\" test \\\" ] --target module.alarms_portal [ \\\" test \\\" ] # or $ terraform apply -target 'module.alarms_api[\"test\"]' -target 'module.alarms_portal[\"test\"]' The terraform plan should indicate that only test alarms will be updated.","title":"Testing Monitoring Changes in TEST"},{"location":"infra/5-making-changes-to-alerts/#resources","text":"Here is a list of external documentation resources that could prove useful while making changes: Building New Relic Alarms - Kevin Yeh - Confluence Streaming alerts: key terms and concepts | New Relic Documentation Create NRQL alert conditions | New Relic Documenation Docs overview | newrelic/newrelic | Terraform Registry","title":"Resources"},{"location":"infra/5-making-changes-to-alerts/#getting-access","text":"The documentation here assumes the reader has access to AWS and has Terraform configured locally. Additionally, it is useful to have access to New Relic to view and test queries. View the Tools Access documentation for instructions on how to get access, then see the First Time Setup for setup instructions.","title":"Getting Access"},{"location":"infra/6-github-action-workflows/","text":"Github Action Workflows Github Action workflows are defined in .github/workflows . Github has helpful tutorials for learning syntax and usage. Testing workflows When creating or updating Github Action workflows, it is often difficult to test them due to how workflows are built and triggered. For this reason, it is recommended that every workflow includes an on: workflow_dispatch trigger, which will allow anyone with write access to the repository the ability to run the workflow manually in the UI once it's in the main branch. If you are editing an existing workflow with a workflow_dispatch trigger, you can run the workflow at any time using your feature branch in the UI: Github Actions Select the workflow from the sidebar Select your branch and fill in any inputs Select 'Run Workflow'. If you are creating a new workflow or editing one that does not allow this trigger in the main branch, the easiest way to test the workflow is to add an on: push trigger. This will cause the workflow to run on every push to the feature branch. You can then push \"dummy\" commits to force it to run. git commit --allow-empty If you are testing in this way, please provide testing documentation and results in the body of your pull request for historical purposes, and so reviewers can follow along.","title":"Github Action Workflows"},{"location":"infra/6-github-action-workflows/#github-action-workflows","text":"Github Action workflows are defined in .github/workflows . Github has helpful tutorials for learning syntax and usage.","title":"Github Action Workflows"},{"location":"infra/6-github-action-workflows/#testing-workflows","text":"When creating or updating Github Action workflows, it is often difficult to test them due to how workflows are built and triggered. For this reason, it is recommended that every workflow includes an on: workflow_dispatch trigger, which will allow anyone with write access to the repository the ability to run the workflow manually in the UI once it's in the main branch. If you are editing an existing workflow with a workflow_dispatch trigger, you can run the workflow at any time using your feature branch in the UI: Github Actions Select the workflow from the sidebar Select your branch and fill in any inputs Select 'Run Workflow'. If you are creating a new workflow or editing one that does not allow this trigger in the main branch, the easiest way to test the workflow is to add an on: push trigger. This will cause the workflow to run on every push to the feature branch. You can then push \"dummy\" commits to force it to run. git commit --allow-empty If you are testing in this way, please provide testing documentation and results in the body of your pull request for historical purposes, and so reviewers can follow along.","title":"Testing workflows"},{"location":"infra/7-environment-management/","text":"Environment Management Setting up a new environment \ud83d\udd17 See docs/creating-environments.md for instructions on how to create a new environment. tfstate files Each environment for a component has a .tfstate file that is stored in S3 and synchronized using a DynamoDB lock table. Terraform relies on this state file for every command and must acquire the DynamoDB lock in order to use it, so only one person or system can run a terraform command at a time. S3 \u2514\u2500\u2500 massgov-pfml-aws-account-mgmt \u2514\u2500\u2500 terraform \u2514\u2500\u2500 aws.tfstate \u2514\u2500\u2500 monitoring.tfstate \u2514\u2500\u2500 massgov-pfml-test-env-mgmt \u2514\u2500\u2500 terraform \u2514\u2500\u2500 env-shared.tfstate \u2514\u2500\u2500 portal.tfstate \u2514\u2500\u2500 api.tfstate \u2514\u2500\u2500 ecs-tasks.tfstate \u2514\u2500\u2500 ... Note that modules like constants , modules/ecs_task_scheduler , etc. are not deployed on their own and therefore do not have their own tfstate files. These modules are used within deployable modules like monitoring and api .","title":"Environment Management"},{"location":"infra/7-environment-management/#environment-management","text":"","title":"Environment Management"},{"location":"infra/7-environment-management/#setting-up-a-new-environment","text":"\ud83d\udd17 See docs/creating-environments.md for instructions on how to create a new environment.","title":"Setting up a new environment"},{"location":"infra/7-environment-management/#tfstate-files","text":"Each environment for a component has a .tfstate file that is stored in S3 and synchronized using a DynamoDB lock table. Terraform relies on this state file for every command and must acquire the DynamoDB lock in order to use it, so only one person or system can run a terraform command at a time. S3 \u2514\u2500\u2500 massgov-pfml-aws-account-mgmt \u2514\u2500\u2500 terraform \u2514\u2500\u2500 aws.tfstate \u2514\u2500\u2500 monitoring.tfstate \u2514\u2500\u2500 massgov-pfml-test-env-mgmt \u2514\u2500\u2500 terraform \u2514\u2500\u2500 env-shared.tfstate \u2514\u2500\u2500 portal.tfstate \u2514\u2500\u2500 api.tfstate \u2514\u2500\u2500 ecs-tasks.tfstate \u2514\u2500\u2500 ... Note that modules like constants , modules/ecs_task_scheduler , etc. are not deployed on their own and therefore do not have their own tfstate files. These modules are used within deployable modules like monitoring and api .","title":"tfstate files"},{"location":"infra/extras/adding-ses-email-address/","text":"Adding a new SES email address In order to send emails from an SES email address, the email must first be verified. Ensure you have someone who can access the inbox of the email you'll be setting, so they can verify it. Creating the email in Terraform Create an ses_email_address resource. Run terraform apply , which will likely return an error indicating you need to verify the email address. Verify the email address by clicking the link in the verification email that should have been sent after the previous step. Run terraform apply again, which should be successful this time. Creating the email in AWS Console Alternatively, you can create the email in the AWS Console and verify it: Add the email to SES through the AWS Console Verify the email address by clicking the link in the verification email that should have been sent after the previous step. Import the SES email resource: terraform import aws_ses_email_identity.example email@example.com","title":"Adding a new SES email address"},{"location":"infra/extras/adding-ses-email-address/#adding-a-new-ses-email-address","text":"In order to send emails from an SES email address, the email must first be verified. Ensure you have someone who can access the inbox of the email you'll be setting, so they can verify it.","title":"Adding a new SES email address"},{"location":"infra/extras/adding-ses-email-address/#creating-the-email-in-terraform","text":"Create an ses_email_address resource. Run terraform apply , which will likely return an error indicating you need to verify the email address. Verify the email address by clicking the link in the verification email that should have been sent after the previous step. Run terraform apply again, which should be successful this time.","title":"Creating the email in Terraform"},{"location":"infra/extras/adding-ses-email-address/#creating-the-email-in-aws-console","text":"Alternatively, you can create the email in the AWS Console and verify it: Add the email to SES through the AWS Console Verify the email address by clicking the link in the verification email that should have been sent after the previous step. Import the SES email resource: terraform import aws_ses_email_identity.example email@example.com","title":"Creating the email in AWS Console"},{"location":"infra/extras/testing-github-action-permissions/","text":"Testing Github Actions permissions Since Github Actions has different permissions than developers and admins, it's useful to test terraform configs using our CI/CD role so we know that they can be run on Github Actions with the right read/write permissions. This is recommended if you're adding a new service into our ecosystem. Ensure you have the AWS CLI: pip install awscli Generate a session: aws sts assume-role --role-arn arn:aws:iam::498823821309:role/ci-run-deploys --role-session-name <any-session-name> Copy the access key, secret, and session token into your ~/.aws/credentials under a new profile so it looks like this: [ci-run-deploys] aws_access_key_id = 123 aws_secret_access_key = 456 aws_session_token = 789 Use the profile: export AWS_PROFILE=ci-run-deploys Run terraform as usual. Alternatives to steps 3-4 are using environment variables or using the AWS CLI ( aws configure ).","title":"Testing Github Actions permissions"},{"location":"infra/extras/testing-github-action-permissions/#testing-github-actions-permissions","text":"Since Github Actions has different permissions than developers and admins, it's useful to test terraform configs using our CI/CD role so we know that they can be run on Github Actions with the right read/write permissions. This is recommended if you're adding a new service into our ecosystem. Ensure you have the AWS CLI: pip install awscli Generate a session: aws sts assume-role --role-arn arn:aws:iam::498823821309:role/ci-run-deploys --role-session-name <any-session-name> Copy the access key, secret, and session token into your ~/.aws/credentials under a new profile so it looks like this: [ci-run-deploys] aws_access_key_id = 123 aws_secret_access_key = 456 aws_session_token = 789 Use the profile: export AWS_PROFILE=ci-run-deploys Run terraform as usual. Alternatives to steps 3-4 are using environment variables or using the AWS CLI ( aws configure ).","title":"Testing Github Actions permissions"},{"location":"portal/browser-support/","text":"Browser support Portal aims to support all modern browsers (Edge, Firefox, Chrome, Safari, Opera, et al). Portal also aims to support Internet Explorer 11. Portal does not support Internet Explorer version 10 or below. Read more about this in Confluence . Read more about Next.js browser support here . Things to be aware of A browserslist file informs our CSS build process which browsers we support. Learn more . Next.js automatically injects polyfills required for IE11 compatibility of our source code, however can't do this for any NPM dependencies that lack IE 11 compatibility. As a result, we have some manual polyfills in src/polyfills.js For all users of Internet Explorer, we display a banner at the top of the site to communicate their browser is not fully supported. See UnsupportedBrowserBanner.js Manual testing in Internet Explorer To test the Portal locally using Internet Explorer 11 in BrowserStack or Sauce Labs, you will need to temporarily disable Amplify's cookie storage config setting in _app.js by commenting out the cookieStorage object. To test in IE 10 or below, you will need to build and export the static site: $ npm build $ npm start These requirements are unique to local development.","title":"Browser support"},{"location":"portal/browser-support/#browser-support","text":"Portal aims to support all modern browsers (Edge, Firefox, Chrome, Safari, Opera, et al). Portal also aims to support Internet Explorer 11. Portal does not support Internet Explorer version 10 or below. Read more about this in Confluence . Read more about Next.js browser support here .","title":"Browser support"},{"location":"portal/browser-support/#things-to-be-aware-of","text":"A browserslist file informs our CSS build process which browsers we support. Learn more . Next.js automatically injects polyfills required for IE11 compatibility of our source code, however can't do this for any NPM dependencies that lack IE 11 compatibility. As a result, we have some manual polyfills in src/polyfills.js For all users of Internet Explorer, we display a banner at the top of the site to communicate their browser is not fully supported. See UnsupportedBrowserBanner.js","title":"Things to be aware of"},{"location":"portal/browser-support/#manual-testing-in-internet-explorer","text":"To test the Portal locally using Internet Explorer 11 in BrowserStack or Sauce Labs, you will need to temporarily disable Amplify's cookie storage config setting in _app.js by commenting out the cookieStorage object. To test in IE 10 or below, you will need to build and export the static site: $ npm build $ npm start These requirements are unique to local development.","title":"Manual testing in Internet Explorer"},{"location":"portal/creating-environments/","text":"Setting up a new environment This docs covers the steps you need to take to configure the Portal web app to support a new environment. Before reading this, make sure you've already ran through the steps to setup the infrastructure for a new environment . Add environment variables Create a new config file for the environment in portal/config , copying the properties from another environment. Update the variables for the environment. Primarily, you'll want to update the environment name and the Cognito IDs, which should have been output after you ran terraform apply e.g.: module.massgov_pfml.aws_cognito_user_pool.claimants_pool: Creation complete after 2s [id=us-east-1_Bi6tPV5hz] ... module.massgov_pfml.aws_cognito_user_pool_client.massgov_pfml_client: Creation complete after 0s [id=606qc6fmb1sn3pcujrav20h66l] Set up a New Relic Browser app. Go to New Relic > Add More Data > New Relic Browser. Select Copy/Paste Javascript Code Select Pro + SPA Enable cookies and distributed tracing Click CONTINUE Copy only the applicationId from the JS snippet and add it to the config file you created. See Monitoring and Web Analytics READMEs for more details about how New Relic and Google Tag Manager are configured. Add an entry for the new environment to portal/config/index.js , copying the properties from another environment. Make sure the associated API environment's origin URL is listed in the list of allowed_origins in new-relic.js . Add a build script Each environment should have its own build script in portal/package.json so that the site can be built using the correct environment variables. Refer to other build scripts, like build:test for an example. Update GitHub Actions Each environment will have its own GitHub branch that will deploy when changes are pushed. Create a new deploy branch (e.g. deploy/portal/breakfix ) and update the deployment doc with details about deploying to the new environment. Once all of this is set up, run a real deployment using the steps in deployment .","title":"Setting up a new environment"},{"location":"portal/creating-environments/#setting-up-a-new-environment","text":"This docs covers the steps you need to take to configure the Portal web app to support a new environment. Before reading this, make sure you've already ran through the steps to setup the infrastructure for a new environment .","title":"Setting up a new environment"},{"location":"portal/creating-environments/#add-environment-variables","text":"Create a new config file for the environment in portal/config , copying the properties from another environment. Update the variables for the environment. Primarily, you'll want to update the environment name and the Cognito IDs, which should have been output after you ran terraform apply e.g.: module.massgov_pfml.aws_cognito_user_pool.claimants_pool: Creation complete after 2s [id=us-east-1_Bi6tPV5hz] ... module.massgov_pfml.aws_cognito_user_pool_client.massgov_pfml_client: Creation complete after 0s [id=606qc6fmb1sn3pcujrav20h66l] Set up a New Relic Browser app. Go to New Relic > Add More Data > New Relic Browser. Select Copy/Paste Javascript Code Select Pro + SPA Enable cookies and distributed tracing Click CONTINUE Copy only the applicationId from the JS snippet and add it to the config file you created. See Monitoring and Web Analytics READMEs for more details about how New Relic and Google Tag Manager are configured. Add an entry for the new environment to portal/config/index.js , copying the properties from another environment. Make sure the associated API environment's origin URL is listed in the list of allowed_origins in new-relic.js .","title":"Add environment variables"},{"location":"portal/creating-environments/#add-a-build-script","text":"Each environment should have its own build script in portal/package.json so that the site can be built using the correct environment variables. Refer to other build scripts, like build:test for an example.","title":"Add a build script"},{"location":"portal/creating-environments/#update-github-actions","text":"Each environment will have its own GitHub branch that will deploy when changes are pushed. Create a new deploy branch (e.g. deploy/portal/breakfix ) and update the deployment doc with details about deploying to the new environment. Once all of this is set up, run a real deployment using the steps in deployment .","title":"Update GitHub Actions"},{"location":"portal/development/","text":"Portal Development This page covers development practices for working on the Mass PFML Portal. Please document liberally so that others may benefit from your learning. TODO comments Our linter enforces that TODO comments include a reference to a Jira ticket that tracks the work to implement the TODO. The expected format of a TODO is: TODO (CP-123): Message ...where CP-123 is the Jira ticket number. Why? We want to avoid losing track of work that we haven't completed, and unintentionally ship an application that doesn't work end-to-end. Making sure that we have tickets in our backlog is one major way we can avoid losing track of this work. In addition, having ticket references within TODO comments provides an engineer a way to learn additional context, and potentially learn that the work has already been completed, but the TODO got left behind in the code by accident. Creating a page All files in the portal/src/pages directory are automatically available as routes based on their name, e.g. about.js is routed to /about . Files named index.js are routed to the root of the directory. See more at the Next.js docs on routing and pages . For Employer-specific pages, files will be nested in an /employers subdirectory. Each time you add a new page, add a new route to src/routes.js . Add content strings for the page to src/locales/app/en-US.js . Add a test file for the page (and for any new components) to tests Question Pages We use a state machine to control routing between question pages in a flow. The library we use behind the scenes for this is XState . Add a new state node for the route to src/flows/claimaint.js , and add a CONTINUE transition. Read more on XState configs here . Within a meta object on the state node, add a step and fields properties. The fields value should contain the field paths for every field that may be displayed on the question page. This is important because this array is what's used for determining what validation errors should display on the question page, and also is used for identifying when a step on the checklist is In Progress or Completed. If the page should display validation issues for specific rules, add an array of applicableRules to the state's meta object. If routing to or from the page is conditional, you'll need to define guards that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/claimant.test.js If routing is conditional, add items with appropriate data to the testData array. Auth Pages Routing Similar to Question Pages, pages used in the Auth flow are also in the /portal/src/pages subdirectory, but routing is controlled using the state machine. Add a new state for the route to src/flows/auth.js . Include a CONTINUE transition. If routing to or from the page is conditional, you'll need to define guard s that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/auth.test.js If routing is conditional, add items with appropriate data to the testData array. next.config.js The next.config.js file is a Node.js module that can be used to configure build and export behavior, such as Webpack settings. Frontend <> Backend Configuration By default, portal is set up to connect to the backend staging environment. If you wish to connect local to local, please see detailed instructions here .","title":"Portal Development"},{"location":"portal/development/#portal-development","text":"This page covers development practices for working on the Mass PFML Portal. Please document liberally so that others may benefit from your learning.","title":"Portal Development"},{"location":"portal/development/#todo-comments","text":"Our linter enforces that TODO comments include a reference to a Jira ticket that tracks the work to implement the TODO. The expected format of a TODO is: TODO (CP-123): Message ...where CP-123 is the Jira ticket number. Why? We want to avoid losing track of work that we haven't completed, and unintentionally ship an application that doesn't work end-to-end. Making sure that we have tickets in our backlog is one major way we can avoid losing track of this work. In addition, having ticket references within TODO comments provides an engineer a way to learn additional context, and potentially learn that the work has already been completed, but the TODO got left behind in the code by accident.","title":"TODO comments"},{"location":"portal/development/#creating-a-page","text":"All files in the portal/src/pages directory are automatically available as routes based on their name, e.g. about.js is routed to /about . Files named index.js are routed to the root of the directory. See more at the Next.js docs on routing and pages . For Employer-specific pages, files will be nested in an /employers subdirectory. Each time you add a new page, add a new route to src/routes.js . Add content strings for the page to src/locales/app/en-US.js . Add a test file for the page (and for any new components) to tests","title":"Creating a page"},{"location":"portal/development/#question-pages","text":"We use a state machine to control routing between question pages in a flow. The library we use behind the scenes for this is XState . Add a new state node for the route to src/flows/claimaint.js , and add a CONTINUE transition. Read more on XState configs here . Within a meta object on the state node, add a step and fields properties. The fields value should contain the field paths for every field that may be displayed on the question page. This is important because this array is what's used for determining what validation errors should display on the question page, and also is used for identifying when a step on the checklist is In Progress or Completed. If the page should display validation issues for specific rules, add an array of applicableRules to the state's meta object. If routing to or from the page is conditional, you'll need to define guards that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/claimant.test.js If routing is conditional, add items with appropriate data to the testData array.","title":"Question Pages"},{"location":"portal/development/#auth-pages-routing","text":"Similar to Question Pages, pages used in the Auth flow are also in the /portal/src/pages subdirectory, but routing is controlled using the state machine. Add a new state for the route to src/flows/auth.js . Include a CONTINUE transition. If routing to or from the page is conditional, you'll need to define guard s that determine the state. Read more on xstate guards here . Add a test state for the new page to the machineTests object in tests/flows/auth.test.js If routing is conditional, add items with appropriate data to the testData array.","title":"Auth Pages Routing"},{"location":"portal/development/#nextconfigjs","text":"The next.config.js file is a Node.js module that can be used to configure build and export behavior, such as Webpack settings.","title":"next.config.js"},{"location":"portal/development/#frontend-backend-configuration","text":"By default, portal is set up to connect to the backend staging environment. If you wish to connect local to local, please see detailed instructions here .","title":"Frontend &lt;&gt; Backend Configuration"},{"location":"portal/environment-variables/","text":"Environment variables Environment variables include feature flags , maintenance pages , and URLs and keys for external resources such as Cognito and the API. Configuring environment variables Default environment variables live in portal/config/default.js . Each environment has a corresponding configuration file in portal/config , for example production.js The default environment variables are merged with the environment-specific config file in portal/config/index.js . A default environment variable can be overridden in the environment-specific config file. Portal environment variables should never include a secret! Since the Portal is only served on the client-side, these environment variables will be publicly accessible. Each time you add a new environment variable, ensure that you add it to each environment's config file, so that an environment isn't missing anything. If the variable value is shared across many environments, consider adding it as a default environment variable in portal/config/default.js . Referencing an environment variable Within our codebase, environment variables are referenced from process.env . For example: Amplify . config ( process . env . myCustomKey ); How it works We use environment specific NPM scripts in portal/package.json to bundle builds with the correct configuration. For example build:stage . The target environment is set as the BUILD_ENV . For example: BUILD_ENV=stage npm run build When the build script is ran, the contents of the configuration file corresponding to BUILD_ENV are assigned to the Next.js env config option in portal/next.config.js . Next.js replaces process.env references with their values at build time. NODE_ENV The NODE_ENV environment variable is automatically set by Next.js during development and builds. For our test scripts, we manually set this in the test's NPM scripts. This variable determines whether our JS bundle includes the production build of React or the dev build . When our NPM scripts call next dev , NODE_ENV is automatically set to development and our JS bundle includes the React development build. When our NPM scripts call next build , NODE_ENV is automatically set to production and our JS bundle includes the optimized React production build. The NODE_ENV variable is also exposed to our code for use, allowing us to conditionally enable behavior for an environment, like only logging warnings when in development . Related Portal Configuration Management","title":"Environment variables"},{"location":"portal/environment-variables/#environment-variables","text":"Environment variables include feature flags , maintenance pages , and URLs and keys for external resources such as Cognito and the API.","title":"Environment variables"},{"location":"portal/environment-variables/#configuring-environment-variables","text":"Default environment variables live in portal/config/default.js . Each environment has a corresponding configuration file in portal/config , for example production.js The default environment variables are merged with the environment-specific config file in portal/config/index.js . A default environment variable can be overridden in the environment-specific config file. Portal environment variables should never include a secret! Since the Portal is only served on the client-side, these environment variables will be publicly accessible. Each time you add a new environment variable, ensure that you add it to each environment's config file, so that an environment isn't missing anything. If the variable value is shared across many environments, consider adding it as a default environment variable in portal/config/default.js .","title":"Configuring environment variables"},{"location":"portal/environment-variables/#referencing-an-environment-variable","text":"Within our codebase, environment variables are referenced from process.env . For example: Amplify . config ( process . env . myCustomKey );","title":"Referencing an environment variable"},{"location":"portal/environment-variables/#how-it-works","text":"We use environment specific NPM scripts in portal/package.json to bundle builds with the correct configuration. For example build:stage . The target environment is set as the BUILD_ENV . For example: BUILD_ENV=stage npm run build When the build script is ran, the contents of the configuration file corresponding to BUILD_ENV are assigned to the Next.js env config option in portal/next.config.js . Next.js replaces process.env references with their values at build time.","title":"How it works"},{"location":"portal/environment-variables/#node_env","text":"The NODE_ENV environment variable is automatically set by Next.js during development and builds. For our test scripts, we manually set this in the test's NPM scripts. This variable determines whether our JS bundle includes the production build of React or the dev build . When our NPM scripts call next dev , NODE_ENV is automatically set to development and our JS bundle includes the React development build. When our NPM scripts call next build , NODE_ENV is automatically set to production and our JS bundle includes the optimized React production build. The NODE_ENV variable is also exposed to our code for use, allowing us to conditionally enable behavior for an environment, like only logging warnings when in development .","title":"NODE_ENV"},{"location":"portal/environment-variables/#related","text":"Portal Configuration Management","title":"Related"},{"location":"portal/error-handling/","text":"Error handling All API requests flow through a \"logic hook\", which sends the request through the BaseAPI module. The BaseAPI module is then responsible for processing the response. Below is a visual representing the flow of a request and how an API error response flows back through the app and is rendered to the user: BaseAPI The normal API error response includes an errors property in its body representing the request's issues. When errors are present, the BaseAPI throws a ValidationError that holds all API errors that were in the response body. The BaseAPI may also throw other error types, such as NotFoundError or ForbiddenError , depending on the response's status code. Logic hook The logic hooks, like useBenefitsApplicationsLogic , are responsible for catching any errors thrown by the API module and sending the error into appErrorsLogic . A typical pattern for this looks like: async updateClaim ( patchData ) { try { // Send the API request await myApi . update ( patchData ) } catch ( error ) { // Handle any API errors appErrorsLogic . catchError ( error ) } } App Errors Logic hook The useAppErrorsLogic hook is where errors get sent to for processing and storing. When appErrorsLogic.catchError receives a ValidationError holding the API response's errors, it parses each API error and creates an AppErrorInfo instance with a message generated from the API issue's type , rule , and field properties. See the getMessageFromApiIssue method for how i18n keys are generated. If you're unsure what i18n key will be generated for the issue, you can trigger the error in local development and view the console to see what key it says is missing: If a matching i18n key isn't found for the API issue, the app falls back to the original error message sent from the API. We should always have an internationalized error message, and this fallback behavior should not be relied upon. <ErrorsSummary> The ErrorsSummary component is rendered by our app container ( _app.js ) above the page component. When the appErrorsLogic module has errors present, the ErrorsSummary component renders each error's message . Inline errors Each page has access to appErrorsLogic via the appLogic prop. The errors can be read from appLogic.appErrors . If the errors are associated with a specific field on the page, you can render them inline by setting the field component's errorMsg prop. The easiest way to do this is by using the useFunctionalInputProps to set the common props for your fields, one of which is errorMsg . A common pattern for forms in our app looks like this: const { formState, updateFields } = useFormState({ first_name: claim.first_name, }); const getFunctionalInputProps = useFunctionalInputProps({ appErrors: props.appLogic.appErrors, formState, updateFields, }); return ( <form> <InputText {...getFunctionalInputProps(\"first_name\")} label={t(\"pages.claimsName.firstNameLabel\")} /> </form> ); Alternatively, if you're not using useFunctionalInputProps , you can use appErrors.fieldErrorMessage(fieldName) to return the error message for a specific field: <InputText errorMsg={props.appErrors.fieldErrorMessage(\"first_name\")} name=\"first_name\" ...","title":"Error handling"},{"location":"portal/error-handling/#error-handling","text":"All API requests flow through a \"logic hook\", which sends the request through the BaseAPI module. The BaseAPI module is then responsible for processing the response. Below is a visual representing the flow of a request and how an API error response flows back through the app and is rendered to the user:","title":"Error handling"},{"location":"portal/error-handling/#baseapi","text":"The normal API error response includes an errors property in its body representing the request's issues. When errors are present, the BaseAPI throws a ValidationError that holds all API errors that were in the response body. The BaseAPI may also throw other error types, such as NotFoundError or ForbiddenError , depending on the response's status code.","title":"BaseAPI"},{"location":"portal/error-handling/#logic-hook","text":"The logic hooks, like useBenefitsApplicationsLogic , are responsible for catching any errors thrown by the API module and sending the error into appErrorsLogic . A typical pattern for this looks like: async updateClaim ( patchData ) { try { // Send the API request await myApi . update ( patchData ) } catch ( error ) { // Handle any API errors appErrorsLogic . catchError ( error ) } }","title":"Logic hook"},{"location":"portal/error-handling/#app-errors-logic-hook","text":"The useAppErrorsLogic hook is where errors get sent to for processing and storing. When appErrorsLogic.catchError receives a ValidationError holding the API response's errors, it parses each API error and creates an AppErrorInfo instance with a message generated from the API issue's type , rule , and field properties. See the getMessageFromApiIssue method for how i18n keys are generated. If you're unsure what i18n key will be generated for the issue, you can trigger the error in local development and view the console to see what key it says is missing: If a matching i18n key isn't found for the API issue, the app falls back to the original error message sent from the API. We should always have an internationalized error message, and this fallback behavior should not be relied upon.","title":"App Errors Logic hook"},{"location":"portal/error-handling/#errorssummary","text":"The ErrorsSummary component is rendered by our app container ( _app.js ) above the page component. When the appErrorsLogic module has errors present, the ErrorsSummary component renders each error's message .","title":"&lt;ErrorsSummary&gt;"},{"location":"portal/error-handling/#inline-errors","text":"Each page has access to appErrorsLogic via the appLogic prop. The errors can be read from appLogic.appErrors . If the errors are associated with a specific field on the page, you can render them inline by setting the field component's errorMsg prop. The easiest way to do this is by using the useFunctionalInputProps to set the common props for your fields, one of which is errorMsg . A common pattern for forms in our app looks like this: const { formState, updateFields } = useFormState({ first_name: claim.first_name, }); const getFunctionalInputProps = useFunctionalInputProps({ appErrors: props.appLogic.appErrors, formState, updateFields, }); return ( <form> <InputText {...getFunctionalInputProps(\"first_name\")} label={t(\"pages.claimsName.firstNameLabel\")} /> </form> ); Alternatively, if you're not using useFunctionalInputProps , you can use appErrors.fieldErrorMessage(fieldName) to return the error message for a specific field: <InputText errorMsg={props.appErrors.fieldErrorMessage(\"first_name\")} name=\"first_name\" ...","title":"Inline errors"},{"location":"portal/feature-flags/","text":"Feature flags Original tech spec for feature flags is available on Confluence . Note that a separate system is currently maintained for maintenance pages; see Maintenance Pages and Feature Gate Flags . Defining feature flags Feature flags are defined in portal/config/featureFlags.js . These flags are then set as environment variables in portal/next.config.js at initial build time (so they don't live reload when running locally). If you define a new feature flag during local development, you will need to restart the dev server in order for the flag to become available . Feature flags should be named so that the absence of a default value is interpreted as false \u2013 this way if someone forgets to define a feature flag, it doesn\u2019t unintentionally enable the feature for everyone. Checking feature flags in the code The services/featureFlags.js file includes methods for checking the value of a feature flag, as well as a method for overriding environment-level feature flags through the URL's query string. The isFeatureEnabled method can be used to check if a feature flag is enabled: if ( isFeatureEnabled ( \"showExample\" )) { return \"This is an example\" ; } Overriding a feature flag in the browser Cookies are used for overriding an environment's default feature flag. To do so, add a query param of _ff to the site's URL and use JSON notation for its value. A feature flag's value can be true , false , or reset . Setting a flag to reset will restore it to the environment's default. For example: To enable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:true To disable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:false To reset a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:reset You can also manage multiple flags by separating their key/value pairs with a ; . For example: {Site URL}?_ff=unrestrictedClaimFlow:true;anotherFlag:true Preventing / Allowing the site to be rendered While the site is under development, we don't want it to be visible to the press or the public . As a low-tech solution, we use a feature flag prefixed with pfml to determine whether the site should be rendered. To render the site, enable the pfmlTerriyay flag by visiting: {Site URL}?_ff=pfmlTerriyay:true \u26a0\ufe0f The exact flag name may change if we need to force the site to be hidden for people who previously enabled the flag through through browser. If the above flag doesn't work, you should check config/featureFlags.js to verify whether it's still the correct flag name we're using.","title":"Feature flags"},{"location":"portal/feature-flags/#feature-flags","text":"Original tech spec for feature flags is available on Confluence . Note that a separate system is currently maintained for maintenance pages; see Maintenance Pages and Feature Gate Flags .","title":"Feature flags"},{"location":"portal/feature-flags/#defining-feature-flags","text":"Feature flags are defined in portal/config/featureFlags.js . These flags are then set as environment variables in portal/next.config.js at initial build time (so they don't live reload when running locally). If you define a new feature flag during local development, you will need to restart the dev server in order for the flag to become available . Feature flags should be named so that the absence of a default value is interpreted as false \u2013 this way if someone forgets to define a feature flag, it doesn\u2019t unintentionally enable the feature for everyone.","title":"Defining feature flags"},{"location":"portal/feature-flags/#checking-feature-flags-in-the-code","text":"The services/featureFlags.js file includes methods for checking the value of a feature flag, as well as a method for overriding environment-level feature flags through the URL's query string. The isFeatureEnabled method can be used to check if a feature flag is enabled: if ( isFeatureEnabled ( \"showExample\" )) { return \"This is an example\" ; }","title":"Checking feature flags in the code"},{"location":"portal/feature-flags/#overriding-a-feature-flag-in-the-browser","text":"Cookies are used for overriding an environment's default feature flag. To do so, add a query param of _ff to the site's URL and use JSON notation for its value. A feature flag's value can be true , false , or reset . Setting a flag to reset will restore it to the environment's default. For example: To enable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:true To disable a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:false To reset a feature flag called unrestrictedClaimFlow , you would visit: {Site URL}?_ff=unrestrictedClaimFlow:reset You can also manage multiple flags by separating their key/value pairs with a ; . For example: {Site URL}?_ff=unrestrictedClaimFlow:true;anotherFlag:true","title":"Overriding a feature flag in the browser"},{"location":"portal/feature-flags/#preventing-allowing-the-site-to-be-rendered","text":"While the site is under development, we don't want it to be visible to the press or the public . As a low-tech solution, we use a feature flag prefixed with pfml to determine whether the site should be rendered. To render the site, enable the pfmlTerriyay flag by visiting: {Site URL}?_ff=pfmlTerriyay:true \u26a0\ufe0f The exact flag name may change if we need to force the site to be hidden for people who previously enabled the flag through through browser. If the above flag doesn't work, you should check config/featureFlags.js to verify whether it's still the correct flag name we're using.","title":"Preventing / Allowing the site to be rendered"},{"location":"portal/internationalization/","text":"Internationalization All Portal internationalization (i18n) is configured in the portal/src/locales/i18n.js module. The locale selection for both i18n systems is configured in i18n.js to allow locale synchronization across the app. Application This app uses i18next and its React library for application localization. You can find i18next translation resources in portal/locales/app/ . Each language has a file with a set of keys and values for every content string we need to localize for our site. The keys are how we will refer to these strings throughout our codebase, rather than hard coding the content strings. i18next patterns Our application takes advantage of some advanced patterns supported by the i18next library. It can be useful to be aware of these while working in the codebase: Context allows us to have variations of a string based on the value of a context property: t ( \"haveIncome\" , { context : \"married\" }); // -> \"Do you or your spouse have income sources?\" { \"haveIncome\" : \"Do you have income sources?\" , \"haveIncome_married\" : \"Do you or your spouse have income sources?\" } The Trans component allows us to integrate html tags such as links ( <a> tags) and text formatting tags (e.g. <strong> or <em> ) into translated text: <Trans i18nKey=\"userAgreement\" components={{ \"consent-link\": <a href=\"https://www.mass.gov/paidleave-informedconsent\" />, \"privacy-policy-link\": <a href=\"https://www.mass.gov/privacypolicy\" />, }} /> { \"userAgreement\" : \"To find out more about how the Commonwealth might use the information you share with DFML, please read the <consent-link>DFML Informed Consent Agreement</consent-link> and the <privacy-policy-link>Privacy Policy for Mass.gov</privacy-policy-link>.\" , } Note that we are using the alternative usage of Trans introduced in v11.6.0 where components are passed in as props rather than as children of Trans . This method allows the use of named tags in locale strings rather than needing to refer to child components by their index. Formatters are functions that define locale-specific formats for specially-formatted values such as currencies or time durations. t ( \"timeDuration\" , { minutes : 480 }); // -> \"8h\" t ( \"timeDuration\" , { minutes : 475 }); // -> \"7h 55m\" function formatValue ( value , format , locale ) { if ( format === \"hoursMinutesDuration\" ) { // Could also internationalize by using the locale value const { hours , minutes } = convertMinutesToHours ( value ); if ( minutes === 0 ) return ` ${ hours } h` ; return ` ${ hours } h ${ minutes } m` ; } return value ; { t imeDura t io n : \"{{minutes, hoursMinutesDuration}}\" , } Conventions Internationalization content can get messy and lead to hard-to-find bugs during translation. As such we strictly follow the below conventions to preserve readability, maintainability, and avoid errors. Organization Keys are organized under top-level objects by how they're used: components defines content used in specific components pages defines content used in specific pages errors defines content used in error messages, which aren\u2019t page or component specific shared defines content shared between multiple components or pages chars defines special characters (such as non-breaking spaces, which are difficult to distinguish in values) Keys are limited to three levels deep, for example pages.claimsDateOfBirth.title . This makes the structure easier to navigate and the process of finding a specific element more consistent. Naming Prioritize readability, and then brevity. Try to be consistent with existing keys. For example the title content for each page should be under a key, pages.<page-identifier>.title . When a page is related to a larger series of pages you can indicate that with a prefix. For example, the name form page within the claims flow is identified as pages.claimsName . Avoid repeating context in the key. For example, prefer pages.claimsName.sectionHint over pages.claimsName.nameSectionHint . Try to name keys after the purpose of the key, not just an underscored version of your translation. This may result in duplication of translations (i.e. multiple keys for \"First name\"), but this is much more flexible for cases down the line when one of those translations needs to change (i.e \"First name\" changes to \"Spouse's first name\"). Keys should be alphabetical so they're easier for others to find and update. We also considered sorting them by page-order where they occur but alphabetical is more easily enforced (with linting) and doesn\u2019t require re-ordering even if content on a page is re-ordered. Keys must be unique - there can't be two keys with the same name. This only applies to the entire key, for example having pages.claimsName.sectionHint and pages.claimsSsn.sectionHint is fine. Common page element terminology The PFML pages follow a design system that uses common terms for various page elements. It's helpful to use these terms when defining content strings for both the developer experience (when implementing a page design this gives you tips on how to name content strings) and in tracing content from the page back to the i18n key. These terms may change over time so this will need to be updated when they do. Some common element terms include: title - one per page sectionLabel - one per section or fieldset. This is typically either an HTML legend or label, depending on the page/section lead - additional context about an entire page, section, or fieldset legend - context about an embedded fieldset (note that sectionLabel content is always called sectionLabel even if we render it with an HTML legend element) label - typically one per input hint - additional context about a specific input Note: we use snake_case for input field names to match the names used by the API, but we don\u2019t carry that over to i18n keys. For example the state_id field on the claims license page has a label called pages.claimsLicense.stateIdLabel . For visual examples of different text elements on a page see the design team\u2019s page template designs. Sharing content All shared keys are located inside the shared object; this makes it obvious that when you're changing one of them your changes will impact multiple components/pages. This is meant to prevent accidental content changes if someone is only trying to update content in one place. Sharing content should follow this pattern: The shared content is defined with a key inside the shared object. Each page or component that uses the shared content should define its own key and reference the shared key. Shared keys, such as shared.claimsPages.leaveTypeTitle , shouldn't be referenced directly in application code. Example shared: { claimsPages: // Define the shared key here -- anyone changing this will know it affects multiple pages takingLeaveTitle: \"Who is taking leave?\", }, } pages: { claimsName: { // Reference the shared content with a key where it's used. This key, `pages.claimsName.title`, // will be the key used within the page. This makes it easy to stop using the shared content // by changing this to a content string. title: \"$t(shared.claimsPages.takingLeaveTitle)\", }, } Using markup in values Sometimes content necessitates the use of specific HTML tags. However, markup should only include tag names and not tag attributes. A tag's className , href , and other attributes should be set by the page or component that renders the key (see usage example below). Using markup in content strings can be useful but it can also make them more difficult to read and edit without introducing errors. Markup should be used sparingly. Some of the reasons for using markup in content strings include answering yes to any of these questions: Does this content require specific embedded tags for formatting? (e.g. <a> , <em> , <strong> , <ul> , <ol> , or multiple <p> tags) Does this content serve one primary function for the user? Is it helpful to edit this content as one key? (e.g. two paragraphs that serve one purpose for the user, or list items with an explicit order) Would breaking this content into multiple keys make them more difficult to name semantically or add unnecessary structure? Rendering values in pages and components Using the useTranslation hook With simple values, you only need to get the t function to render values in the functional component: import React from \"react\" ; import { useTranslation } from \"react-i18next\" ; export function MyComponent () { const { t } = useTranslation (); return < p > { t ( \"translationResourceKey\" )} < /p>; } The Trans component Since the t function can't be used for values that include markup, the Trans component is used to interpolate or translate complex react elements. Set the key in the language file, excluding all tag attributes: htmlKey: \"<ul><li>List item one</li><li>List item two has <my-link>a link</my-link></li></ul>\", Anchors are given a unique tag name in the language file, and the corresponding href value is defined in routes.js : myLink : \"http://www.example.com\" , The Trans component renders the basic tags, br , strong , i , and p . However, other tags must be explicitly defined. The components object is where attributes such as className , href can be set: import React from \"react\" ; import { Trans } from \"react-i18next\" ; < Trans i18nKey = \"htmlKey\" components = {{ \"my-link\" : ( < a target = \"_blank\" rel = \"noopener\" href = { routes . external . myLink } /> ), ul : < ul className = \"usa-list\" /> , li : < li /> , }} />","title":"Internationalization"},{"location":"portal/internationalization/#internationalization","text":"All Portal internationalization (i18n) is configured in the portal/src/locales/i18n.js module. The locale selection for both i18n systems is configured in i18n.js to allow locale synchronization across the app.","title":"Internationalization"},{"location":"portal/internationalization/#application","text":"This app uses i18next and its React library for application localization. You can find i18next translation resources in portal/locales/app/ . Each language has a file with a set of keys and values for every content string we need to localize for our site. The keys are how we will refer to these strings throughout our codebase, rather than hard coding the content strings.","title":"Application"},{"location":"portal/internationalization/#i18next-patterns","text":"Our application takes advantage of some advanced patterns supported by the i18next library. It can be useful to be aware of these while working in the codebase: Context allows us to have variations of a string based on the value of a context property: t ( \"haveIncome\" , { context : \"married\" }); // -> \"Do you or your spouse have income sources?\" { \"haveIncome\" : \"Do you have income sources?\" , \"haveIncome_married\" : \"Do you or your spouse have income sources?\" } The Trans component allows us to integrate html tags such as links ( <a> tags) and text formatting tags (e.g. <strong> or <em> ) into translated text: <Trans i18nKey=\"userAgreement\" components={{ \"consent-link\": <a href=\"https://www.mass.gov/paidleave-informedconsent\" />, \"privacy-policy-link\": <a href=\"https://www.mass.gov/privacypolicy\" />, }} /> { \"userAgreement\" : \"To find out more about how the Commonwealth might use the information you share with DFML, please read the <consent-link>DFML Informed Consent Agreement</consent-link> and the <privacy-policy-link>Privacy Policy for Mass.gov</privacy-policy-link>.\" , } Note that we are using the alternative usage of Trans introduced in v11.6.0 where components are passed in as props rather than as children of Trans . This method allows the use of named tags in locale strings rather than needing to refer to child components by their index. Formatters are functions that define locale-specific formats for specially-formatted values such as currencies or time durations. t ( \"timeDuration\" , { minutes : 480 }); // -> \"8h\" t ( \"timeDuration\" , { minutes : 475 }); // -> \"7h 55m\" function formatValue ( value , format , locale ) { if ( format === \"hoursMinutesDuration\" ) { // Could also internationalize by using the locale value const { hours , minutes } = convertMinutesToHours ( value ); if ( minutes === 0 ) return ` ${ hours } h` ; return ` ${ hours } h ${ minutes } m` ; } return value ; { t imeDura t io n : \"{{minutes, hoursMinutesDuration}}\" , }","title":"i18next patterns"},{"location":"portal/internationalization/#conventions","text":"Internationalization content can get messy and lead to hard-to-find bugs during translation. As such we strictly follow the below conventions to preserve readability, maintainability, and avoid errors.","title":"Conventions"},{"location":"portal/internationalization/#organization","text":"Keys are organized under top-level objects by how they're used: components defines content used in specific components pages defines content used in specific pages errors defines content used in error messages, which aren\u2019t page or component specific shared defines content shared between multiple components or pages chars defines special characters (such as non-breaking spaces, which are difficult to distinguish in values) Keys are limited to three levels deep, for example pages.claimsDateOfBirth.title . This makes the structure easier to navigate and the process of finding a specific element more consistent.","title":"Organization"},{"location":"portal/internationalization/#naming","text":"Prioritize readability, and then brevity. Try to be consistent with existing keys. For example the title content for each page should be under a key, pages.<page-identifier>.title . When a page is related to a larger series of pages you can indicate that with a prefix. For example, the name form page within the claims flow is identified as pages.claimsName . Avoid repeating context in the key. For example, prefer pages.claimsName.sectionHint over pages.claimsName.nameSectionHint . Try to name keys after the purpose of the key, not just an underscored version of your translation. This may result in duplication of translations (i.e. multiple keys for \"First name\"), but this is much more flexible for cases down the line when one of those translations needs to change (i.e \"First name\" changes to \"Spouse's first name\"). Keys should be alphabetical so they're easier for others to find and update. We also considered sorting them by page-order where they occur but alphabetical is more easily enforced (with linting) and doesn\u2019t require re-ordering even if content on a page is re-ordered. Keys must be unique - there can't be two keys with the same name. This only applies to the entire key, for example having pages.claimsName.sectionHint and pages.claimsSsn.sectionHint is fine.","title":"Naming"},{"location":"portal/internationalization/#common-page-element-terminology","text":"The PFML pages follow a design system that uses common terms for various page elements. It's helpful to use these terms when defining content strings for both the developer experience (when implementing a page design this gives you tips on how to name content strings) and in tracing content from the page back to the i18n key. These terms may change over time so this will need to be updated when they do. Some common element terms include: title - one per page sectionLabel - one per section or fieldset. This is typically either an HTML legend or label, depending on the page/section lead - additional context about an entire page, section, or fieldset legend - context about an embedded fieldset (note that sectionLabel content is always called sectionLabel even if we render it with an HTML legend element) label - typically one per input hint - additional context about a specific input Note: we use snake_case for input field names to match the names used by the API, but we don\u2019t carry that over to i18n keys. For example the state_id field on the claims license page has a label called pages.claimsLicense.stateIdLabel . For visual examples of different text elements on a page see the design team\u2019s page template designs.","title":"Common page element terminology"},{"location":"portal/internationalization/#sharing-content","text":"All shared keys are located inside the shared object; this makes it obvious that when you're changing one of them your changes will impact multiple components/pages. This is meant to prevent accidental content changes if someone is only trying to update content in one place. Sharing content should follow this pattern: The shared content is defined with a key inside the shared object. Each page or component that uses the shared content should define its own key and reference the shared key. Shared keys, such as shared.claimsPages.leaveTypeTitle , shouldn't be referenced directly in application code.","title":"Sharing content"},{"location":"portal/internationalization/#example","text":"shared: { claimsPages: // Define the shared key here -- anyone changing this will know it affects multiple pages takingLeaveTitle: \"Who is taking leave?\", }, } pages: { claimsName: { // Reference the shared content with a key where it's used. This key, `pages.claimsName.title`, // will be the key used within the page. This makes it easy to stop using the shared content // by changing this to a content string. title: \"$t(shared.claimsPages.takingLeaveTitle)\", }, }","title":"Example"},{"location":"portal/internationalization/#using-markup-in-values","text":"Sometimes content necessitates the use of specific HTML tags. However, markup should only include tag names and not tag attributes. A tag's className , href , and other attributes should be set by the page or component that renders the key (see usage example below). Using markup in content strings can be useful but it can also make them more difficult to read and edit without introducing errors. Markup should be used sparingly. Some of the reasons for using markup in content strings include answering yes to any of these questions: Does this content require specific embedded tags for formatting? (e.g. <a> , <em> , <strong> , <ul> , <ol> , or multiple <p> tags) Does this content serve one primary function for the user? Is it helpful to edit this content as one key? (e.g. two paragraphs that serve one purpose for the user, or list items with an explicit order) Would breaking this content into multiple keys make them more difficult to name semantically or add unnecessary structure?","title":"Using markup in values"},{"location":"portal/internationalization/#rendering-values-in-pages-and-components","text":"","title":"Rendering values in pages and components"},{"location":"portal/internationalization/#using-the-usetranslation-hook","text":"With simple values, you only need to get the t function to render values in the functional component: import React from \"react\" ; import { useTranslation } from \"react-i18next\" ; export function MyComponent () { const { t } = useTranslation (); return < p > { t ( \"translationResourceKey\" )} < /p>; }","title":"Using the useTranslation hook"},{"location":"portal/internationalization/#the-trans-component","text":"Since the t function can't be used for values that include markup, the Trans component is used to interpolate or translate complex react elements. Set the key in the language file, excluding all tag attributes: htmlKey: \"<ul><li>List item one</li><li>List item two has <my-link>a link</my-link></li></ul>\", Anchors are given a unique tag name in the language file, and the corresponding href value is defined in routes.js : myLink : \"http://www.example.com\" , The Trans component renders the basic tags, br , strong , i , and p . However, other tags must be explicitly defined. The components object is where attributes such as className , href can be set: import React from \"react\" ; import { Trans } from \"react-i18next\" ; < Trans i18nKey = \"htmlKey\" components = {{ \"my-link\" : ( < a target = \"_blank\" rel = \"noopener\" href = { routes . external . myLink } /> ), ul : < ul className = \"usa-list\" /> , li : < li /> , }} />","title":"The Trans component"},{"location":"portal/maintenance-pages/","text":"Maintenance pages The Portal includes the ability to have maintenance pages that we can turn on in case we need to shut down all or part of the website. Maintenance pages are controlled through the maintenance entry in the S3 feature-gate file corresponding to the environment. To update the maintenance status, you should update the appropriate file ( feature_flags/test.yaml to update the test environment) in a new branch. In your pull request, request the Production Admin on-call engineer for review. The Prod Admin will be accountable for any issues that may come up during that time, if it\u2019s off-hours. Once that branch is merged into the main branch, it will update the maintenance status on that environment due to the Github workflow that will sync the yaml files in that directory to s3. The data is retrieved from the file via API request in the portal and then set in appLogic.featureFlags.flags . The API request is cached for five minutes and only executed once for every full page request (not on react render/re-render). It may take up to 20 minutes for the portal to read the updated maintenance status due to s3 file caching in the API and the browser caching the API response. All available options for maintenance are: maintenance : start : <ISO 8601 timestamp> end : <ISO 8601 timestamp> enabled : 0 options : page_routes : - /* Add the options:page_routes key to the environment you want to target. It accepts an array of routes (without trailing slashes) that should render a maintenance page. The maintenance page will render on all routes by default if page_routes is omitted. Enable site wide Use * to match any string: enabled : 1 options : page_routes : - /* Enable on a group of pages enabled : 1 options : page_routes : - /applications/* Enable on specific pages enabled : 1 options : page_routes : - /create-account Enable scheduled maintenance page If the maintenance page is being added because of scheduled down time, you can optionally schedule the beginning and end of this maintenance time by setting the start and/or end keys to an ISO 8601 datetime string. Daylight savings time needs taken into account! For Eastern Daylight Time (EDT), use -04:00 , for Eastern Standard Time (EST), use -05:00 . The start and end time are optional. If start time is not set, the start time begins immediately. If end time is not set, an engineer will need to manually turn off the maintenance page. When end time is set, it will be displayed to the user using their timezone and localization preferences. For example, to enable the maintenance page on all routes, starting at March 25 at 3:30am EDT and ending at March 26 at 8pm EDT. start : \"2021-03-25T03:30:00-04:00\" end : \"2021-03-26T20:00:00-04:00\" enabled : 1 options : page_routes : - /* We're using Eastern time zone above since that's what Massachusetts is, but it could technically be whatever timezone. Disable site wide Any time enabled: 0 is set for maintenance, the maintenance status is disabled for the entire site. Disabling maintenance is very similar to enabling maintenance. Set enabled: 0 in the relevant feature flag file for the environment in a new branch. Once the PR is merged into main, the maintenance status will be disabled on that environment within 20 minutes. Testing To manually change the maintenance schedule without making a change to the YAML file in the repo, you can edit the file directly in the S3 bucket: massgov-pfml-{ environment }-feature-gate . There will be up to a 20 minute delay before your change is recognized by the API. This will cause the repo file and the S3 file to be out of sync, so you should revert your changes to the S3 file after you're done testing. If you are testing the configuration locally, just note that you need to restart the local development server in order for new environment variables to kick in. Bypassing maintenance pages It can be helpful to bypass a maintenance page to test the page itself. To do so, you can enable the noMaintenance feature flag in the browser. For example, to bypass the maintenance pages in the Test environment: https://paidleave-test.mass.gov?_ff=noMaintenance:true","title":"Maintenance pages"},{"location":"portal/maintenance-pages/#maintenance-pages","text":"The Portal includes the ability to have maintenance pages that we can turn on in case we need to shut down all or part of the website. Maintenance pages are controlled through the maintenance entry in the S3 feature-gate file corresponding to the environment. To update the maintenance status, you should update the appropriate file ( feature_flags/test.yaml to update the test environment) in a new branch. In your pull request, request the Production Admin on-call engineer for review. The Prod Admin will be accountable for any issues that may come up during that time, if it\u2019s off-hours. Once that branch is merged into the main branch, it will update the maintenance status on that environment due to the Github workflow that will sync the yaml files in that directory to s3. The data is retrieved from the file via API request in the portal and then set in appLogic.featureFlags.flags . The API request is cached for five minutes and only executed once for every full page request (not on react render/re-render). It may take up to 20 minutes for the portal to read the updated maintenance status due to s3 file caching in the API and the browser caching the API response. All available options for maintenance are: maintenance : start : <ISO 8601 timestamp> end : <ISO 8601 timestamp> enabled : 0 options : page_routes : - /* Add the options:page_routes key to the environment you want to target. It accepts an array of routes (without trailing slashes) that should render a maintenance page. The maintenance page will render on all routes by default if page_routes is omitted.","title":"Maintenance pages"},{"location":"portal/maintenance-pages/#enable-site-wide","text":"Use * to match any string: enabled : 1 options : page_routes : - /*","title":"Enable site wide"},{"location":"portal/maintenance-pages/#enable-on-a-group-of-pages","text":"enabled : 1 options : page_routes : - /applications/*","title":"Enable on a group of pages"},{"location":"portal/maintenance-pages/#enable-on-specific-pages","text":"enabled : 1 options : page_routes : - /create-account","title":"Enable on specific pages"},{"location":"portal/maintenance-pages/#enable-scheduled-maintenance-page","text":"If the maintenance page is being added because of scheduled down time, you can optionally schedule the beginning and end of this maintenance time by setting the start and/or end keys to an ISO 8601 datetime string. Daylight savings time needs taken into account! For Eastern Daylight Time (EDT), use -04:00 , for Eastern Standard Time (EST), use -05:00 . The start and end time are optional. If start time is not set, the start time begins immediately. If end time is not set, an engineer will need to manually turn off the maintenance page. When end time is set, it will be displayed to the user using their timezone and localization preferences. For example, to enable the maintenance page on all routes, starting at March 25 at 3:30am EDT and ending at March 26 at 8pm EDT. start : \"2021-03-25T03:30:00-04:00\" end : \"2021-03-26T20:00:00-04:00\" enabled : 1 options : page_routes : - /* We're using Eastern time zone above since that's what Massachusetts is, but it could technically be whatever timezone.","title":"Enable scheduled maintenance page"},{"location":"portal/maintenance-pages/#disable-site-wide","text":"Any time enabled: 0 is set for maintenance, the maintenance status is disabled for the entire site. Disabling maintenance is very similar to enabling maintenance. Set enabled: 0 in the relevant feature flag file for the environment in a new branch. Once the PR is merged into main, the maintenance status will be disabled on that environment within 20 minutes.","title":"Disable site wide"},{"location":"portal/maintenance-pages/#testing","text":"To manually change the maintenance schedule without making a change to the YAML file in the repo, you can edit the file directly in the S3 bucket: massgov-pfml-{ environment }-feature-gate . There will be up to a 20 minute delay before your change is recognized by the API. This will cause the repo file and the S3 file to be out of sync, so you should revert your changes to the S3 file after you're done testing. If you are testing the configuration locally, just note that you need to restart the local development server in order for new environment variables to kick in.","title":"Testing"},{"location":"portal/maintenance-pages/#bypassing-maintenance-pages","text":"It can be helpful to bypass a maintenance page to test the page itself. To do so, you can enable the noMaintenance feature flag in the browser. For example, to bypass the maintenance pages in the Test environment: https://paidleave-test.mass.gov?_ff=noMaintenance:true","title":"Bypassing maintenance pages"},{"location":"portal/monitoring/","text":"Monitoring We use New Relic Browser to monitor errors in our application. This works by including a JS snippet towards the top of our HTML. This snippet, new-relic.js , works out of the box by wrapping low-level browser APIs, and globally exposes an API ( newrelic ) for explicitly tracking interactions and errors that the app catches itself. New Relic Configuration In order for the New Relic snippet to send data to the correct New Relic Application, we need to set a newRelicAppId environment variable with the New Relic Browser Application ID . We then use the environment variable to configure the New Relic snippet by setting the window.NREUM global variable, in services/tracker.js . \ud83d\udea8 If we ever update the New Relic snippet, we should ensure that the configuration portion at the bottom is removed, so that we're not overwriting window.NREUM.loader_config or window.NREUM.info . Content Security Policy In order for the New Relic script to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: Scripts from https://js-agent.newrelic.com/ Scripts from https://bam.nr-data.net/ Correlating Events Across Systems In order to correlate New Relic events and logs between the frontend and backend, we enabled a New Relic feature called Distributed Tracing . This feature is enabled through the JavaScript snippet at the top of new-relic.js . Note that the ability to enable and disable this feature through the Application Settings in the New Relic Browser UI does not actually have any effect, and is therefore not used. In order for Distributed Tracing to work, all of the API environment origins need to be added to the list of allowed_origins at the top of new-relic.js . JS Errors New Relic Browser is used for monitoring JS Errors. JS Errors are reported to New Relic in a variety of ways: When we add a try/catch statement, we should call the newrelic API's noticeError method. For example: try { await doSomething (); } catch ( error ) { tracker . noticeError ( error ); } Our ErrorBoundary component catches any errors that bubble up from its descendant child components The New Relic snippet should automatically catch errors that our React app doesn't catch Errors caught by our ErrorBoundary component are reported to New Relic and also include a componentStack custom attribute, which provides additional information about which component threw the error. New Relic Browser's UI only displays the error's stack trace, so if you want to view the componentStack value, you'll need to use NRQL to query the JavaScriptError : SELECT timestamp , componentStack , stackTrace FROM JavaScriptError WHERE appName = 'PUT THE APP NAME HERE' Environments Each environment requires its own New Relic Browser \"app\" in order for us to differentiate between data coming in from the different environments. Create these through the New Relic site. Each New Relic app has its own applicationId , which will need set as part of the JS snippet included in our HTML. Enabling New Relic Browser If you have a browser extension installed to block trackers, you may need to safelist the New Relic snippet to allow it to make requests. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe payloads occasionally being sent to bam.nr-data.net Related Error monitoring research","title":"Monitoring"},{"location":"portal/monitoring/#monitoring","text":"We use New Relic Browser to monitor errors in our application. This works by including a JS snippet towards the top of our HTML. This snippet, new-relic.js , works out of the box by wrapping low-level browser APIs, and globally exposes an API ( newrelic ) for explicitly tracking interactions and errors that the app catches itself.","title":"Monitoring"},{"location":"portal/monitoring/#new-relic","text":"","title":"New Relic"},{"location":"portal/monitoring/#configuration","text":"In order for the New Relic snippet to send data to the correct New Relic Application, we need to set a newRelicAppId environment variable with the New Relic Browser Application ID . We then use the environment variable to configure the New Relic snippet by setting the window.NREUM global variable, in services/tracker.js . \ud83d\udea8 If we ever update the New Relic snippet, we should ensure that the configuration portion at the bottom is removed, so that we're not overwriting window.NREUM.loader_config or window.NREUM.info .","title":"Configuration"},{"location":"portal/monitoring/#content-security-policy","text":"In order for the New Relic script to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: Scripts from https://js-agent.newrelic.com/ Scripts from https://bam.nr-data.net/","title":"Content Security Policy"},{"location":"portal/monitoring/#correlating-events-across-systems","text":"In order to correlate New Relic events and logs between the frontend and backend, we enabled a New Relic feature called Distributed Tracing . This feature is enabled through the JavaScript snippet at the top of new-relic.js . Note that the ability to enable and disable this feature through the Application Settings in the New Relic Browser UI does not actually have any effect, and is therefore not used. In order for Distributed Tracing to work, all of the API environment origins need to be added to the list of allowed_origins at the top of new-relic.js .","title":"Correlating Events Across Systems"},{"location":"portal/monitoring/#js-errors","text":"New Relic Browser is used for monitoring JS Errors. JS Errors are reported to New Relic in a variety of ways: When we add a try/catch statement, we should call the newrelic API's noticeError method. For example: try { await doSomething (); } catch ( error ) { tracker . noticeError ( error ); } Our ErrorBoundary component catches any errors that bubble up from its descendant child components The New Relic snippet should automatically catch errors that our React app doesn't catch Errors caught by our ErrorBoundary component are reported to New Relic and also include a componentStack custom attribute, which provides additional information about which component threw the error. New Relic Browser's UI only displays the error's stack trace, so if you want to view the componentStack value, you'll need to use NRQL to query the JavaScriptError : SELECT timestamp , componentStack , stackTrace FROM JavaScriptError WHERE appName = 'PUT THE APP NAME HERE'","title":"JS Errors"},{"location":"portal/monitoring/#environments","text":"Each environment requires its own New Relic Browser \"app\" in order for us to differentiate between data coming in from the different environments. Create these through the New Relic site. Each New Relic app has its own applicationId , which will need set as part of the JS snippet included in our HTML.","title":"Environments"},{"location":"portal/monitoring/#enabling-new-relic-browser","text":"If you have a browser extension installed to block trackers, you may need to safelist the New Relic snippet to allow it to make requests. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe payloads occasionally being sent to bam.nr-data.net","title":"Enabling New Relic Browser"},{"location":"portal/monitoring/#related","text":"Error monitoring research","title":"Related"},{"location":"portal/naming-conventions/","text":"Naming conventions Field names Input and model field names should match the API field names, which means they should be formatted as snake case. \u2705 Like this \ud83d\uded1 Not like this first_name firstName Benefits Eliminates the need for additional code to convert a Portal naming style to the API naming style Consistent with what the field names would need to be if we were submitting the data directly through the HTML <form> rather than through JS.","title":"Naming conventions"},{"location":"portal/naming-conventions/#naming-conventions","text":"","title":"Naming conventions"},{"location":"portal/naming-conventions/#field-names","text":"Input and model field names should match the API field names, which means they should be formatted as snake case. \u2705 Like this \ud83d\uded1 Not like this first_name firstName Benefits Eliminates the need for additional code to convert a Portal naming style to the API naming style Consistent with what the field names would need to be if we were submitting the data directly through the HTML <form> rather than through JS.","title":"Field names"},{"location":"portal/software-architecture/","text":"Software Architecture This page describes the software architecture and design patterns used for the Portal. Overview The primary design pattern used in the Portal is Model-View-Controller , which is the design pattern most commonly used whenever designing any application with a user interface. Data Models Not to be confused with the \"Model\" in Model-View-Controller (MVC), data models represent concepts in the context of the PFML program and the Portal. Data models are used throughout business logic and controller logic, but are not used by lower level View components which should be domain agnostic. Data models should only represent data and not have any business logic or behavior or have any dependencies outside of data models (with the possible exception of simple helper functions). App The App is the \"Controller\" for the application and is the effective entry point to the application as directed by the Nextjs framework. The App controls web application logic, what state to pass to pages, and how to handle application events like submitting claims. Top level application logic should live here. Pages Page components are the primary \"Controller\" unit of the application and represent a single page of the user experience. Page components are the lowest level \"Controller\" in our application (with the exception of more complex View components that have their own nested MVC pattern). Page components control which \"View\" components to render, what state to pass to the components to render, and what event handlers to use to handle \"View\" events. Components Components are the primary \"View\" unit of the application. Simple components simply render data that is passed to them, and exposes events that parent components (the view's controller) can listen to. More complex components can themselves act as controllers and have their own models and nested controllers/views. The QuestionPage component is an example of a more complex component that includes a back button and a \"Save and Continue\" button with onSave and onContinue events that can be listened to. API The API module is reponsible for application-level business logic. This represents the top level \"Model\" in the MVC design pattern. This includes starting claims, submitting claims, identity proofing, setting payment preferences, etc. State Hooks State hooks are modules used by various components to define and update the state of the application. These hooks represent the \"Model\" in the MVC design pattern and should not have any knowledge of controller or view code. An example of a state hook is useFormState . Event Hooks Event hooks are modules that define functions to handle view events. These modules attach view events to the appropriate model update functions, and are the glue that allows us to keep \"Model\" and \"View\" decoupled from each other. Examples of event hooks include useHandleInputChange and useAppLogic . Dependencies To help prevent technical debt, when adding new modules, consider adding assertions to dependencies.test.js to restrict where the module is used, or what dependencies the new module is allowed to have. Services Services expose functionality that offers a coherent feature / addresses a business need (e.g. services/featureFlags.js ). A rule of thumb to use is that services are things that could conceptually be turned into separate microservices. In practice, some of these will actually be served by external services and others will not. Examples include caching, translation/localization, validation, authentication, and any APIs. For example, in our codebase we already have these services: Error monitoring: uses external New Relic API, so this is an \"actual\" service Feature flags: these don't use an external APIs in this implementation but instead just uses the browser APIs (cookies) to accomplish the task. We could conceivably refactor this module to rely on an external API though. For example LaunchDarkly is precisely a product that offers \"feature flags as a service\" but for now we're just using a homegrown \"feature flags service\") Utilities Utilities ( utils ) on the other hand are lightweight functions that are useful but don't really offer any business value on their own. They often fall into the category of filling in gaps of the programming language like manipulating strings or other simple data structures that the language itself doesn't offer functions for, and for which we don't feel the need to introduce a separate library/dependency for. A rule of thumb to use for utilities is that they should be generally small and standalone i.e. they shouldn't have any non-trivial dependencies (like external APIs or large libraries).","title":"Software Architecture"},{"location":"portal/software-architecture/#software-architecture","text":"This page describes the software architecture and design patterns used for the Portal.","title":"Software Architecture"},{"location":"portal/software-architecture/#overview","text":"The primary design pattern used in the Portal is Model-View-Controller , which is the design pattern most commonly used whenever designing any application with a user interface.","title":"Overview"},{"location":"portal/software-architecture/#data-models","text":"Not to be confused with the \"Model\" in Model-View-Controller (MVC), data models represent concepts in the context of the PFML program and the Portal. Data models are used throughout business logic and controller logic, but are not used by lower level View components which should be domain agnostic. Data models should only represent data and not have any business logic or behavior or have any dependencies outside of data models (with the possible exception of simple helper functions).","title":"Data Models"},{"location":"portal/software-architecture/#app","text":"The App is the \"Controller\" for the application and is the effective entry point to the application as directed by the Nextjs framework. The App controls web application logic, what state to pass to pages, and how to handle application events like submitting claims. Top level application logic should live here.","title":"App"},{"location":"portal/software-architecture/#pages","text":"Page components are the primary \"Controller\" unit of the application and represent a single page of the user experience. Page components are the lowest level \"Controller\" in our application (with the exception of more complex View components that have their own nested MVC pattern). Page components control which \"View\" components to render, what state to pass to the components to render, and what event handlers to use to handle \"View\" events.","title":"Pages"},{"location":"portal/software-architecture/#components","text":"Components are the primary \"View\" unit of the application. Simple components simply render data that is passed to them, and exposes events that parent components (the view's controller) can listen to. More complex components can themselves act as controllers and have their own models and nested controllers/views. The QuestionPage component is an example of a more complex component that includes a back button and a \"Save and Continue\" button with onSave and onContinue events that can be listened to.","title":"Components"},{"location":"portal/software-architecture/#api","text":"The API module is reponsible for application-level business logic. This represents the top level \"Model\" in the MVC design pattern. This includes starting claims, submitting claims, identity proofing, setting payment preferences, etc.","title":"API"},{"location":"portal/software-architecture/#state-hooks","text":"State hooks are modules used by various components to define and update the state of the application. These hooks represent the \"Model\" in the MVC design pattern and should not have any knowledge of controller or view code. An example of a state hook is useFormState .","title":"State Hooks"},{"location":"portal/software-architecture/#event-hooks","text":"Event hooks are modules that define functions to handle view events. These modules attach view events to the appropriate model update functions, and are the glue that allows us to keep \"Model\" and \"View\" decoupled from each other. Examples of event hooks include useHandleInputChange and useAppLogic .","title":"Event Hooks"},{"location":"portal/software-architecture/#dependencies","text":"To help prevent technical debt, when adding new modules, consider adding assertions to dependencies.test.js to restrict where the module is used, or what dependencies the new module is allowed to have.","title":"Dependencies"},{"location":"portal/software-architecture/#services","text":"Services expose functionality that offers a coherent feature / addresses a business need (e.g. services/featureFlags.js ). A rule of thumb to use is that services are things that could conceptually be turned into separate microservices. In practice, some of these will actually be served by external services and others will not. Examples include caching, translation/localization, validation, authentication, and any APIs. For example, in our codebase we already have these services: Error monitoring: uses external New Relic API, so this is an \"actual\" service Feature flags: these don't use an external APIs in this implementation but instead just uses the browser APIs (cookies) to accomplish the task. We could conceivably refactor this module to rely on an external API though. For example LaunchDarkly is precisely a product that offers \"feature flags as a service\" but for now we're just using a homegrown \"feature flags service\")","title":"Services"},{"location":"portal/software-architecture/#utilities","text":"Utilities ( utils ) on the other hand are lightweight functions that are useful but don't really offer any business value on their own. They often fall into the category of filling in gaps of the programming language like manipulating strings or other simple data structures that the language itself doesn't offer functions for, and for which we don't feel the need to introduce a separate library/dependency for. A rule of thumb to use for utilities is that they should be generally small and standalone i.e. they shouldn't have any non-trivial dependencies (like external APIs or large libraries).","title":"Utilities"},{"location":"portal/storybook/","text":"Storybook Storybook is a tool for UI development. It makes development faster and easier by isolating components. This allows you to work on one component at a time. You can develop entire UIs without needing to run the Portal application, or navigating around the Portal flow. You can read more below about the process and collaboration goals behind our usage of Storybook. The ultimate output is a static site that currently gets deployed here . (Related: Deploy docs ) Contributing to Storybook First: The Storybook docs are great. If your question isn't answered in this doc, it's likely answered in their docs. This doc is more of a TLDR to get you started contributing to our Storybook. Developers can run Storybook locally by running: npm run docs Adding a new story Each page in Storybook is generated from *.stories.js files located in portal/storybook/stories/ . This file exports functions ( \"stories\" ), each of which is rendered on the page. The functions can render any arbitrary React, so a Story can render a page, component, or plain HTML. For example, here's a Storybook page with a single story that renders a preview of our Button component: // portal/storybook/stories/components/Button.stories.js import Button from \"src/components/Button\"; import React from \"react\"; export default { // this determines where the story shows in the site's sidebar menu: title: \"Components/Button\", // this is used to generate the props table on the page: component: Button, // these are passed into each story and enable nifty // live editing in the site's UI: args: { children: \"Save and continue\", handleClick: () => alert(\"Clicked!\"), }, }; export const Primary = (args) => { return <Button {...args} />; }; Learn more about writing stories \u2192 Previewing entire pages In Next.js, a page is just a React component . This enables some pretty cool capabilities when combined with Storybook. Specifically, this means we can create a Story that renders a Page component, which allows us to preview an entire page without navigating through the entire application flow to preview the page. For the Create Claim flow, we're generating a Storybook page for each page in the flow . Engineers can override the generated story by adding a *.stories.js file for the page in the storybook/stories/pages/applications directory. How Storybook interacts with our source code Storybook stories import and render the same components used in the live site. Below is a visualization attempting to demonstrate how assets are shared between our web application and the Storybook site. For engineers For the engineering team, Storybook provides a place to document components and also provides a place outside of our Next.js app to preview new components being developed. For engineers, Storybook aims to answer questions like: What components exist? How does this component behave when rendered? What is this component for? What props does this component expect? Engineers could also look at the source code to answer most of these questions, but Storybook provides a friendly UI to answer them. It also provides a mechanism for producing smaller pull requests \u2014 for instance, rather than creating a component and connecting it to our application logic, an engineer could first create a pull request that adds just the component and a corresponding Storybook story for previewing the component. For designers There's also a world where Storybook can be used as a shared tool, supporting a more collaborative workflow between Engineering, Design, and Product teams. One goal is to have a source of truth for design pattern documentation and guidance. Ideally that lives close to the actual implementation of the patterns. The goals for Storybook in this case would be to answer the questions above, as well as questions like: Why does this design pattern exist? What problems is it addressing? What does this design pattern look like at different screen sizes? What have we learned about this pattern through user research? How has it evolved? This may mean that some Storybook pages are more in-depth than others. These more extensive documentation pages would be a collaboration between Engineering and Design. Engineering's role might be: code the components and setup the Storybook page with an example of the component. It would then be Design's role to contribute design documentation, like any decisions or research that fed into the creation of the pattern, or guidance on different ways the component should be used. There may be times when Engineering and Design pair on a Storybook page to ensure the implementation achieves the intended design.","title":"Storybook"},{"location":"portal/storybook/#storybook","text":"Storybook is a tool for UI development. It makes development faster and easier by isolating components. This allows you to work on one component at a time. You can develop entire UIs without needing to run the Portal application, or navigating around the Portal flow. You can read more below about the process and collaboration goals behind our usage of Storybook. The ultimate output is a static site that currently gets deployed here . (Related: Deploy docs )","title":"Storybook"},{"location":"portal/storybook/#contributing-to-storybook","text":"First: The Storybook docs are great. If your question isn't answered in this doc, it's likely answered in their docs. This doc is more of a TLDR to get you started contributing to our Storybook. Developers can run Storybook locally by running: npm run docs","title":"Contributing to Storybook"},{"location":"portal/storybook/#adding-a-new-story","text":"Each page in Storybook is generated from *.stories.js files located in portal/storybook/stories/ . This file exports functions ( \"stories\" ), each of which is rendered on the page. The functions can render any arbitrary React, so a Story can render a page, component, or plain HTML. For example, here's a Storybook page with a single story that renders a preview of our Button component: // portal/storybook/stories/components/Button.stories.js import Button from \"src/components/Button\"; import React from \"react\"; export default { // this determines where the story shows in the site's sidebar menu: title: \"Components/Button\", // this is used to generate the props table on the page: component: Button, // these are passed into each story and enable nifty // live editing in the site's UI: args: { children: \"Save and continue\", handleClick: () => alert(\"Clicked!\"), }, }; export const Primary = (args) => { return <Button {...args} />; }; Learn more about writing stories \u2192","title":"Adding a new story"},{"location":"portal/storybook/#previewing-entire-pages","text":"In Next.js, a page is just a React component . This enables some pretty cool capabilities when combined with Storybook. Specifically, this means we can create a Story that renders a Page component, which allows us to preview an entire page without navigating through the entire application flow to preview the page. For the Create Claim flow, we're generating a Storybook page for each page in the flow . Engineers can override the generated story by adding a *.stories.js file for the page in the storybook/stories/pages/applications directory.","title":"Previewing entire pages"},{"location":"portal/storybook/#how-storybook-interacts-with-our-source-code","text":"Storybook stories import and render the same components used in the live site. Below is a visualization attempting to demonstrate how assets are shared between our web application and the Storybook site.","title":"How Storybook interacts with our source code"},{"location":"portal/storybook/#for-engineers","text":"For the engineering team, Storybook provides a place to document components and also provides a place outside of our Next.js app to preview new components being developed. For engineers, Storybook aims to answer questions like: What components exist? How does this component behave when rendered? What is this component for? What props does this component expect? Engineers could also look at the source code to answer most of these questions, but Storybook provides a friendly UI to answer them. It also provides a mechanism for producing smaller pull requests \u2014 for instance, rather than creating a component and connecting it to our application logic, an engineer could first create a pull request that adds just the component and a corresponding Storybook story for previewing the component.","title":"For engineers"},{"location":"portal/storybook/#for-designers","text":"There's also a world where Storybook can be used as a shared tool, supporting a more collaborative workflow between Engineering, Design, and Product teams. One goal is to have a source of truth for design pattern documentation and guidance. Ideally that lives close to the actual implementation of the patterns. The goals for Storybook in this case would be to answer the questions above, as well as questions like: Why does this design pattern exist? What problems is it addressing? What does this design pattern look like at different screen sizes? What have we learned about this pattern through user research? How has it evolved? This may mean that some Storybook pages are more in-depth than others. These more extensive documentation pages would be a collaboration between Engineering and Design. Engineering's role might be: code the components and setup the Storybook page with an example of the component. It would then be Design's role to contribute design documentation, like any decisions or research that fed into the creation of the pattern, or guidance on different ways the component should be used. There may be times when Engineering and Design pair on a Storybook page to ensure the implementation achieves the intended design.","title":"For designers"},{"location":"portal/tests/","text":"Tests Introduction Jest is used as our JS test runner and is very similar to Jasmine. We also use jest-dom custom matchers. Read the Jest documentation to learn how to write assertions. A good place to start if you are new to JS testing is to learn about Using Matchers . Below is an example of a test: import sum from \"./sum\" ; describe ( \"sum\" , () => { it ( \"adds 1 + 2 to equal 3\" , () => { const result = sum ( 1 , 2 ); expect ( result ). toBe ( 3 ); }); }); Creating new test files A test file should be placed in the appropriate tests directory (e.g. portal/tests ) rather than alongside the file it tests. These test files should have the same name as the file they're testing, and have .test.js as the extension. For example, pages/index.js and tests/pages/index.test.js . Unit tests We use React Testing Library alongside Jest. React Testing Library (RTL), enables us to interact with rendered DOM nodes directly in our tests. This helps us to write tests that resemble the way our software is used (more on that in the RTL guiding principles ). React Testing Library Resources: - React Testing Library Docs - Cheatsheet guide to reference all the available React Testing Library fucntions - Playground to test out your queries Below is an example of a React component test: import { fireEvent , render , screen } from \"@testing-library/react\" ; import Button from \"./Button\" ; describe ( \"Button\" , () => { it ( \"is clickable\" , () => { const onClickHandler = jest . fn (); render ( < Button onClick = { onClickHandler } > Click me < /Button>); fireEvent . click ( screen . getByText ( /Click me/ )); expect ( onClickHandler ). toHaveBeenCalled (); }); }); Snapshot tests Snapshot tests are useful for testing when a React component or JSON output changes unexpectedly. A typical snapshot test case is to render a UI component, take a snapshot, then compares it to the last snapshot that was taken. The test will fail if the two snapshots do not match. If a snapshot test fails, you should identify whether it failed because the change was unexpected. If it was an unexpected change, then you may have unintentionally broke an expected behavior, in which case you should investigate and fix it before sending the PR out for review. If it failed because you intentionally changed something related to the test, then the snapshot should be updated to reflect the intended change. To update snapshots, run: npm run test:update-snapshots Learn more about Snapshot Testing. JSON snapshot example it ( \"renders the fields with the expected content and attributes\" , () => { const output = callMethodAndReturnJSON (); // You can use inline snapshots if the output is fairly short: expect ( output ). toMatchInlineSnapshot (); }); React snapshot example it ( \"renders the component as expected\" , () => { const { container } = render ( < ExampleComponent /> ); expect ( container . firstChild ). toMatchSnapshot (); }); Mocks Mock functions make it easy to test the links between code by erasing the actual implementation of a function, capturing calls to the function (and the parameters passed in those calls), capturing instances of constructor functions when instantiated with new, and allowing test-time configuration of return values. The quickest way to mock a module is to call jest.mock('MODULE_NAME_HERE') at the top of your test file. To create a manual mock of a Node module, create a file in a top-level __mocks__ directory. You can also create a Mock function/spy using jest.fn() Learn more about Mock Functions . Test coverage Jest includes built-in support for measuring test coverage , using Istanbul . The coverageReporters Jest setting can be modified for more advanced test coverage use cases.","title":"Tests"},{"location":"portal/tests/#tests","text":"","title":"Tests"},{"location":"portal/tests/#introduction","text":"Jest is used as our JS test runner and is very similar to Jasmine. We also use jest-dom custom matchers. Read the Jest documentation to learn how to write assertions. A good place to start if you are new to JS testing is to learn about Using Matchers . Below is an example of a test: import sum from \"./sum\" ; describe ( \"sum\" , () => { it ( \"adds 1 + 2 to equal 3\" , () => { const result = sum ( 1 , 2 ); expect ( result ). toBe ( 3 ); }); });","title":"Introduction"},{"location":"portal/tests/#creating-new-test-files","text":"A test file should be placed in the appropriate tests directory (e.g. portal/tests ) rather than alongside the file it tests. These test files should have the same name as the file they're testing, and have .test.js as the extension. For example, pages/index.js and tests/pages/index.test.js .","title":"Creating new test files"},{"location":"portal/tests/#unit-tests","text":"We use React Testing Library alongside Jest. React Testing Library (RTL), enables us to interact with rendered DOM nodes directly in our tests. This helps us to write tests that resemble the way our software is used (more on that in the RTL guiding principles ). React Testing Library Resources: - React Testing Library Docs - Cheatsheet guide to reference all the available React Testing Library fucntions - Playground to test out your queries Below is an example of a React component test: import { fireEvent , render , screen } from \"@testing-library/react\" ; import Button from \"./Button\" ; describe ( \"Button\" , () => { it ( \"is clickable\" , () => { const onClickHandler = jest . fn (); render ( < Button onClick = { onClickHandler } > Click me < /Button>); fireEvent . click ( screen . getByText ( /Click me/ )); expect ( onClickHandler ). toHaveBeenCalled (); }); });","title":"Unit tests"},{"location":"portal/tests/#snapshot-tests","text":"Snapshot tests are useful for testing when a React component or JSON output changes unexpectedly. A typical snapshot test case is to render a UI component, take a snapshot, then compares it to the last snapshot that was taken. The test will fail if the two snapshots do not match. If a snapshot test fails, you should identify whether it failed because the change was unexpected. If it was an unexpected change, then you may have unintentionally broke an expected behavior, in which case you should investigate and fix it before sending the PR out for review. If it failed because you intentionally changed something related to the test, then the snapshot should be updated to reflect the intended change. To update snapshots, run: npm run test:update-snapshots Learn more about Snapshot Testing.","title":"Snapshot tests"},{"location":"portal/tests/#json-snapshot-example","text":"it ( \"renders the fields with the expected content and attributes\" , () => { const output = callMethodAndReturnJSON (); // You can use inline snapshots if the output is fairly short: expect ( output ). toMatchInlineSnapshot (); });","title":"JSON snapshot example"},{"location":"portal/tests/#react-snapshot-example","text":"it ( \"renders the component as expected\" , () => { const { container } = render ( < ExampleComponent /> ); expect ( container . firstChild ). toMatchSnapshot (); });","title":"React snapshot example"},{"location":"portal/tests/#mocks","text":"Mock functions make it easy to test the links between code by erasing the actual implementation of a function, capturing calls to the function (and the parameters passed in those calls), capturing instances of constructor functions when instantiated with new, and allowing test-time configuration of return values. The quickest way to mock a module is to call jest.mock('MODULE_NAME_HERE') at the top of your test file. To create a manual mock of a Node module, create a file in a top-level __mocks__ directory. You can also create a Mock function/spy using jest.fn() Learn more about Mock Functions .","title":"Mocks"},{"location":"portal/tests/#test-coverage","text":"Jest includes built-in support for measuring test coverage , using Istanbul . The coverageReporters Jest setting can be modified for more advanced test coverage use cases.","title":"Test coverage"},{"location":"portal/typescript/","text":"TypeScript Portal's source code is written in TypeScript . TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. The TypeScript landing page does a good job of describing TypeScript, and how it relates to JavaScript. Give it a scroll. Portal was originally written in JavaScript and later migrated to TypeScript in October 2021. You can reference the original migration tech spec in Confluence for additional context. Conventions and norms Use interface instead of class for data models that have no getters, setters, or default values. This simplifies the code and results in less compiled JavaScript. Portal data models should match the API's. For example, if an API model indicates a field is nullable / Optional , the corresponding Portal data model should reflect the field can be null as well. Our type declarations should be as narrow as possible however, and we should be comfortable questioning if an API data model field is truly ever empty in reality. View the API's schema here View an overview of the API DB models here Explicitly declaring function return types is encouraged . This helps make the engineer's intent clear. We're not currently enforcing this with a lint rule , since this can produce a lot of noise due to React functional components and hooks. It's solvable, but was deprioritized during the initial TypeScript migration effort. Avoid TypeScript features that produce different JS code when compiled when there is a suitable alternative. For example, we avoid using TypeScript's enum and parameter properties . Lint rules are configured to prevent those two particular examples. Why? \u2935\ufe0f TypeScript is supposed to be JavaScript, but with static type features added. If we remove all of the types from TypeScript code, what's left should be valid JavaScript code. The formal word used in the TypeScript documentation is \"type-level extension\": most TypeScript features are type-level extensions to JavaScript, but they don't affect the code's runtime behavior. Unfortunately, TypeScript's solution is to break its own rule in this case [for enums]. When compiling an enum, the compiler adds extra JavaScript code that never existed in the original TypeScript code. There are very few TypeScript features like this. Each of these unusual features adds a confusing complication to the otherwise simple TypeScript compiler model. \u2014 Execute Program's TypeScript course. Tricky errors & solutions Writing boolean expressions The strict-boolean-expressions lint rule is enforced, and can be confusing when initially encountered. Before this rule, we could (and did) have conditions that would check if a number was set, for instance: const size : number = props . size ; const className = size ? `margin- ${ size } ` : \"margin-5\" ; A bug is present in that example, because 0 is falsey in JavaScript, so if size was 0 , className would be margin-5 (the fallback) instead of margin-0 . With the lint rule, this is now an error indicating: Unexpected nullable number value in conditional. Please handle the nullish/zero/NaN cases explicitly. You can now fix the lint error with a few approaches. Nullish coalescing : const className = `margin- ${ size ?? 5 } ` ; Using the isBlank utility: const className = isBlank ( size ) ? \"margin-5\" : `margin- ${ size } ` ; Handling null/undefined values null and undefined have their own distinct types and you\u2019ll get a type error if you try to use them where a concrete value is expected. For example with this TypeScript code, users.find has no guarantee that it will actually find a user, but you can write code as though it will: declare const loggedInUsername : string ; const users = [ { name : \"Oby\" , age : 12 }, { name : \"Heera\" , age : 32 }, ]; const loggedInUser = users . find (( u ) => u . name === loggedInUsername ); console . log ( loggedInUser . age ); You'll receive an error that you have not made a guarantee that the loggedInUser exists before trying to use it. One approach to solving this is to use the optional chaining operator : console . log ( loggedInUser ? . age ); This behavior is due to strictNullChecks . Narrowing an object's type TypeScript supports narrowing objects by using the in operator . For example: interface ErrorWithIssues { issues : string []; } interface BlankError {} function handleError ( error : ErrorWithIssues | BlankError ) { if ( \"issues\" in error ) { // We've now narrowed error down to ErrorWithIssues console . error ( error . issues . join ( \",\" )); } else { // We've now narrowed error down to BlankError console . error ( \"Generic error occurred\" ); } } Learning TypeScript Crash courses and references TypeScript Handbook TypeScript Playground React TypeScript Cheatsheets More in depth course material Execute Program Effective TypeScript","title":"TypeScript"},{"location":"portal/typescript/#typescript","text":"Portal's source code is written in TypeScript . TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. The TypeScript landing page does a good job of describing TypeScript, and how it relates to JavaScript. Give it a scroll. Portal was originally written in JavaScript and later migrated to TypeScript in October 2021. You can reference the original migration tech spec in Confluence for additional context.","title":"TypeScript"},{"location":"portal/typescript/#conventions-and-norms","text":"Use interface instead of class for data models that have no getters, setters, or default values. This simplifies the code and results in less compiled JavaScript. Portal data models should match the API's. For example, if an API model indicates a field is nullable / Optional , the corresponding Portal data model should reflect the field can be null as well. Our type declarations should be as narrow as possible however, and we should be comfortable questioning if an API data model field is truly ever empty in reality. View the API's schema here View an overview of the API DB models here Explicitly declaring function return types is encouraged . This helps make the engineer's intent clear. We're not currently enforcing this with a lint rule , since this can produce a lot of noise due to React functional components and hooks. It's solvable, but was deprioritized during the initial TypeScript migration effort. Avoid TypeScript features that produce different JS code when compiled when there is a suitable alternative. For example, we avoid using TypeScript's enum and parameter properties . Lint rules are configured to prevent those two particular examples. Why? \u2935\ufe0f TypeScript is supposed to be JavaScript, but with static type features added. If we remove all of the types from TypeScript code, what's left should be valid JavaScript code. The formal word used in the TypeScript documentation is \"type-level extension\": most TypeScript features are type-level extensions to JavaScript, but they don't affect the code's runtime behavior. Unfortunately, TypeScript's solution is to break its own rule in this case [for enums]. When compiling an enum, the compiler adds extra JavaScript code that never existed in the original TypeScript code. There are very few TypeScript features like this. Each of these unusual features adds a confusing complication to the otherwise simple TypeScript compiler model. \u2014 Execute Program's TypeScript course.","title":"Conventions and norms"},{"location":"portal/typescript/#tricky-errors-solutions","text":"","title":"Tricky errors &amp; solutions"},{"location":"portal/typescript/#writing-boolean-expressions","text":"The strict-boolean-expressions lint rule is enforced, and can be confusing when initially encountered. Before this rule, we could (and did) have conditions that would check if a number was set, for instance: const size : number = props . size ; const className = size ? `margin- ${ size } ` : \"margin-5\" ; A bug is present in that example, because 0 is falsey in JavaScript, so if size was 0 , className would be margin-5 (the fallback) instead of margin-0 . With the lint rule, this is now an error indicating: Unexpected nullable number value in conditional. Please handle the nullish/zero/NaN cases explicitly. You can now fix the lint error with a few approaches. Nullish coalescing : const className = `margin- ${ size ?? 5 } ` ; Using the isBlank utility: const className = isBlank ( size ) ? \"margin-5\" : `margin- ${ size } ` ;","title":"Writing boolean expressions"},{"location":"portal/typescript/#handling-nullundefined-values","text":"null and undefined have their own distinct types and you\u2019ll get a type error if you try to use them where a concrete value is expected. For example with this TypeScript code, users.find has no guarantee that it will actually find a user, but you can write code as though it will: declare const loggedInUsername : string ; const users = [ { name : \"Oby\" , age : 12 }, { name : \"Heera\" , age : 32 }, ]; const loggedInUser = users . find (( u ) => u . name === loggedInUsername ); console . log ( loggedInUser . age ); You'll receive an error that you have not made a guarantee that the loggedInUser exists before trying to use it. One approach to solving this is to use the optional chaining operator : console . log ( loggedInUser ? . age ); This behavior is due to strictNullChecks .","title":"Handling null/undefined values"},{"location":"portal/typescript/#narrowing-an-objects-type","text":"TypeScript supports narrowing objects by using the in operator . For example: interface ErrorWithIssues { issues : string []; } interface BlankError {} function handleError ( error : ErrorWithIssues | BlankError ) { if ( \"issues\" in error ) { // We've now narrowed error down to ErrorWithIssues console . error ( error . issues . join ( \",\" )); } else { // We've now narrowed error down to BlankError console . error ( \"Generic error occurred\" ); } }","title":"Narrowing an object's type"},{"location":"portal/typescript/#learning-typescript","text":"Crash courses and references TypeScript Handbook TypeScript Playground React TypeScript Cheatsheets More in depth course material Execute Program Effective TypeScript","title":"Learning TypeScript"},{"location":"portal/web-analytics/","text":"Web Analytics Massachusetts uses Google Analytics for cross-domain tracking on mass.gov properties. This works by including a Google Tag Manager JS snippet in the document head. Google Tag Manager is configured to dynamically insert Google Analytics onto the page when the web page is initially loaded. Viewing analytics Analytics for all of our environments are visible in the \"mass.gov cross domain tracking\" Google Analytics property. From within this property, you can filter results by the environment's domain you're interested in seeing. For example, to view real time traffic for all paidleave environments, you can visit this page . Custom metrics Single-page Application page views and custom event tracking should be setup through Google Tag Manager triggers . Using the Google Analytics ga function isn't recommended when using Google Tag Manager. Environment Configuration Each Portal environment should have a corresponding Google Tag Manager environment. Once a Google Tag Manager environment exists, we need to set the Portal gtmConfig.auth and gtmConfig.preview environment variables based on that Google Tag Manager environment's values. You can find these values through the Google Tag Manager website in the Admin section by navigating to the environment and looking at the JavaScript snippet for that environment. Look for a URL with gtm_auth and gtm_preview query parameters. The values for those query parameters should be set as the corresponding Portal environment variables. Content Security Policy In order for the Google Tag Manager scripts and Google Analytics scripts to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: The inline Google Tag Manager scripts Scripts from https://www.googletagmanager.com/ Scripts from https://www.google-analytics.com/ Tracking images from https://www.google-analytics.com/ The inline scripts need to be allowed by computing a Base64 encoded SHA-256 hash of the inline script and adding the hash to the allowed script sources in the Content Security Policy. To compute a hash, you can use an online tool like SHA256 Hash Generator . Or to use the Terminal, you can save the JavaScript snippet into a file e.g. gtm-snippet and run $ cat gtm-snippet | openssl sha256 -binary | openssl base64 Change Management To make changes to Google Tag Manager, create a new Google Tag Manager version with the desired changes, and publish those changes to the Google Tag Manager Test environment. To test those changes before publishing to Google Tag Manager's Stage and Prod environments, we need to be able to override the environment configuration to use the Google Tag Manager Test environment, which is something that will be implemented as part of CP-645 . In the meantime, you can manually insert the Google Tag Manager Test environment snippet onto the page you're viewing by executing the following lines of code in the browser console. Note the gtm_auth and gtm_preview values below are for the Google Tag Manager Test environment. var myScript = document.createElement('script'); myScript.textContent = \"(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\\n\" + \"new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\\n\" + \"j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\\n\" + \"'https://www.googletagmanager.com/gtm.js?id='+i+dl+ '&gtm_auth=SiSVu0U7VjoUiceaFWQeqA&gtm_preview=env-5&gtm_cookies_win=x';f.parentNode.insertBefore(j,f);\\n\" + \"})(window,document,'script','dataLayer','GTM-MCLNNQC');\"; document.head.appendChild(myScript); Publishing a new GTM version Once you've tested the changes in a Test environment, you can publish a new Google Tag Manager version to other environments. You can do this from the Versions tab, clicking the ... icon, and then clicking \"Publish to...\" Account Set Up In order to update Google Tag Manager configuration, you need a Google account associated with your @mass.gov email, and that account needs to be granted publish access in Google Tag Manager. In order to view Google Analytics data, your @mass.gov Google account needs to be added as a user to the mass.gov Google Analytics account. Enabling Google Tag Manager and Google Analytics If you have a browser extension installed to block trackers, you may need to safelist the google tag manager and google analytics. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe network requests to googletagmanager.com and google-analytics.com . Related Web analytics research","title":"Web Analytics"},{"location":"portal/web-analytics/#web-analytics","text":"Massachusetts uses Google Analytics for cross-domain tracking on mass.gov properties. This works by including a Google Tag Manager JS snippet in the document head. Google Tag Manager is configured to dynamically insert Google Analytics onto the page when the web page is initially loaded.","title":"Web Analytics"},{"location":"portal/web-analytics/#viewing-analytics","text":"Analytics for all of our environments are visible in the \"mass.gov cross domain tracking\" Google Analytics property. From within this property, you can filter results by the environment's domain you're interested in seeing. For example, to view real time traffic for all paidleave environments, you can visit this page .","title":"Viewing analytics"},{"location":"portal/web-analytics/#custom-metrics","text":"Single-page Application page views and custom event tracking should be setup through Google Tag Manager triggers . Using the Google Analytics ga function isn't recommended when using Google Tag Manager.","title":"Custom metrics"},{"location":"portal/web-analytics/#environment-configuration","text":"Each Portal environment should have a corresponding Google Tag Manager environment. Once a Google Tag Manager environment exists, we need to set the Portal gtmConfig.auth and gtmConfig.preview environment variables based on that Google Tag Manager environment's values. You can find these values through the Google Tag Manager website in the Admin section by navigating to the environment and looking at the JavaScript snippet for that environment. Look for a URL with gtm_auth and gtm_preview query parameters. The values for those query parameters should be set as the corresponding Portal environment variables.","title":"Environment Configuration"},{"location":"portal/web-analytics/#content-security-policy","text":"In order for the Google Tag Manager scripts and Google Analytics scripts to run, we need to make sure the Content Security Policy set by CloudFront in cloudfront-handler.js allows: The inline Google Tag Manager scripts Scripts from https://www.googletagmanager.com/ Scripts from https://www.google-analytics.com/ Tracking images from https://www.google-analytics.com/ The inline scripts need to be allowed by computing a Base64 encoded SHA-256 hash of the inline script and adding the hash to the allowed script sources in the Content Security Policy. To compute a hash, you can use an online tool like SHA256 Hash Generator . Or to use the Terminal, you can save the JavaScript snippet into a file e.g. gtm-snippet and run $ cat gtm-snippet | openssl sha256 -binary | openssl base64","title":"Content Security Policy"},{"location":"portal/web-analytics/#change-management","text":"To make changes to Google Tag Manager, create a new Google Tag Manager version with the desired changes, and publish those changes to the Google Tag Manager Test environment. To test those changes before publishing to Google Tag Manager's Stage and Prod environments, we need to be able to override the environment configuration to use the Google Tag Manager Test environment, which is something that will be implemented as part of CP-645 . In the meantime, you can manually insert the Google Tag Manager Test environment snippet onto the page you're viewing by executing the following lines of code in the browser console. Note the gtm_auth and gtm_preview values below are for the Google Tag Manager Test environment. var myScript = document.createElement('script'); myScript.textContent = \"(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\\n\" + \"new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\\n\" + \"j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\\n\" + \"'https://www.googletagmanager.com/gtm.js?id='+i+dl+ '&gtm_auth=SiSVu0U7VjoUiceaFWQeqA&gtm_preview=env-5&gtm_cookies_win=x';f.parentNode.insertBefore(j,f);\\n\" + \"})(window,document,'script','dataLayer','GTM-MCLNNQC');\"; document.head.appendChild(myScript);","title":"Change Management"},{"location":"portal/web-analytics/#publishing-a-new-gtm-version","text":"Once you've tested the changes in a Test environment, you can publish a new Google Tag Manager version to other environments. You can do this from the Versions tab, clicking the ... icon, and then clicking \"Publish to...\"","title":"Publishing a new GTM version"},{"location":"portal/web-analytics/#account-set-up","text":"In order to update Google Tag Manager configuration, you need a Google account associated with your @mass.gov email, and that account needs to be granted publish access in Google Tag Manager. In order to view Google Analytics data, your @mass.gov Google account needs to be added as a user to the mass.gov Google Analytics account.","title":"Account Set Up"},{"location":"portal/web-analytics/#enabling-google-tag-manager-and-google-analytics","text":"If you have a browser extension installed to block trackers, you may need to safelist the google tag manager and google analytics. If it's not being blocked, then it should Just Work. You can verify this by viewing the Network tab in DevTools, and observe network requests to googletagmanager.com and google-analytics.com .","title":"Enabling Google Tag Manager and Google Analytics"},{"location":"portal/web-analytics/#related","text":"Web analytics research","title":"Related"},{"location":"releases/api/","text":"environment release deployments test 6e566bc31 ( 21 commits ahead of api/v2.44.0 ) GitHub stage api/v2.44.0 GitHub prod api/v2.44.0 GitHub performance 645c9a13b ( 11 commits ahead of api/v2.44.0 ) GitHub training api/v2.44.0 GitHub uat api/v2.44.0 GitHub breakfix api/v2.44.0 GitHub cps-preview api/v2.44.0 GitHub long api/v2.44.0-rc1 GitHub trn2 api/v2.44.0 GitHub Full release notes are available in Confluence . Next Release ( Jira List ): PORTAL-1413 : Add long and trn2 to release pages ( #6640 ) PORTAL-1410 : Fix test operator for IAM conditions ( #6637 ) PORTAL-1208 : Part 1 add logging attributes about absence period and MR ( #6589 ) PAY-160 : Upload 1099 File name changes ( #6651 ) PAY-145 : tax withholding tests ( #6652 ) PAY-139 : s3 upload error ( #6588 ) INFRA-881 : update newrelic.ini to change ignore_errors to ignore_classes ( #6636 ) API-2246 : - Fix small dor-pending-filing-response-import issues ( #6656 ) API-2212 : - Accept CSV to set cease date and exemptions ( #6633 ) API-2125 : Utility to copy/download/move files in SFTP (MoveIT) ( #6373 ) API-1980 : Add DUA staging tables ( #6594 ) Separate bucket locations for input and output folders. Fixed tests to match. ( #6649 ) Log quarterly wages ( #6660 ) Raw * [PORTAL-1413](https://lwd.atlassian.net/browse/PORTAL-1413): Add long and trn2 to release pages ([#6640](https://github.com/EOLWD/pfml/pull/6640)) * [PORTAL-1410](https://lwd.atlassian.net/browse/PORTAL-1410): Fix test operator for IAM conditions ([#6637](https://github.com/EOLWD/pfml/pull/6637)) * [PORTAL-1208](https://lwd.atlassian.net/browse/PORTAL-1208): Part 1 add logging attributes about absence period and MR ([#6589](https://github.com/EOLWD/pfml/pull/6589)) * [PAY-160](https://lwd.atlassian.net/browse/PAY-160): Upload 1099 File name changes ([#6651](https://github.com/EOLWD/pfml/pull/6651)) * [PAY-145](https://lwd.atlassian.net/browse/PAY-145): tax withholding tests ([#6652](https://github.com/EOLWD/pfml/pull/6652)) * [PAY-139](https://lwd.atlassian.net/browse/PAY-139): s3 upload error ([#6588](https://github.com/EOLWD/pfml/pull/6588)) * [INFRA-881](https://lwd.atlassian.net/browse/INFRA-881): update newrelic.ini to change `ignore_errors` to `ignore_classes` ([#6636](https://github.com/EOLWD/pfml/pull/6636)) * [API-2246](https://lwd.atlassian.net/browse/API-2246): - Fix small dor-pending-filing-response-import issues ([#6656](https://github.com/EOLWD/pfml/pull/6656)) * [API-2212](https://lwd.atlassian.net/browse/API-2212): - Accept CSV to set cease date and exemptions ([#6633](https://github.com/EOLWD/pfml/pull/6633)) * [API-2125](https://lwd.atlassian.net/browse/API-2125): Utility to copy/download/move files in SFTP (MoveIT) ([#6373](https://github.com/EOLWD/pfml/pull/6373)) * [API-1980](https://lwd.atlassian.net/browse/API-1980): Add DUA staging tables ([#6594](https://github.com/EOLWD/pfml/pull/6594)) * Separate bucket locations for input and output folders. Fixed tests to match. ([#6649](https://github.com/EOLWD/pfml/pull/6649)) * Log quarterly wages ([#6660](https://github.com/EOLWD/pfml/pull/6660))","title":"Api"},{"location":"releases/portal/","text":"environment release deployments test 6e566bc31 ( 7 commits ahead of portal/v50.0-rc1 ) GitHub stage portal/v50.0-rc1 GitHub prod portal/v49.0 GitHub performance portal/v50.0-rc1 GitHub training portal/v49.0 GitHub uat portal/v50.0-rc1 GitHub breakfix portal/v49.0 GitHub cps-preview portal/v50.0-rc1 GitHub long portal/v50.0-rc1 GitHub trn2 portal/v49.0 GitHub Full release notes are available in Confluence . Next Release ( Jira List ): PORTAL-1417 : Disable Jest's printBasicPrototype setting ( #6634 ) Raw * [PORTAL-1417](https://lwd.atlassian.net/browse/PORTAL-1417): Disable Jest's `printBasicPrototype` setting ([#6634](https://github.com/EOLWD/pfml/pull/6634))","title":"Portal"}]}